# {py:mod}`src.models.subnets.encoders.moe`

```{py:module} src.models.subnets.encoders.moe
```

```{autodoc2-docstring} src.models.subnets.encoders.moe
:allowtitles:
```

## Submodules

```{toctree}
:titlesonly:
:maxdepth: 1

src.models.subnets.encoders.moe.moe_multi_head_attention_layer
src.models.subnets.encoders.moe.encoder
```

## Package Contents

### Data

````{list-table}
:class: autosummary longtable
:align: left

* - {py:obj}`__all__ <src.models.subnets.encoders.moe.__all__>`
  - ```{autodoc2-docstring} src.models.subnets.encoders.moe.__all__
    :summary:
    ```
````

### API

````{py:data} __all__
:canonical: src.models.subnets.encoders.moe.__all__
:value: >
   ['MoEGraphAttentionEncoder']

```{autodoc2-docstring} src.models.subnets.encoders.moe.__all__
```

````
