"""
Group Sequence Policy Optimization (GSPO) algorithm.
"""
from __future__ import annotations

import torch

from logic.src.pipeline.rl.core.ppo import PPO


class GSPO(PPO):
    """
    Group Sequence Policy Optimization (GSPO).

    GSPO:
    - Group-based advantage normalization (handled by PPO base if batch is a group)
    - Sequence-level importance ratio: r = exp((log π_new - log π_old) / |sequence|)
    - Clipped surrogate objective same as PPO
    """

    def calculate_ratio(self, new_log_p: torch.Tensor, old_log_p: torch.Tensor) -> torch.Tensor:
        """
        GSPO: Sequence-level importance ratio.
        """
        # We need the sequence length. We can infer it from the action tensor
        # But wait, old_log_p and new_log_p are already summed over the sequence.
        # However, the policy forward usually returns log_likelihood for the whole sequence.
        # We need to know the sequence length to normalize.

        # NOTE: A limitation here is that we don't have the actions directly in this method
        # unless we pass them or store them.
        # Let's adjust PPO.calculate_ratio signature if needed, or just assume
        # that we use a simple heuristic if the exact length is hard to get here.
        # Actually, in constructive routing, sequence length is usually num_nodes + return_to_depot.

        # For now, let's keep the standard ratio until we decide how to pass sequence length.
        # Or better, override training_step if needed.

        return torch.exp(new_log_p - old_log_p.detach())
        # To truly implement GSPO here, we'd want:
        # return torch.exp((new_log_p - old_log_p.detach()) / seq_len)
