"""
Post-Processing module for routing solutions.

This module provides a modular framework for refining tours generated by
base routing policies. It follows the Registry/Factory pattern to support
various refinement strategies like Fast TSP, Vectorized 2-opt, Swap, etc.

Decouples tour construction from tour optimization.
"""

from abc import ABC, abstractmethod
from typing import Any, Callable, Dict, List, Optional, Type

import numpy as np
import torch

from .single_vehicle import find_route


# --- IPostProcessor Interface ---
class IPostProcessor(ABC):
    """
    Interface for all routing post-processors.
    """

    @abstractmethod
    def process(self, tour: List[int], **kwargs: Any) -> List[int]:
        """
        Refine a given tour.

        Args:
            tour: Initial tour (List of bin IDs including depot 0s)
            **kwargs: Context dictionary containing distance matrix, etc.

        Returns:
            List[int]: Refined tour.
        """
        pass


# --- Post-Processor Registry ---
class PostProcessorRegistry:
    """Registry for routing post-processing strategies."""

    _strategies: Dict[str, Type[IPostProcessor]] = {}

    @classmethod
    def register(cls, name: str) -> Callable:
        """Decorator to register a post-processor."""

        def wrapper(processor_cls: Type[IPostProcessor]):
            cls._strategies[name.lower()] = processor_cls
            return processor_cls

        return wrapper

    @classmethod
    def get(cls, name: str) -> Optional[Type[IPostProcessor]]:
        """Retrieve a post-processor by name."""
        return cls._strategies.get(name.lower())


# --- Post-Processor Factory ---
class PostProcessorFactory:
    """Factory for creating post-processing strategy instances."""

    @staticmethod
    def create(name: str) -> IPostProcessor:
        """
        Create a post-processor instance by name.
        """
        cls = PostProcessorRegistry.get(name)
        if not cls:
            # Fallback for dynamic/mapped names if needed
            if name.lower() == "fast_tsp":
                return FastTSPPostProcessor()
            elif name.lower() in ["2opt", "swap", "relocate", "swap_star", "3opt"]:
                return ClassicalLocalSearchPostProcessor(operator_name=name.lower())
            elif name.lower() == "random":
                return RandomLocalSearchPostProcessor()

            raise ValueError(f"Unknown post-processor: {name}")
        return cls()


# --- Concrete Implementation: Fast TSP ---
@PostProcessorRegistry.register("fast_tsp")
class FastTSPPostProcessor(IPostProcessor):
    """
    Refines all sub-tours using the fast_tsp library.
    Splits long tours by depot (0), re-optimizes each segment, and reconstructs.
    """

    def process(self, tour: List[int], **kwargs: Any) -> List[int]:
        distance_matrix = kwargs.get("distance_matrix")
        if distance_matrix is None:
            return tour

        if isinstance(distance_matrix, torch.Tensor):
            distance_matrix = distance_matrix.cpu().numpy()

        # Split tour into sub-tours (trips)
        trips = []
        current_trip = []
        for node in tour:
            if node == 0:
                if current_trip:
                    trips.append(current_trip)
                    current_trip = []
            else:
                current_trip.append(node)

        if not trips:
            return tour

        refined_tour = [0]
        for trip in trips:
            if len(trip) > 1:
                # Re-optimize with fast_tsp
                refined_trip = find_route(distance_matrix, np.array(trip))
                # strip depot 0s from the constructed route to avoid doubles
                refined_tour.extend([n for n in refined_trip if n != 0])
            else:
                refined_tour.extend(trip)
            refined_tour.append(0)

        return refined_tour


# --- Concrete Implementation: Classical Local Search ---
@PostProcessorRegistry.register("classical")
class ClassicalLocalSearchPostProcessor(IPostProcessor):
    """
    Wrapper for vectorized local search operators from
    logic/src/models/policies/classical/local_search.py
    """

    def __init__(self, operator_name: str = "2opt"):
        self.operator_name = operator_name

    def process(self, tour: List[int], **kwargs: Any) -> List[int]:
        import torch

        from logic.src.models.policies.classical.local_search import (
            vectorized_relocate,
            vectorized_swap,
            vectorized_swap_star,
            vectorized_three_opt,
            vectorized_two_opt,
            vectorized_two_opt_star,
        )

        distance_matrix = kwargs.get("distance_matrix", kwargs.get("distancesC"))
        if distance_matrix is None:
            return tour

        # Parameters from kwargs (extracted from context/config in actions.py)
        # Default to 50 iterations as a safe post-processing refinement
        max_iter = kwargs.get("max_iterations", kwargs.get("post_process_max_iter", 50))

        # Ensure Tensors
        device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
        if not isinstance(distance_matrix, torch.Tensor):
            dm_tensor = torch.from_numpy(np.array(distance_matrix)).float().to(device)
        else:
            dm_tensor = distance_matrix.to(device)

        # Handle batching: operators expect [B, N]
        # We ensure tour length matches and no single-node tours
        if len(tour) < 4:
            return tour

        tour_tensor = torch.tensor(tour, device=device).unsqueeze(0)  # (1, N)

        ops = {
            "2opt": vectorized_two_opt,
            "swap": vectorized_swap,
            "relocate": vectorized_relocate,
            "2opt*": vectorized_two_opt_star,
            "swap_star": vectorized_swap_star,
            "3opt": vectorized_three_opt,
            "two_opt": vectorized_two_opt,
            "two_opt_star": vectorized_two_opt_star,
            "three_opt": vectorized_three_opt,
        }

        op_fn = ops.get(self.operator_name, vectorized_two_opt)

        try:
            refined_tensor = op_fn(tour_tensor, dm_tensor, max_iterations=max_iter)
            return refined_tensor.squeeze(0).cpu().tolist()
        except Exception:
            # Fallback to original on error
            return tour


# --- Concrete Implementation: Random Local Search ---
@PostProcessorRegistry.register("random")
class RandomLocalSearchPostProcessor(IPostProcessor):
    """
    Performs stochastic local search refinement by applying random operators.
    Mirrors the logic of RandomLocalSearchPolicy but as a post-processor.
    """

    def process(self, tour: List[int], **kwargs: Any) -> List[int]:
        import torch

        from logic.src.models.policies.classical.local_search import (
            vectorized_relocate,
            vectorized_swap,
            vectorized_swap_star,
            vectorized_three_opt,
            vectorized_two_opt,
            vectorized_two_opt_star,
        )

        distance_matrix = kwargs.get("distance_matrix", kwargs.get("distancesC"))
        if distance_matrix is None or len(tour) < 4:
            return tour

        # 1. Configuration Extracted from context
        # n_iterations is the number of random operator applications
        n_iterations = kwargs.get("n_iterations", kwargs.get("post_process_iterations", 50))
        op_probs = kwargs.get("op_probs") or {
            "two_opt": 0.25,
            "swap": 0.15,
            "relocate": 0.15,
            "two_opt_star": 0.2,
            "swap_star": 0.15,
            "three_opt": 0.1,
        }

        # 2. Setup Device and Tensors
        device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
        if not isinstance(distance_matrix, torch.Tensor):
            dm_tensor = torch.from_numpy(np.array(distance_matrix)).float().to(device)
        else:
            dm_tensor = distance_matrix.to(device)

        current_routes = torch.tensor(tour, device=device).unsqueeze(0)  # (1, N)

        # 3. Setup Operator Map and Sampling
        op_map = {
            "two_opt": vectorized_two_opt,
            "swap": vectorized_swap,
            "relocate": vectorized_relocate,
            "two_opt_star": vectorized_two_opt_star,
            "swap_star": vectorized_swap_star,
            "three_opt": vectorized_three_opt,
        }

        ops_sorted = sorted(op_map.keys())
        probs = torch.tensor([op_probs.get(op, 0.0) for op in ops_sorted], dtype=torch.float32)
        probs = probs / (probs.sum() + 1e-10)

        # 4. Iterative stochastic refinement
        iter_count = int(n_iterations) if n_iterations is not None else 50
        try:
            # Sample operators
            op_indices = torch.multinomial(probs, iter_count, replacement=True).tolist()
            for op_idx in op_indices:
                op_name = ops_sorted[op_idx]
                op_func = op_map[op_name]
                # Apply 1 iteration of the sampled operator
                current_routes = op_func(current_routes, dm_tensor, max_iterations=1)

            return current_routes.squeeze(0).cpu().tolist()
        except Exception:
            return tour
