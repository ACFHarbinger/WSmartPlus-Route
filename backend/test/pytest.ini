# pytest configuration file
[pytest]
# Test discovery patterns
python_files = test_*.py *_test.py
python_classes = Test* *Test
python_functions = test_* *_test

# Test paths
testpaths = test

# Console output options
addopts = 
    # Verbose output
    -v
    # Show summary of all test outcomes
    -ra
    # Show local variables in tracebacks
    --showlocals
    # Strict markers (fail on unknown markers)
    --strict-markers
    # Strict config (fail on unknown config options)
    --strict-config
    # Shorter traceback format
    --tb=short
    # Color output
    --color=yes
    # Show captured output on failure
    --show-capture=all
    # Disable warnings to reduce noise
    --disable-warnings

# Markers for categorizing tests
markers =
    # Speed categories
    slow: marks tests as slow (deselect with '-m "not slow"')
    fast: marks tests as fast running
    
    # Test type categories
    unit: marks tests as unit tests
    integration: marks tests as integration tests
    validation: marks tests that validate argument combinations
    edge_case: marks tests for edge cases
    parametrize: marks parametrized tests
    
    # Command-specific categories
    train: marks tests related to training command
    mrl_train: marks tests related to meta-reinforcement learning training
    hp_optim: marks tests related to hyperparameter optimization
    eval: marks tests related to evaluation command
    test_sim: marks tests related to simulator test command
    gen_data: marks tests related to data generation command
    file_system: marks tests related to file system operations
    gui: marks tests related to GUI operations
    test_suite: marks tests related to test suite command
    
    # Functional categories
    arg_parser: marks tests for argument parser functionality
    cli_args: marks tests for command line interface arguments
    model: marks tests for model functionality
    data: marks tests for data processing
    
    # Special markers
    skip: skip this test
    skipif: skip this test if condition is true
    xfail: expected to fail

# Filter warnings to reduce noise
filterwarnings =
    ignore::DeprecationWarning
    ignore::PendingDeprecationWarning
    ignore::UserWarning
    ignore::FutureWarning
    
    # Ignore specific library warnings
    ignore:.*urllib3.*:UserWarning
    ignore:.*distutils.*:DeprecationWarning
    
    # Treat specific warnings as errors
    error::ResourceWarning

# Logging configuration
log_level = INFO
log_format = %(asctime)s [%(levelname)8s] %(name)s: %(message)s
log_date_format = %Y-%m-%d %H:%M:%S
log_cli = true
log_cli_level = INFO
log_cli_format = %(asctime)s [%(levelname)8s] %(name)s: %(message)s
log_cli_date_format = %Y-%m-%d %H:%M:%S

# JUnit XML output (for CI systems)
junit_suite_name = wsmart_route
junit_logging = all
junit_duration_report = call

# Test timeout configuration (if using pytest-timeout)
#timeout = 300
#timeout_method = thread

# Coverage options (when using --cov)
[tool:coverage]
run =
    source =
        backend
    omit =
        */test/*
        */test_*
        */__pycache__/*
        */site-packages/*
        */.*/*
        setup.py
        conftest.py
    branch = True
    data_file = .coverage

[coverage:report]
# Fail if coverage is below this percentage
fail_under = 0
precision = 2
show_missing = True
skip_covered = False
exclude_lines =
    # Have to re-enable the standard pragma
    pragma: no cover
    # Don't complain about missing debug-only code
    def __repr__
    def __str__
    # Don't complain if tests don't hit defensive assertion code
    raise AssertionError
    raise NotImplementedError
    # Don't complain if non-runnable code isn't run
    if __name__ == .__main__.:
    if TYPE_CHECKING:
    @abstract
    # Ignore logging statements
    logger\.
    log\.
    # Ignore empty functions and classes
    pass
    # Ignore type checking imports
    from typing import
    import typing

[coverage:html]
directory = htmlcov
title = wsmart_route Test Coverage Report

[coverage:xml]
output = coverage.xml

[coverage:json]
output = coverage.json

# pytest-xdist configuration (for parallel testing)
# numprocesses = auto
# dist = loadscope
# maxprocesses = 4

# Custom configuration for your project
wsr_test_mode = true
wsr_temp_dir = ./temp_test