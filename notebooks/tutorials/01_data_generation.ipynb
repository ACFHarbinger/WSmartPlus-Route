{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tutorial 1: Data Generation and Problem Instances\n",
    "\n",
    "**WSmart+ Route Tutorial Series**\n",
    "\n",
    "This tutorial covers how to generate problem instances for Vehicle Routing Problems (VRP) using WSmart+ Route. You'll learn:\n",
    "\n",
    "1. The **TensorDict** data format used throughout the framework\n",
    "2. Using **generators** to create problem instances on-the-fly\n",
    "3. The **VRPInstanceBuilder** for batch dataset creation\n",
    "4. Different **distribution types** for waste/prize values\n",
    "5. **Dataset classes** for PyTorch DataLoader integration\n",
    "6. **Visualizing** problem instances\n",
    "\n",
    "**Prerequisites**: A working WSmart+ Route installation (`uv sync`)\n",
    "\n",
    "**Next tutorial**: [02_environments.ipynb](02_environments.ipynb) - RL Environments and Problem Formulation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Environment setup\n",
    "import os\n",
    "import sys\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# Add project root to path (two levels up from notebooks/tutorials/)\n",
    "PROJECT_ROOT = os.path.abspath(os.path.join(os.getcwd(), \"..\", \"..\"))\n",
    "if PROJECT_ROOT not in sys.path:\n",
    "    sys.path.insert(0, PROJECT_ROOT)\n",
    "\n",
    "print(f\"Project root: {PROJECT_ROOT}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch\n",
    "from tensordict import TensorDict\n",
    "\n",
    "# Reproducibility\n",
    "SEED = 42\n",
    "torch.manual_seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "\n",
    "# Device configuration\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "print(f\"PyTorch version: {torch.__version__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 1. Understanding TensorDict\n",
    "\n",
    "WSmart+ Route uses **TensorDict** as the universal data format for problem instances. A TensorDict is a dictionary-like container for tensors that supports batching, indexing, and device management."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a simple TensorDict representing 4 VRP instances with 10 nodes\n",
    "td = TensorDict(\n",
    "    {\n",
    "        \"locs\": torch.rand(4, 10, 2),       # Customer locations (x, y)\n",
    "        \"depot\": torch.rand(4, 2),           # Depot location\n",
    "        \"demand\": torch.rand(4, 10),         # Demand at each node\n",
    "        \"prize\": torch.rand(4, 10),          # Prize for visiting each node\n",
    "    },\n",
    "    batch_size=[4],\n",
    ")\n",
    "\n",
    "print(f\"Batch size: {td.batch_size}\")\n",
    "print(f\"Keys: {list(td.keys())}\")\n",
    "print(f\"Locations shape: {td['locs'].shape}  (batch, nodes, coords)\")\n",
    "print(f\"Demand shape: {td['demand'].shape}     (batch, nodes)\")\n",
    "print()\n",
    "\n",
    "# Indexing: get a single instance\n",
    "instance = td[0]\n",
    "print(f\"Single instance keys: {list(instance.keys())}\")\n",
    "print(f\"Single instance locs: {instance['locs'].shape}\")\n",
    "print()\n",
    "\n",
    "# Slicing: get a sub-batch\n",
    "sub_batch = td[:2]\n",
    "print(f\"Sub-batch size: {sub_batch.batch_size}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 2. Problem Instance Generators\n",
    "\n",
    "Generators create batches of problem instances on-the-fly. They're the primary way to create training data in the RL pipeline.\n",
    "\n",
    "### Available Generators\n",
    "\n",
    "| Generator | Problem | Key Features |\n",
    "|-----------|---------|-------------|\n",
    "| `VRPPGenerator` | Vehicle Routing with Profits | Locations, waste, prizes, capacity |\n",
    "| `WCVRPGenerator` | Waste Collection VRP | Locations, fill levels, overflow penalties |\n",
    "| `SCWCVRPGenerator` | Stochastic WCVRP | Noisy observations of fill levels |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from logic.src.envs.generators import VRPPGenerator, WCVRPGenerator, get_generator\n",
    "\n",
    "# Create a VRPP generator for 20-node problems\n",
    "gen = VRPPGenerator(\n",
    "    num_loc=20,\n",
    "    min_loc=0.0,\n",
    "    max_loc=1.0,\n",
    "    loc_distribution=\"uniform\",\n",
    "    waste_distribution=\"uniform\",\n",
    "    prize_distribution=\"uniform\",\n",
    "    capacity=1.0,\n",
    "    depot_type=\"center\",\n",
    ")\n",
    "\n",
    "# Generate a batch of 16 instances\n",
    "td = gen(batch_size=16)\n",
    "\n",
    "print(\"Generated TensorDict:\")\n",
    "print(f\"  Batch size: {td.batch_size}\")\n",
    "print(f\"  Keys: {list(td.keys())}\")\n",
    "print(f\"  Locations shape: {td['locs'].shape}\")\n",
    "print(f\"  Depot shape: {td['depot'].shape}\")\n",
    "print(f\"  Waste shape: {td['waste'].shape}\")\n",
    "print(f\"  Prize shape: {td['prize'].shape}\")\n",
    "print(f\"  Capacity: {td['capacity'][0].item():.1f}\")\n",
    "print(f\"  Max length: {td['max_length'][0].item():.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_instance(td, idx=0, title=\"VRPP Instance\"):\n",
    "    \"\"\"Plot a single problem instance with depot, nodes, and prize values.\"\"\"\n",
    "    locs = td[\"locs\"][idx].numpy()\n",
    "    depot = td[\"depot\"][idx].numpy()\n",
    "    prizes = td[\"prize\"][idx].numpy()\n",
    "\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "    # Left: Node locations colored by prize\n",
    "    ax = axes[0]\n",
    "    scatter = ax.scatter(locs[:, 0], locs[:, 1], c=prizes, cmap=\"YlOrRd\",\n",
    "                         s=80, edgecolors=\"black\", linewidth=0.5, zorder=2)\n",
    "    ax.scatter(depot[0], depot[1], c=\"blue\", s=200, marker=\"s\",\n",
    "               edgecolors=\"black\", linewidth=1, zorder=3, label=\"Depot\")\n",
    "    ax.set_xlabel(\"X\")\n",
    "    ax.set_ylabel(\"Y\")\n",
    "    ax.set_title(f\"{title} - Node Locations\")\n",
    "    ax.legend()\n",
    "    plt.colorbar(scatter, ax=ax, label=\"Prize Value\")\n",
    "\n",
    "    # Right: Distribution of prizes and waste\n",
    "    ax = axes[1]\n",
    "    waste = td[\"waste\"][idx].numpy()\n",
    "    ax.hist(prizes, bins=15, alpha=0.6, label=\"Prize\", color=\"green\")\n",
    "    ax.hist(waste, bins=15, alpha=0.6, label=\"Waste/Demand\", color=\"orange\")\n",
    "    ax.set_xlabel(\"Value\")\n",
    "    ax.set_ylabel(\"Count\")\n",
    "    ax.set_title(\"Prize & Waste Distributions\")\n",
    "    ax.legend()\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "plot_instance(td, idx=0, title=\"VRPP (20 nodes, uniform)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare different location distributions\n",
    "fig, axes = plt.subplots(1, 3, figsize=(15, 4))\n",
    "distributions = [\"uniform\", \"normal\", \"clustered\"]\n",
    "\n",
    "for ax, dist in zip(axes, distributions):\n",
    "    gen_dist = VRPPGenerator(num_loc=50, loc_distribution=dist)\n",
    "    td_dist = gen_dist(batch_size=1)\n",
    "    locs = td_dist[\"locs\"][0].numpy()\n",
    "    depot = td_dist[\"depot\"][0].numpy()\n",
    "\n",
    "    ax.scatter(locs[:, 0], locs[:, 1], s=30, alpha=0.8, label=\"Customers\")\n",
    "    ax.scatter(depot[0], depot[1], c=\"red\", s=100, marker=\"s\", label=\"Depot\")\n",
    "    ax.set_title(f\"Location Distribution: {dist}\")\n",
    "    ax.set_xlim(-0.05, 1.05)\n",
    "    ax.set_ylim(-0.05, 1.05)\n",
    "    ax.set_aspect(\"equal\")\n",
    "    ax.legend(fontsize=8)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Waste Collection VRP generator\n",
    "wc_gen = WCVRPGenerator(\n",
    "    num_loc=20,\n",
    "    min_fill=0.0,\n",
    "    max_fill=1.0,\n",
    "    fill_distribution=\"uniform\",\n",
    "    capacity=1.0,\n",
    ")\n",
    "\n",
    "td_wc = wc_gen(batch_size=16)\n",
    "\n",
    "print(\"WCVRP TensorDict:\")\n",
    "print(f\"  Keys: {list(td_wc.keys())}\")\n",
    "print(f\"  Fill levels shape: {td_wc['demand'].shape}\")\n",
    "print(f\"  Fill level range: [{td_wc['demand'].min():.3f}, {td_wc['demand'].max():.3f}]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from logic.src.envs import GENERATOR_REGISTRY, get_generator\n",
    "\n",
    "print(\"Available generators:\")\n",
    "for name, gen_cls in GENERATOR_REGISTRY.items():\n",
    "    print(f\"  {name}: {gen_cls.__name__}\")\n",
    "\n",
    "# Use the factory function\n",
    "gen_vrpp = get_generator(\"vrpp\", num_loc=20)\n",
    "gen_wcvrp = get_generator(\"wcvrp\", num_loc=20)\n",
    "print(f\"\\nVRPP generator: {type(gen_vrpp).__name__}\")\n",
    "print(f\"WCVRP generator: {type(gen_wcvrp).__name__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 3. VRPInstanceBuilder: Batch Dataset Creation\n",
    "\n",
    "For generating larger datasets (training, validation, test), use the **VRPInstanceBuilder** which provides a fluent builder pattern with support for empirical and gamma distributions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from logic.src.data.builders import VRPInstanceBuilder\n",
    "\n",
    "# Create builder and configure it\n",
    "builder = VRPInstanceBuilder()\n",
    "builder.set_dataset_size(32)\n",
    "builder.set_problem_size(20)\n",
    "builder.set_distribution(\"unif\")\n",
    "builder.set_problem_name(\"vrpp\")\n",
    "builder.set_num_days(1)\n",
    "\n",
    "# Build as TensorDict\n",
    "td_built = builder.build_td()\n",
    "\n",
    "print(\"Built TensorDict:\")\n",
    "print(f\"  Keys: {list(td_built.keys())}\")\n",
    "print(f\"  Batch size: {td_built.batch_size}\")\n",
    "for key in td_built.keys():\n",
    "    print(f\"  {key}: shape={td_built[key].shape}, dtype={td_built[key].dtype}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare different waste/prize distributions\n",
    "dist_names = [\"unif\", \"gamma1\", \"gamma2\", \"gamma3\", \"const\"]\n",
    "\n",
    "fig, axes = plt.subplots(1, len(dist_names), figsize=(18, 3.5))\n",
    "\n",
    "for ax, dist in zip(axes, dist_names):\n",
    "    builder = VRPInstanceBuilder()\n",
    "    builder.set_dataset_size(100)\n",
    "    builder.set_problem_size(20)\n",
    "    builder.set_distribution(dist)\n",
    "    builder.set_problem_name(\"vrpp\")\n",
    "    builder.set_num_days(1)\n",
    "\n",
    "    td_d = builder.build_td()\n",
    "    values = td_d[\"demand\"].flatten().numpy()\n",
    "\n",
    "    ax.hist(values, bins=30, alpha=0.7, color=\"steelblue\", edgecolor=\"white\")\n",
    "    ax.set_title(f\"Distribution: {dist}\")\n",
    "    ax.set_xlabel(\"Demand/Waste Value\")\n",
    "    ax.set_ylabel(\"Frequency\")\n",
    "\n",
    "plt.suptitle(\"Comparing Demand Distributions\", fontsize=14, y=1.02)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 4. Dataset Classes and DataLoaders\n",
    "\n",
    "WSmart+ Route provides dataset classes compatible with PyTorch's DataLoader for efficient batched training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from logic.src.data.datasets import GeneratorDataset, TensorDictDataset, tensordict_collate_fn\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# Option 1: From pre-generated TensorDict\n",
    "td_data = gen(batch_size=128)\n",
    "dataset = TensorDictDataset(td_data)\n",
    "print(f\"TensorDictDataset: {len(dataset)} instances\")\n",
    "\n",
    "# Create DataLoader with proper collation\n",
    "loader = DataLoader(\n",
    "    dataset,\n",
    "    batch_size=32,\n",
    "    shuffle=True,\n",
    "    collate_fn=tensordict_collate_fn,\n",
    ")\n",
    "\n",
    "# Iterate through one batch\n",
    "batch = next(iter(loader))\n",
    "print(f\"Batch keys: {list(batch.keys())}\")\n",
    "print(f\"Batch locs shape: {batch['locs'].shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Option 2: GeneratorDataset generates data on-the-fly\n",
    "gen_dataset = GeneratorDataset(gen, size=256)\n",
    "print(f\"GeneratorDataset: {len(gen_dataset)} instances\")\n",
    "\n",
    "gen_loader = DataLoader(\n",
    "    gen_dataset,\n",
    "    batch_size=32,\n",
    "    collate_fn=tensordict_collate_fn,\n",
    ")\n",
    "\n",
    "batch = next(iter(gen_loader))\n",
    "print(f\"On-the-fly batch locs shape: {batch['locs'].shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 5. Saving and Loading Datasets\n",
    "\n",
    "Datasets can be saved to disk for reproducible experiments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tempfile\n",
    "\n",
    "# Generate a dataset\n",
    "td_save = gen(batch_size=64)\n",
    "\n",
    "# Save to a temporary file\n",
    "with tempfile.NamedTemporaryFile(suffix=\".pt\", delete=False) as f:\n",
    "    save_path = f.name\n",
    "    torch.save(td_save, save_path)\n",
    "    print(f\"Saved dataset to: {save_path}\")\n",
    "    print(f\"File size: {os.path.getsize(save_path) / 1024:.1f} KB\")\n",
    "\n",
    "# Load it back\n",
    "td_loaded = torch.load(save_path, weights_only=False)\n",
    "print(f\"\\nLoaded dataset:\")\n",
    "print(f\"  Batch size: {td_loaded.batch_size}\")\n",
    "print(f\"  Keys: {list(td_loaded.keys())}\")\n",
    "\n",
    "# Verify equality\n",
    "assert torch.allclose(td_save[\"locs\"], td_loaded[\"locs\"])\n",
    "print(\"\\nDatasets match!\")\n",
    "\n",
    "# Cleanup\n",
    "os.unlink(save_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 6. Putting It All Together: Generate a Complete Dataset\n",
    "\n",
    "Let's create a full training/validation/test dataset split."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_dataset_split(gen, train_size=512, val_size=128, test_size=128, seed=42):\n",
    "    \"\"\"Generate train/val/test splits with different seeds.\"\"\"\n",
    "    torch.manual_seed(seed)\n",
    "    train_data = gen(batch_size=train_size)\n",
    "\n",
    "    torch.manual_seed(seed + 1)\n",
    "    val_data = gen(batch_size=val_size)\n",
    "\n",
    "    torch.manual_seed(seed + 2)\n",
    "    test_data = gen(batch_size=test_size)\n",
    "\n",
    "    return train_data, val_data, test_data\n",
    "\n",
    "\n",
    "# Generate splits for VRPP with 20 nodes\n",
    "gen_20 = VRPPGenerator(num_loc=20, loc_distribution=\"uniform\")\n",
    "train_data, val_data, test_data = generate_dataset_split(gen_20)\n",
    "\n",
    "print(\"Dataset splits:\")\n",
    "print(f\"  Train: {train_data.batch_size[0]} instances\")\n",
    "print(f\"  Val:   {val_data.batch_size[0]} instances\")\n",
    "print(f\"  Test:  {test_data.batch_size[0]} instances\")\n",
    "\n",
    "# Verify no overlap by checking first instance locations\n",
    "assert not torch.allclose(train_data[\"locs\"][0], val_data[\"locs\"][0])\n",
    "print(\"\\nSplits are independent (no data leakage).\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summary statistics of the generated data\n",
    "def dataset_summary(td, name=\"Dataset\"):\n",
    "    \"\"\"Print summary statistics for a TensorDict dataset.\"\"\"\n",
    "    print(f\"\\n{name}:\")\n",
    "    print(f\"  Instances: {td.batch_size[0]}\")\n",
    "    print(f\"  Nodes per instance: {td['locs'].shape[1]}\")\n",
    "    print(f\"  Location range: [{td['locs'].min():.3f}, {td['locs'].max():.3f}]\")\n",
    "    if \"prize\" in td.keys():\n",
    "        print(f\"  Prize - mean: {td['prize'].mean():.3f}, std: {td['prize'].std():.3f}\")\n",
    "    if \"waste\" in td.keys():\n",
    "        print(f\"  Waste - mean: {td['waste'].mean():.3f}, std: {td['waste'].std():.3f}\")\n",
    "    if \"demand\" in td.keys():\n",
    "        print(f\"  Demand - mean: {td['demand'].mean():.3f}, std: {td['demand'].std():.3f}\")\n",
    "\n",
    "\n",
    "dataset_summary(train_data, \"Training Set\")\n",
    "dataset_summary(val_data, \"Validation Set\")\n",
    "dataset_summary(test_data, \"Test Set\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Summary\n",
    "\n",
    "In this tutorial, you learned:\n",
    "\n",
    "- **TensorDict** is the universal data format \u2014 a batched dictionary of tensors with keys like `locs`, `depot`, `waste`, `prize`, `capacity`\n",
    "- **Generators** (`VRPPGenerator`, `WCVRPGenerator`) create instances on-the-fly with configurable distributions (`uniform`, `normal`, `clustered`)\n",
    "- **VRPInstanceBuilder** provides batch dataset generation with additional distribution types (`gamma1-4`, `empirical`)\n",
    "- **Dataset classes** (`TensorDictDataset`, `GeneratorDataset`) integrate with PyTorch DataLoaders\n",
    "- Datasets can be **saved/loaded** with `torch.save`/`torch.load`\n",
    "\n",
    "### Next Steps\n",
    "\n",
    "Continue to **[Tutorial 2: RL Environments](02_environments.ipynb)** to learn how these problem instances are used in the reinforcement learning environment for training routing agents."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbformat_minor": 2,
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
