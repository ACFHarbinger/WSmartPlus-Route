{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tutorial 2: RL Environments and Problem Formulation\n",
    "\n",
    "**WSmart+ Route Tutorial Series**\n",
    "\n",
    "This tutorial covers the RL4CO-style environment abstraction used in WSmart+ Route. You'll learn:\n",
    "\n",
    "1. The **environment registry** and factory pattern\n",
    "2. **VRPPEnv** deep dive: reset, step, action masks, rewards\n",
    "3. **Manual rollout**: stepping through episodes\n",
    "4. **WCVRPEnv** and **CVRPPEnv** variants\n",
    "5. **Visualizing** routes and state evolution\n",
    "\n",
    "**Previous**: [01_data_generation.ipynb](01_data_generation.ipynb) | **Next**: [03_models_and_policies.ipynb](03_models_and_policies.ipynb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "PROJECT_ROOT = os.path.abspath(os.path.join(os.getcwd(), \"..\", \"..\"))\n",
    "if PROJECT_ROOT not in sys.path:\n",
    "    sys.path.insert(0, PROJECT_ROOT)\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 1. Environment Registry\n",
    "\n",
    "WSmart+ Route provides several environment types, all accessible through a unified registry."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from logic.src.envs import ENV_REGISTRY, get_env\n",
    "\n",
    "print(\"Available Environments:\")\n",
    "print(\"-\" * 50)\n",
    "for name, env_cls in ENV_REGISTRY.items():\n",
    "    print(f\"  {name:10s} -> {env_cls.__name__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Environment Types\n",
    "\n",
    "| Environment | Description | Key Features |\n",
    "|------------|-------------|--------------|\n",
    "| `vrpp` | Vehicle Routing with Profits | Maximize prize - travel cost |\n",
    "| `cvrpp` | Capacitated VRPP | VRPP + vehicle capacity limits |\n",
    "| `wcvrp` | Waste Collection VRP | Collect waste with fill levels |\n",
    "| `cwcvrp` | Capacitated WCVRP | WCVRP + capacity constraints |\n",
    "| `sdwcvrp` | Stochastic Demand WCVRP | Noisy demand observations |\n",
    "| `scwcvrp` | Selective Capacitated WCVRP | Selective collection under noise |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 2. VRPPEnv Deep Dive\n",
    "\n",
    "The `VRPPEnv` is the foundational environment. The agent selects which nodes to visit to maximize **total prize** minus **travel cost**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create VRPP environment with 20 nodes\n",
    "env = get_env(\"vrpp\", num_loc=20)\n",
    "\n",
    "# Generate instances using the environment's built-in generator\n",
    "td = env.generator(batch_size=4)\n",
    "print(\"Generated instance keys:\", list(td.keys()))\n",
    "print(f\"  locs:     {td['locs'].shape}  (batch, nodes, 2)\")\n",
    "print(f\"  depot:    {td['depot'].shape}     (batch, 2)\")\n",
    "print(f\"  waste:    {td['waste'].shape}    (batch, nodes)\")\n",
    "print(f\"  prize:    {td['prize'].shape}    (batch, nodes)\")\n",
    "\n",
    "# Reset environment - initializes episode state\n",
    "td = env.reset(td)\n",
    "print(\"\\nAfter reset, new keys added:\")\n",
    "new_keys = [k for k in td.keys() if k not in [\"locs\", \"depot\", \"waste\", \"prize\", \"capacity\", \"max_length\", \"max_waste\"]]\n",
    "for key in sorted(new_keys):\n",
    "    shape = td[key].shape\n",
    "    print(f\"  {key:20s} shape={shape}  value={td[key][0].tolist() if td[key][0].numel() <= 5 else '...'}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Action mask: which nodes can we visit?\n",
    "mask = td[\"action_mask\"]\n",
    "print(f\"Action mask shape: {mask.shape}\")\n",
    "print(f\"Action mask for instance 0: {mask[0].int().tolist()}\")\n",
    "print(f\"  Number of valid actions: {mask[0].sum().item()}\")\n",
    "print(f\"  Depot (index 0) is always reachable: {mask[0, 0].item()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Take a step: visit node 5\n",
    "td[\"action\"] = torch.tensor([5, 3, 7, 1])  # Different action per batch element\n",
    "result = env.step(td)\n",
    "\n",
    "# The step returns a dict with \"next\" containing the updated state\n",
    "td_next = result[\"next\"]\n",
    "print(\"After stepping to nodes [5, 3, 7, 1]:\")\n",
    "print(f\"  Current node: {td_next['current_node'].squeeze(-1).tolist()}\")\n",
    "print(f\"  Tour length:  {td_next['tour_length'].tolist()}\")\n",
    "print(f\"  Prize collected: {td_next['collected_prize'].tolist()}\")\n",
    "print(f\"  Done: {td_next['done'].squeeze(-1).tolist()}\")\n",
    "print(f\"  Visited nodes: {td_next['visited'][0].int().tolist()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Take another step: visit node 10\n",
    "td_next[\"action\"] = torch.tensor([10, 8, 2, 15])\n",
    "result2 = env.step(td_next)\n",
    "td_next2 = result2[\"next\"]\n",
    "\n",
    "print(\"After second step:\")\n",
    "print(f\"  Current node: {td_next2['current_node'].squeeze(-1).tolist()}\")\n",
    "print(f\"  Tour length:  {[f'{x:.4f}' for x in td_next2['tour_length'].tolist()]}\")\n",
    "print(f\"  Prize collected: {[f'{x:.4f}' for x in td_next2['collected_prize'].tolist()]}\")\n",
    "print(f\"  Visited count: {td_next2['visited'].sum(dim=-1).tolist()}\")\n",
    "\n",
    "# Return to depot to end episode\n",
    "td_next2[\"action\"] = torch.zeros(4, dtype=torch.long)  # Action 0 = depot\n",
    "result3 = env.step(td_next2)\n",
    "td_final = result3[\"next\"]\n",
    "print(f\"\\nAfter returning to depot:\")\n",
    "print(f\"  Done: {td_final['done'].squeeze(-1).tolist()}\")\n",
    "print(f\"  Reward: {td_final['reward'].squeeze(-1).tolist()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 3. Manual Rollout\n",
    "\n",
    "Let's step through complete episodes with different action selection strategies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_rollout(env, td, max_steps=100):\n",
    "    \"\"\"Execute a complete episode with random valid actions.\"\"\"\n",
    "    td = env.reset(td.clone())\n",
    "    actions = []\n",
    "    step = 0\n",
    "\n",
    "    while not td[\"done\"].all() and step < max_steps:\n",
    "        # Select random valid actions\n",
    "        mask = td[\"action_mask\"].float()\n",
    "        action = torch.multinomial(mask, 1).squeeze(-1)\n",
    "        actions.append(action.clone())\n",
    "        td[\"action\"] = action\n",
    "        td = env.step(td)[\"next\"]\n",
    "        step += 1\n",
    "\n",
    "    reward = td[\"reward\"].squeeze(-1) if \"reward\" in td.keys() else env._get_reward(td)\n",
    "    return td, actions, reward\n",
    "\n",
    "\n",
    "def greedy_nearest_rollout(env, td, max_steps=100):\n",
    "    \"\"\"Greedy nearest-neighbor heuristic.\"\"\"\n",
    "    td = env.reset(td.clone())\n",
    "    actions = []\n",
    "    step = 0\n",
    "\n",
    "    while not td[\"done\"].all() and step < max_steps:\n",
    "        mask = td[\"action_mask\"]  # (batch, nodes)\n",
    "        current = td[\"current_node\"].squeeze(-1)  # (batch,)\n",
    "        locs = td[\"locs\"]  # (batch, nodes, 2)\n",
    "\n",
    "        # Compute distances from current node to all nodes\n",
    "        current_loc = locs.gather(1, current[:, None, None].expand(-1, -1, 2)).squeeze(1)\n",
    "        distances = torch.norm(locs - current_loc.unsqueeze(1), dim=-1)  # (batch, nodes)\n",
    "\n",
    "        # Mask invalid nodes with large distance\n",
    "        distances = distances.masked_fill(~mask, float(\"inf\"))\n",
    "\n",
    "        # Don't go to depot unless it's the only option\n",
    "        non_depot_mask = mask.clone()\n",
    "        non_depot_mask[:, 0] = False\n",
    "        has_non_depot = non_depot_mask.any(dim=-1)\n",
    "        distances[:, 0] = torch.where(has_non_depot, torch.tensor(float(\"inf\")), distances[:, 0])\n",
    "\n",
    "        action = distances.argmin(dim=-1)\n",
    "        actions.append(action.clone())\n",
    "        td[\"action\"] = action\n",
    "        td = env.step(td)[\"next\"]\n",
    "        step += 1\n",
    "\n",
    "    reward = td[\"reward\"].squeeze(-1) if \"reward\" in td.keys() else env._get_reward(td)\n",
    "    return td, actions, reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare random vs greedy nearest-neighbor\n",
    "env = get_env(\"vrpp\", num_loc=20)\n",
    "td_test = env.generator(batch_size=32)\n",
    "\n",
    "td_rand, actions_rand, reward_rand = random_rollout(env, td_test)\n",
    "td_greedy, actions_greedy, reward_greedy = greedy_nearest_rollout(env, td_test)\n",
    "\n",
    "print(\"Rollout Comparison (32 instances, 20 nodes):\")\n",
    "print(f\"  Random  - Mean reward: {reward_rand.mean():.4f} (+/- {reward_rand.std():.4f})\")\n",
    "print(f\"  Greedy  - Mean reward: {reward_greedy.mean():.4f} (+/- {reward_greedy.std():.4f})\")\n",
    "print(f\"  Greedy improvement: {((reward_greedy.mean() - reward_rand.mean()) / reward_rand.mean().abs() * 100):.1f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_tour(td_initial, td_final, actions, idx=0, title=\"Tour\"):\n",
    "    \"\"\"Visualize a completed tour.\"\"\"\n",
    "    locs = td_initial[\"locs\"][idx].numpy()\n",
    "    depot = td_initial[\"depot\"][idx].numpy()\n",
    "    prizes = td_initial[\"prize\"][idx].numpy()\n",
    "    visited = td_final[\"visited\"][idx].numpy()\n",
    "\n",
    "    fig, ax = plt.subplots(1, 1, figsize=(8, 7))\n",
    "\n",
    "    # Plot unvisited nodes\n",
    "    unvisited_mask = ~visited[1:]  # Exclude depot\n",
    "    if unvisited_mask.any():\n",
    "        ax.scatter(locs[unvisited_mask, 0], locs[unvisited_mask, 1],\n",
    "                   c=\"lightgray\", s=60, edgecolors=\"gray\", linewidth=0.5, zorder=2, label=\"Unvisited\")\n",
    "\n",
    "    # Plot visited nodes colored by prize\n",
    "    visited_mask = visited[1:]\n",
    "    if visited_mask.any():\n",
    "        sc = ax.scatter(locs[visited_mask, 0], locs[visited_mask, 1],\n",
    "                        c=prizes[visited_mask], cmap=\"YlOrRd\", s=80,\n",
    "                        edgecolors=\"black\", linewidth=0.5, zorder=3, label=\"Visited\")\n",
    "        plt.colorbar(sc, ax=ax, label=\"Prize\", shrink=0.8)\n",
    "\n",
    "    # Plot depot\n",
    "    ax.scatter(depot[0], depot[1], c=\"blue\", s=200, marker=\"s\",\n",
    "               edgecolors=\"black\", linewidth=1.5, zorder=4, label=\"Depot\")\n",
    "\n",
    "    # Draw tour path\n",
    "    tour_nodes = [a[idx].item() for a in actions]\n",
    "    all_locs = np.vstack([depot.reshape(1, 2), locs])  # depot at index 0\n",
    "    path = [0] + tour_nodes  # Start from depot\n",
    "\n",
    "    for i in range(len(path) - 1):\n",
    "        start = all_locs[path[i]]\n",
    "        end = all_locs[path[i + 1]]\n",
    "        ax.annotate(\"\", xy=end, xytext=start,\n",
    "                     arrowprops=dict(arrowstyle=\"->\", color=\"steelblue\", lw=1.5))\n",
    "\n",
    "    reward = td_final[\"reward\"][idx].item() if \"reward\" in td_final.keys() else 0\n",
    "    ax.set_title(f\"{title}\\nReward: {reward:.4f} | Nodes visited: {visited.sum() - 1}\")\n",
    "    ax.set_xlabel(\"X\")\n",
    "    ax.set_ylabel(\"Y\")\n",
    "    ax.legend(loc=\"upper right\")\n",
    "    ax.set_xlim(-0.05, 1.05)\n",
    "    ax.set_ylim(-0.05, 1.05)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "# Visualize random vs greedy tour for same instance\n",
    "plot_tour(td_test, td_rand, actions_rand, idx=0, title=\"Random Policy Tour\")\n",
    "plot_tour(td_test, td_greedy, actions_greedy, idx=0, title=\"Greedy Nearest-Neighbor Tour\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 4. Waste Collection VRP Environment\n",
    "\n",
    "The `WCVRPEnv` models waste collection where bins fill over time. The objective balances **waste collected** against **travel cost** and **overflow penalties**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create waste collection environment\n",
    "wc_env = get_env(\"wcvrp\", num_loc=20)\n",
    "td_wc = wc_env.generator(batch_size=8)\n",
    "td_wc = wc_env.reset(td_wc)\n",
    "\n",
    "print(\"WCVRP State after reset:\")\n",
    "for key in sorted(td_wc.keys()):\n",
    "    shape = td_wc[key].shape\n",
    "    print(f\"  {key:20s} {str(shape):20s}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run greedy rollout on WCVRP\n",
    "td_wc_test = wc_env.generator(batch_size=32)\n",
    "td_wc_final, actions_wc, reward_wc = greedy_nearest_rollout(wc_env, td_wc_test)\n",
    "\n",
    "print(\"WCVRP Greedy Rollout Results:\")\n",
    "print(f\"  Mean reward: {reward_wc.mean():.4f}\")\n",
    "print(f\"  Std reward:  {reward_wc.std():.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 5. Capacitated VRPP Environment\n",
    "\n",
    "The `CVRPPEnv` adds **vehicle capacity constraints** to VRPP. The vehicle can only carry a limited amount of waste/goods before returning to the depot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create capacitated environment\n",
    "c_env = get_env(\"cvrpp\", num_loc=20)\n",
    "td_c = c_env.generator(batch_size=4)\n",
    "td_c = c_env.reset(td_c)\n",
    "\n",
    "print(\"CVRPP State after reset:\")\n",
    "print(f\"  remaining_capacity: {td_c['remaining_capacity'].tolist()}\")\n",
    "print(f\"  collected: {td_c['collected'].tolist()}\")\n",
    "\n",
    "# Step to a node - capacity decreases\n",
    "td_c[\"action\"] = torch.tensor([3, 5, 7, 2])\n",
    "td_c_next = c_env.step(td_c)[\"next\"]\n",
    "print(f\"\\nAfter visiting a node:\")\n",
    "print(f\"  remaining_capacity: {[f'{x:.4f}' for x in td_c_next['remaining_capacity'].tolist()]}\")\n",
    "print(f\"  collected: {[f'{x:.4f}' for x in td_c_next['collected'].tolist()]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 6. Environment Configuration\n",
    "\n",
    "Environments can be configured programmatically using `EnvConfig` dataclass."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from logic.src.configs import EnvConfig\n",
    "\n",
    "# Default configuration\n",
    "default_cfg = EnvConfig()\n",
    "print(\"Default EnvConfig:\")\n",
    "for field_name in [\"name\", \"num_loc\", \"capacity\", \"cost_weight\", \"prize_weight\",\n",
    "                   \"overflow_penalty\", \"collection_reward\"]:\n",
    "    print(f\"  {field_name}: {getattr(default_cfg, field_name)}\")\n",
    "\n",
    "# Custom configuration\n",
    "custom_cfg = EnvConfig(\n",
    "    name=\"vrpp\",\n",
    "    num_loc=50,\n",
    "    cost_weight=2.0,     # Penalize distance more\n",
    "    prize_weight=1.0,\n",
    ")\n",
    "print(f\"\\nCustom config: {custom_cfg.num_loc} nodes, cost_weight={custom_cfg.cost_weight}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show how reward weights affect behavior\n",
    "env_balanced = get_env(\"vrpp\", num_loc=20, prize_weight=1.0, cost_weight=1.0)\n",
    "env_prize_focus = get_env(\"vrpp\", num_loc=20, prize_weight=2.0, cost_weight=0.5)\n",
    "env_cost_focus = get_env(\"vrpp\", num_loc=20, prize_weight=0.5, cost_weight=2.0)\n",
    "\n",
    "td_compare = env_balanced.generator(batch_size=64)\n",
    "\n",
    "configs = [\n",
    "    (\"Balanced (1:1)\", env_balanced),\n",
    "    (\"Prize Focus (2:0.5)\", env_prize_focus),\n",
    "    (\"Cost Focus (0.5:2)\", env_cost_focus),\n",
    "]\n",
    "\n",
    "print(\"Reward Weight Comparison (greedy nearest-neighbor on 64 instances):\")\n",
    "print(\"-\" * 55)\n",
    "for name, env_cfg in configs:\n",
    "    _, _, rewards = greedy_nearest_rollout(env_cfg, td_compare)\n",
    "    print(f\"  {name:25s} Mean reward: {rewards.mean():.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Summary\n",
    "\n",
    "In this tutorial, you learned:\n",
    "\n",
    "- **Environment registry** provides 6 problem types accessible via `get_env(name)`\n",
    "- **VRPPEnv** manages state (visited nodes, tour length, prize collected) through `reset()` and `step()` calls\n",
    "- **Action masks** prevent invalid actions (already visited nodes)\n",
    "- **Manual rollouts** with random and greedy nearest-neighbor strategies\n",
    "- **WCVRPEnv** and **CVRPPEnv** add waste collection and capacity constraints\n",
    "- **Reward weights** (`prize_weight`, `cost_weight`) control the optimization objective\n",
    "\n",
    "### Next Steps\n",
    "\n",
    "Continue to **[Tutorial 3: Neural Models and Classical Policies](03_models_and_policies.ipynb)** to learn how trained neural networks and classical optimization algorithms solve these routing problems."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbformat_minor": 2,
   "pygments_lexer": "ipython3",
   "version": "3.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
