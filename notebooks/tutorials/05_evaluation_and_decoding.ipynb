{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Tutorial 5: Evaluation and Decoding Strategies\n",
                "\n",
                "**WSmart+ Route Tutorial Series**\n",
                "\n",
                "This tutorial covers how to evaluate trained models and explore different decoding strategies. You'll learn:\n",
                "\n",
                "1. **Training a small model** for evaluation demos\n",
                "2. **Decoding strategies**: greedy, sampling, multi-sample\n",
                "3. **Temperature scaling** for sampling\n",
                "4. **Evaluation metrics** and cost decomposition\n",
                "5. **Visualizing tours** and comparing solution quality\n",
                "\n",
                "**Previous**: [04_training_with_lightning.ipynb](04_training_with_lightning.ipynb) | **Next**: [06_simulation_testing.ipynb](06_simulation_testing.ipynb)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import os\n",
                "import sys\n",
                "import warnings\n",
                "\n",
                "warnings.filterwarnings(\"ignore\")\n",
                "\n",
                "PROJECT_ROOT = os.path.abspath(os.path.join(os.getcwd(), \"..\", \"..\"))\n",
                "if PROJECT_ROOT not in sys.path:\n",
                "    sys.path.insert(0, PROJECT_ROOT)\n",
                "\n",
                "import matplotlib.pyplot as plt\n",
                "import numpy as np\n",
                "import torch\n",
                "\n",
                "torch.manual_seed(42)\n",
                "np.random.seed(42)\n",
                "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
                "print(f\"Using device: {device}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "---\n",
                "## 1. Quick Training Setup\n",
                "\n",
                "First, let's train a small model so we have something to evaluate. We'll use a short training run with REINFORCE."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "from logic.src.envs import get_env\n",
                "from logic.src.models.policies import AttentionModelPolicy\n",
                "from logic.src.pipeline.rl import REINFORCE\n",
                "from logic.src.pipeline.rl.common.trainer import WSTrainer\n",
                "\n",
                "# Create components\n",
                "env = get_env(\"vrpp\", num_loc=20)\n",
                "policy = AttentionModelPolicy(\n",
                "    env_name=\"vrpp\", embed_dim=64, n_encode_layers=2, n_decode_layers=2, n_heads=4,\n",
                ")\n",
                "\n",
                "model = REINFORCE(\n",
                "    env=env, policy=policy, baseline=\"exponential\",\n",
                "    optimizer=\"adam\", optimizer_kwargs={\"lr\": 1e-4},\n",
                "    train_data_size=1280, val_data_size=256, batch_size=64, num_workers=0,\n",
                ")\n",
                "\n",
                "# Quick training\n",
                "trainer = WSTrainer(\n",
                "    max_epochs=5, accelerator=\"cpu\", devices=1, precision=\"32-true\",\n",
                "    log_every_n_steps=5, enable_progress_bar=True, logger=False,\n",
                "    reload_dataloaders_every_n_epochs=1,\n",
                ")\n",
                "\n",
                "print(\"Training model for evaluation demos (5 epochs)...\")\n",
                "trainer.fit(model)\n",
                "print(\"Training complete!\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Generate a fixed test dataset\n",
                "torch.manual_seed(999)  # Different seed from training\n",
                "test_data = env.generator(batch_size=128)\n",
                "print(f\"Test dataset: {test_data.batch_size[0]} instances, {test_data['locs'].shape[1]} nodes\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "---\n",
                "## 2. Decoding Strategies\n",
                "\n",
                "The decoder converts the neural network's probability distribution over nodes into actual route selections. Different strategies trade off speed vs solution quality."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Strategy 1: Greedy - always pick the highest probability node\n",
                "td_test = env.reset(test_data.clone())\n",
                "\n",
                "with torch.no_grad():\n",
                "    out_greedy = policy(td_test, env, strategy=\"greedy\", return_actions=True)\n",
                "\n",
                "print(\"Greedy Decoding:\")\n",
                "print(f\"  Mean reward:  {out_greedy['reward'].mean():.4f}\")\n",
                "print(f\"  Std reward:   {out_greedy['reward'].std():.4f}\")\n",
                "print(f\"  Best reward:  {out_greedy['reward'].max():.4f}\")\n",
                "print(f\"  Worst reward: {out_greedy['reward'].min():.4f}\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Strategy 2: Sampling - sample from the probability distribution\n",
                "td_test = env.reset(test_data.clone())\n",
                "\n",
                "with torch.no_grad():\n",
                "    out_sample = policy(td_test, env, strategy=\"sampling\", return_actions=True)\n",
                "\n",
                "print(\"Sampling Decoding (single sample):\")\n",
                "print(f\"  Mean reward:  {out_sample['reward'].mean():.4f}\")\n",
                "print(f\"  Std reward:   {out_sample['reward'].std():.4f}\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Strategy 3: Multi-sample - run N samples, keep the best for each instance\n",
                "def multi_sample_eval(policy, env, test_data, n_samples=8):\n",
                "    \"\"\"Run multiple sampling rollouts and keep the best solution per instance.\"\"\"\n",
                "    all_rewards = []\n",
                "    all_actions = []\n",
                "\n",
                "    for _ in range(n_samples):\n",
                "        td = env.reset(test_data.clone())\n",
                "        with torch.no_grad():\n",
                "            out = policy(td, env, strategy=\"sampling\", return_actions=True)\n",
                "        all_rewards.append(out[\"reward\"])\n",
                "        all_actions.append(out[\"actions\"])\n",
                "\n",
                "    # Stack and find best per instance\n",
                "    stacked_rewards = torch.stack(all_rewards, dim=0)  # (n_samples, batch)\n",
                "    best_idx = stacked_rewards.argmax(dim=0)  # (batch,)\n",
                "    best_rewards = stacked_rewards.max(dim=0).values\n",
                "\n",
                "    return best_rewards, stacked_rewards\n",
                "\n",
                "\n",
                "# Compare different sample counts\n",
                "n_samples_list = [1, 2, 4, 8, 16, 32, 64]\n",
                "multi_results = {}\n",
                "\n",
                "for n in n_samples_list:\n",
                "    best_rewards, _ = multi_sample_eval(policy, env, test_data, n_samples=n)\n",
                "    multi_results[n] = best_rewards.mean().item()\n",
                "    print(f\"  {n:3d} samples -> Mean best reward: {multi_results[n]:.4f}\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "fig, ax = plt.subplots(figsize=(9, 5))\n",
                "\n",
                "ax.plot(n_samples_list, list(multi_results.values()), \"o-\", linewidth=2,\n",
                "        markersize=8, color=\"steelblue\", label=\"Multi-sample best\")\n",
                "ax.axhline(y=out_greedy[\"reward\"].mean().item(), color=\"red\", linestyle=\"--\",\n",
                "           linewidth=1.5, label=f\"Greedy: {out_greedy['reward'].mean():.4f}\")\n",
                "ax.set_xlabel(\"Number of Samples\", fontsize=12)\n",
                "ax.set_ylabel(\"Mean Best Reward\", fontsize=12)\n",
                "ax.set_title(\"Reward vs Number of Samples\")\n",
                "ax.legend(fontsize=11)\n",
                "ax.grid(True, alpha=0.3)\n",
                "ax.set_xscale(\"log\", base=2)\n",
                "ax.set_xticks(n_samples_list)\n",
                "ax.set_xticklabels(n_samples_list)\n",
                "\n",
                "plt.tight_layout()\n",
                "plt.show()\n",
                "\n",
                "print(\"Multi-sampling trades computation time for solution quality.\")\n",
                "print(\"Greedy gives a strong deterministic baseline; sampling with enough\")\n",
                "print(\"trials can exceed it by exploring diverse solutions.\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "---\n",
                "## 3. Temperature Scaling\n",
                "\n",
                "Temperature controls the randomness of sampling. Lower temperatures make the distribution sharper (more greedy-like), while higher temperatures increase exploration."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "temperatures = [0.1, 0.5, 1.0, 2.0, 5.0]\n",
                "temp_results = {}\n",
                "\n",
                "for temp in temperatures:\n",
                "    td = env.reset(test_data.clone())\n",
                "    with torch.no_grad():\n",
                "        out = policy(td, env, strategy=\"sampling\", softmax_temp=temp, return_actions=True)\n",
                "    temp_results[temp] = out[\"reward\"].mean().item()\n",
                "\n",
                "print(\"Temperature Scaling (single sample):\")\n",
                "print(f\"{'Temperature':<15} {'Mean Reward':<15}\")\n",
                "print(\"-\" * 30)\n",
                "for temp, reward in temp_results.items():\n",
                "    print(f\"{temp:<15.1f} {reward:<15.4f}\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "fig, ax = plt.subplots(figsize=(8, 5))\n",
                "\n",
                "temps = list(temp_results.keys())\n",
                "rewards = list(temp_results.values())\n",
                "\n",
                "ax.plot(temps, rewards, \"s-\", linewidth=2, markersize=8, color=\"coral\")\n",
                "ax.axhline(y=out_greedy[\"reward\"].mean().item(), color=\"steelblue\", linestyle=\"--\",\n",
                "           linewidth=1.5, label=f\"Greedy: {out_greedy['reward'].mean():.4f}\")\n",
                "ax.set_xlabel(\"Temperature\", fontsize=12)\n",
                "ax.set_ylabel(\"Mean Reward (single sample)\", fontsize=12)\n",
                "ax.set_title(\"Effect of Softmax Temperature on Sampling Quality\")\n",
                "ax.legend(fontsize=11)\n",
                "ax.grid(True, alpha=0.3)\n",
                "\n",
                "plt.tight_layout()\n",
                "plt.show()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "---\n",
                "## 4. Evaluation Metrics\n",
                "\n",
                "Beyond raw reward, we can decompose performance into meaningful metrics."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Evaluate and decompose metrics\n",
                "td_metrics = env.reset(test_data.clone())\n",
                "\n",
                "with torch.no_grad():\n",
                "    out = policy(td_metrics, env, strategy=\"greedy\", return_actions=True)\n",
                "\n",
                "# Access the final state from the environment\n",
                "td_final = out.get(\"td\", td_metrics)\n",
                "\n",
                "print(\"Evaluation Metrics (128 test instances, greedy decoding):\")\n",
                "print(\"=\" * 50)\n",
                "print(f\"  Reward:          {out['reward'].mean():.4f} +/- {out['reward'].std():.4f}\")\n",
                "print(f\"  Log-likelihood:  {out['log_likelihood'].mean():.4f}\")\n",
                "\n",
                "if \"entropy\" in out:\n",
                "    print(f\"  Entropy:         {out['entropy'].mean():.4f}\")\n",
                "\n",
                "# Tour statistics from actions\n",
                "actions = out[\"actions\"]\n",
                "tour_lengths = (actions != 0).sum(dim=-1).float()  # Non-depot visits\n",
                "print(f\"  Avg tour length: {tour_lengths.mean():.1f} nodes\")\n",
                "print(f\"  Min tour length: {tour_lengths.min():.0f} nodes\")\n",
                "print(f\"  Max tour length: {tour_lengths.max():.0f} nodes\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
                "\n",
                "# Left: Reward distribution\n",
                "ax = axes[0]\n",
                "rewards = out[\"reward\"].numpy()\n",
                "ax.hist(rewards, bins=25, alpha=0.7, color=\"steelblue\", edgecolor=\"white\")\n",
                "ax.axvline(x=rewards.mean(), color=\"red\", linestyle=\"--\", label=f\"Mean: {rewards.mean():.4f}\")\n",
                "ax.set_xlabel(\"Reward\")\n",
                "ax.set_ylabel(\"Count\")\n",
                "ax.set_title(\"Reward Distribution (Greedy)\")\n",
                "ax.legend()\n",
                "\n",
                "# Right: Tour length distribution\n",
                "ax = axes[1]\n",
                "lengths = tour_lengths.numpy()\n",
                "ax.hist(lengths, bins=15, alpha=0.7, color=\"coral\", edgecolor=\"white\")\n",
                "ax.axvline(x=lengths.mean(), color=\"red\", linestyle=\"--\", label=f\"Mean: {lengths.mean():.1f}\")\n",
                "ax.set_xlabel(\"Tour Length (nodes visited)\")\n",
                "ax.set_ylabel(\"Count\")\n",
                "ax.set_title(\"Tour Length Distribution\")\n",
                "ax.legend()\n",
                "\n",
                "plt.tight_layout()\n",
                "plt.show()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "---\n",
                "## 5. Tour Visualization"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "def plot_solution(test_data, actions, idx=0, title=\"Solution\"):\n",
                "    \"\"\"Plot a solution tour.\"\"\"\n",
                "    locs = test_data[\"locs\"][idx].numpy()\n",
                "    depot = test_data[\"depot\"][idx].numpy()\n",
                "    prizes = test_data[\"prize\"][idx].numpy()\n",
                "    tour = actions[idx].numpy()\n",
                "\n",
                "    # Remove trailing zeros (padding)\n",
                "    tour = tour[tour != 0] if (tour != 0).any() else tour[:1]\n",
                "\n",
                "    fig, ax = plt.subplots(figsize=(8, 7))\n",
                "\n",
                "    # All locations\n",
                "    all_locs = np.vstack([depot.reshape(1, 2), locs])\n",
                "\n",
                "    # Determine visited nodes\n",
                "    visited_set = set(tour.tolist())\n",
                "    unvisited = [i for i in range(1, len(locs) + 1) if i not in visited_set]\n",
                "    visited = [i for i in range(1, len(locs) + 1) if i in visited_set]\n",
                "\n",
                "    # Plot unvisited\n",
                "    if unvisited:\n",
                "        uv_locs = all_locs[unvisited]\n",
                "        ax.scatter(uv_locs[:, 0], uv_locs[:, 1], c=\"lightgray\", s=50,\n",
                "                   edgecolors=\"gray\", linewidth=0.5, zorder=2, label=\"Unvisited\")\n",
                "\n",
                "    # Plot visited colored by prize\n",
                "    if visited:\n",
                "        v_locs = all_locs[visited]\n",
                "        v_prizes = prizes[[v - 1 for v in visited]]\n",
                "        sc = ax.scatter(v_locs[:, 0], v_locs[:, 1], c=v_prizes, cmap=\"YlOrRd\",\n",
                "                        s=80, edgecolors=\"black\", linewidth=0.5, zorder=3, label=\"Visited\")\n",
                "        plt.colorbar(sc, ax=ax, label=\"Prize\", shrink=0.8)\n",
                "\n",
                "    # Depot\n",
                "    ax.scatter(depot[0], depot[1], c=\"blue\", s=200, marker=\"s\",\n",
                "               edgecolors=\"black\", linewidth=1.5, zorder=4, label=\"Depot\")\n",
                "\n",
                "    # Draw path\n",
                "    path = [0] + tour.tolist() + [0]\n",
                "    for i in range(len(path) - 1):\n",
                "        start = all_locs[path[i]]\n",
                "        end = all_locs[path[i + 1]]\n",
                "        ax.annotate(\"\", xy=end, xytext=start,\n",
                "                     arrowprops=dict(arrowstyle=\"->\", color=\"steelblue\", lw=1.5))\n",
                "\n",
                "    # Add node labels\n",
                "    for i, (x, y) in enumerate(all_locs):\n",
                "        if i == 0:\n",
                "            continue\n",
                "        ax.annotate(str(i), (x, y), textcoords=\"offset points\",\n",
                "                    xytext=(5, 5), fontsize=7, alpha=0.6)\n",
                "\n",
                "    ax.set_title(title)\n",
                "    ax.set_xlabel(\"X\")\n",
                "    ax.set_ylabel(\"Y\")\n",
                "    ax.legend(loc=\"upper right\", fontsize=9)\n",
                "    ax.set_xlim(-0.05, 1.05)\n",
                "    ax.set_ylim(-0.05, 1.05)\n",
                "    plt.tight_layout()\n",
                "    plt.show()\n",
                "\n",
                "\n",
                "# Visualize best and worst solutions\n",
                "best_idx = out[\"reward\"].argmax().item()\n",
                "worst_idx = out[\"reward\"].argmin().item()\n",
                "\n",
                "plot_solution(test_data, out[\"actions\"], idx=best_idx,\n",
                "              title=f\"Best Solution (reward: {out['reward'][best_idx]:.4f})\")\n",
                "plot_solution(test_data, out[\"actions\"], idx=worst_idx,\n",
                "              title=f\"Worst Solution (reward: {out['reward'][worst_idx]:.4f})\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "---\n",
                "## 6. Greedy vs Sampling Side-by-Side"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Get greedy and sampling solutions for same instance\n",
                "idx = 0\n",
                "\n",
                "td_g = env.reset(test_data.clone())\n",
                "td_s = env.reset(test_data.clone())\n",
                "\n",
                "with torch.no_grad():\n",
                "    out_g = policy(td_g, env, strategy=\"greedy\", return_actions=True)\n",
                "    out_s = policy(td_s, env, strategy=\"sampling\", return_actions=True)\n",
                "\n",
                "fig, axes = plt.subplots(1, 2, figsize=(16, 7))\n",
                "\n",
                "for ax, (out_d, decode_name) in zip(axes, [(out_g, \"Greedy\"), (out_s, \"Sampling\")]):\n",
                "    locs = test_data[\"locs\"][idx].numpy()\n",
                "    depot = test_data[\"depot\"][idx].numpy()\n",
                "    tour = out_d[\"actions\"][idx].numpy()\n",
                "    tour = tour[tour != 0] if (tour != 0).any() else tour[:1]\n",
                "    all_locs = np.vstack([depot.reshape(1, 2), locs])\n",
                "\n",
                "    ax.scatter(locs[:, 0], locs[:, 1], c=\"lightblue\", s=50, edgecolors=\"gray\", linewidth=0.5, zorder=2)\n",
                "    ax.scatter(depot[0], depot[1], c=\"blue\", s=200, marker=\"s\", edgecolors=\"black\", linewidth=1.5, zorder=4)\n",
                "\n",
                "    path = [0] + tour.tolist() + [0]\n",
                "    for i in range(len(path) - 1):\n",
                "        start = all_locs[path[i]]\n",
                "        end = all_locs[path[i + 1]]\n",
                "        ax.annotate(\"\", xy=end, xytext=start,\n",
                "                     arrowprops=dict(arrowstyle=\"->\", color=\"steelblue\", lw=1.5))\n",
                "\n",
                "    reward = out_d[\"reward\"][idx].item()\n",
                "    ax.set_title(f\"{decode_name} (reward: {reward:.4f})\")\n",
                "    ax.set_xlim(-0.05, 1.05)\n",
                "    ax.set_ylim(-0.05, 1.05)\n",
                "    ax.set_aspect(\"equal\")\n",
                "\n",
                "plt.suptitle(\"Greedy vs Sampling Decoding\", fontsize=14)\n",
                "plt.tight_layout()\n",
                "plt.show()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "---\n",
                "## 7. Strategy Selection Guide\n",
                "\n",
                "| Strategy | Speed | Quality | Use Case |\n",
                "|----------|-------|---------|----------|\n",
                "| Greedy | Fast | Good baseline | Real-time deployment, quick evaluation |\n",
                "| Sampling (1x) | Fast | Variable | Training (exploration needed) |\n",
                "| Sampling (Nx) | N x slower | Better than greedy | When quality matters more than speed |\n",
                "| Low temperature | Fast | Near-greedy | Fine-tuning exploration-exploitation |"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "---\n",
                "## Summary\n",
                "\n",
                "In this tutorial, you learned:\n",
                "\n",
                "- **Greedy decoding** always picks the most likely node - fast and deterministic\n",
                "- **Sampling decoding** introduces randomness for exploration during training\n",
                "- **Multi-sample** (best-of-N) improves quality at the cost of N times more computation\n",
                "- **Temperature** controls the sharpness of sampling distributions\n",
                "- **Evaluation metrics** include reward, tour length, and per-instance statistics\n",
                "- **Tour visualization** helps understand model behavior and solution quality\n",
                "\n",
                "### Next Steps\n",
                "\n",
                "Continue to **[Tutorial 6: Multi-Day Simulation Testing](06_simulation_testing.ipynb)** to learn how to test routing policies (both neural and classical) in realistic multi-day waste collection scenarios."
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.10.12"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 5
}
