{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Tutorial 4: Training with PyTorch Lightning\n",
                "\n",
                "**WSmart+ Route Tutorial Series**\n",
                "\n",
                "This tutorial covers end-to-end training of neural routing models using reinforcement learning. You'll learn:\n",
                "\n",
                "1. The **configuration system** (`Config`, `EnvConfig`, `ModelConfig`, etc.)\n",
                "2. **REINFORCE** training with different baselines\n",
                "3. **PPO** training with a critic network\n",
                "4. The `create_model()` high-level helper\n",
                "5. **Monitoring** training progress\n",
                "6. **Saving and loading** trained models\n",
                "\n",
                "**Previous**: [03_models_and_policies.ipynb](03_models_and_policies.ipynb) | **Next**: [05_evaluation_and_decoding.ipynb](05_evaluation_and_decoding.ipynb)\n",
                "\n",
                "> **Note**: This tutorial uses small problem sizes and few epochs for fast execution on CPU. For real training, use GPU with larger settings."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import os\n",
                "import sys\n",
                "import warnings\n",
                "\n",
                "warnings.filterwarnings(\"ignore\")\n",
                "\n",
                "PROJECT_ROOT = os.path.abspath(os.path.join(os.getcwd(), \"..\", \"..\"))\n",
                "if PROJECT_ROOT not in sys.path:\n",
                "    sys.path.insert(0, PROJECT_ROOT)\n",
                "\n",
                "import matplotlib.pyplot as plt\n",
                "import numpy as np\n",
                "import torch\n",
                "\n",
                "torch.manual_seed(42)\n",
                "np.random.seed(42)\n",
                "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
                "print(f\"Using device: {device}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "---\n",
                "## 1. Configuration System\n",
                "\n",
                "WSmart+ Route uses a hierarchy of dataclasses to configure all aspects of training."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "from logic.src.configs import Config, EnvConfig, ModelConfig, OptimConfig, RLConfig, TrainConfig\n",
                "\n",
                "# Create a configuration for a small training run\n",
                "cfg = Config(\n",
                "    env=EnvConfig(\n",
                "        name=\"vrpp\",\n",
                "        num_loc=20,\n",
                "    ),\n",
                "    model=ModelConfig(\n",
                "        name=\"am\",\n",
                "        embed_dim=64,\n",
                "        num_encoder_layers=2,\n",
                "        num_decoder_layers=2,\n",
                "        n_heads=4,\n",
                "    ),\n",
                "    train=TrainConfig(\n",
                "        n_epochs=3,\n",
                "        batch_size=64,\n",
                "        train_data_size=640,   # Small for tutorial\n",
                "        val_data_size=128,\n",
                "        num_workers=0,\n",
                "        precision=\"32-true\",   # Use full precision for CPU\n",
                "    ),\n",
                "    optim=OptimConfig(\n",
                "        optimizer=\"adam\",\n",
                "        lr=1e-4,\n",
                "    ),\n",
                "    rl=RLConfig(\n",
                "        algorithm=\"reinforce\",\n",
                "        baseline=\"rollout\",\n",
                "    ),\n",
                "    device=\"cpu\",\n",
                "    seed=42,\n",
                ")\n",
                "\n",
                "print(\"Configuration Summary:\")\n",
                "print(f\"  Environment:  {cfg.env.name} with {cfg.env.num_loc} nodes\")\n",
                "print(f\"  Model:        {cfg.model.name} (embed={cfg.model.embed_dim}, layers={cfg.model.num_encoder_layers})\")\n",
                "print(f\"  Training:     {cfg.train.n_epochs} epochs, batch_size={cfg.train.batch_size}\")\n",
                "print(f\"  RL Algorithm: {cfg.rl.algorithm} with {cfg.rl.baseline} baseline\")\n",
                "print(f\"  Optimizer:    {cfg.optim.optimizer}, lr={cfg.optim.lr}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "---\n",
                "## 2. Building Components Manually\n",
                "\n",
                "Before using the high-level `create_model()`, let's understand each component."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "from logic.src.envs import get_env\n",
                "from logic.src.models.policies import AttentionModelPolicy\n",
                "from logic.src.pipeline.rl import REINFORCE\n",
                "\n",
                "# Step 1: Create environment\n",
                "env = get_env(\"vrpp\", num_loc=20)\n",
                "print(f\"Environment: {env.name}\")\n",
                "\n",
                "# Step 2: Create policy (neural network)\n",
                "policy = AttentionModelPolicy(\n",
                "    env_name=\"vrpp\",\n",
                "    embed_dim=64,\n",
                "    n_encode_layers=2,\n",
                "    n_decode_layers=2,\n",
                "    n_heads=4,\n",
                ")\n",
                "print(f\"Policy: {type(policy).__name__} ({sum(p.numel() for p in policy.parameters()):,} params)\")\n",
                "\n",
                "# Step 3: Create RL module (REINFORCE)\n",
                "model = REINFORCE(\n",
                "    env=env,\n",
                "    policy=policy,\n",
                "    baseline=\"rollout\",\n",
                "    optimizer=\"adam\",\n",
                "    optimizer_kwargs={\"lr\": 1e-4},\n",
                "    train_data_size=640,\n",
                "    val_data_size=128,\n",
                "    batch_size=64,\n",
                "    num_workers=0,\n",
                ")\n",
                "print(f\"RL Module: {type(model).__name__}\")\n",
                "print(f\"  Baseline: {model.baseline_type}\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Evaluate before training\n",
                "td_eval = env.generator(batch_size=64)\n",
                "td_eval = env.reset(td_eval)\n",
                "\n",
                "with torch.no_grad():\n",
                "    out_before = policy(td_eval.clone(), env, strategy=\"greedy\", return_actions=True)\n",
                "\n",
                "print(f\"Before training (untrained model):\")\n",
                "print(f\"  Mean reward (greedy): {out_before['reward'].mean():.4f}\")\n",
                "print(f\"  Std reward:           {out_before['reward'].std():.4f}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "---\n",
                "## 3. REINFORCE Training\n",
                "\n",
                "The REINFORCE algorithm optimizes the policy using the gradient:\n",
                "\n",
                "$$\\nabla_\\theta J(\\theta) = \\mathbb{E}\\left[(R - b) \\nabla_\\theta \\log \\pi_\\theta(a|s)\\right]$$\n",
                "\n",
                "where $R$ is the reward and $b$ is the baseline for variance reduction."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "from logic.src.pipeline.rl.common.trainer import WSTrainer\n",
                "\n",
                "# Create trainer\n",
                "trainer = WSTrainer(\n",
                "    max_epochs=3,\n",
                "    accelerator=\"cpu\",\n",
                "    devices=1,\n",
                "    precision=\"32-true\",\n",
                "    log_every_n_steps=5,\n",
                "    enable_progress_bar=True,\n",
                "    reload_dataloaders_every_n_epochs=1,\n",
                "    logger=False,  # Disable logging for tutorial\n",
                ")\n",
                "\n",
                "print(\"Starting REINFORCE training (3 epochs)...\")\n",
                "trainer.fit(model)\n",
                "print(\"Training complete!\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Evaluate after training\n",
                "with torch.no_grad():\n",
                "    out_after = policy(td_eval.clone(), env, strategy=\"greedy\", return_actions=True)\n",
                "\n",
                "print(f\"Training Results:\")\n",
                "print(f\"  Before training - Mean reward: {out_before['reward'].mean():.4f}\")\n",
                "print(f\"  After training  - Mean reward: {out_after['reward'].mean():.4f}\")\n",
                "improvement = out_after['reward'].mean() - out_before['reward'].mean()\n",
                "print(f\"  Improvement: {improvement:.4f}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "---\n",
                "## 4. Baselines for Variance Reduction\n",
                "\n",
                "Baselines reduce the variance of policy gradients. WSmart+ Route supports several baseline types:\n",
                "\n",
                "| Baseline | Description |\n",
                "|----------|-------------|\n",
                "| `\"rollout\"` | Uses greedy rollout of a periodically updated policy |\n",
                "| `\"exponential\"` | Exponential moving average of past rewards |\n",
                "| `\"critic\"` | Learned value network |\n",
                "| `\"warmup\"` | Transitions from one baseline to another |\n",
                "| `\"no\"` | No baseline (vanilla REINFORCE) |"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Train with exponential baseline for comparison\n",
                "env2 = get_env(\"vrpp\", num_loc=20)\n",
                "policy2 = AttentionModelPolicy(\n",
                "    env_name=\"vrpp\", embed_dim=64, n_encode_layers=2, n_decode_layers=2, n_heads=4,\n",
                ")\n",
                "\n",
                "model_exp = REINFORCE(\n",
                "    env=env2,\n",
                "    policy=policy2,\n",
                "    baseline=\"exponential\",\n",
                "    optimizer=\"adam\",\n",
                "    optimizer_kwargs={\"lr\": 1e-4},\n",
                "    train_data_size=640,\n",
                "    val_data_size=128,\n",
                "    batch_size=64,\n",
                "    num_workers=0,\n",
                ")\n",
                "\n",
                "trainer2 = WSTrainer(\n",
                "    max_epochs=3, accelerator=\"cpu\", devices=1, precision=\"32-true\",\n",
                "    log_every_n_steps=5, enable_progress_bar=True, logger=False,\n",
                "    reload_dataloaders_every_n_epochs=1,\n",
                ")\n",
                "\n",
                "print(\"Training with exponential baseline...\")\n",
                "trainer2.fit(model_exp)\n",
                "\n",
                "with torch.no_grad():\n",
                "    out_exp = policy2(td_eval.clone(), env2, strategy=\"greedy\", return_actions=True)\n",
                "\n",
                "print(f\"\\nBaseline Comparison:\")\n",
                "print(f\"  Rollout baseline     - Mean reward: {out_after['reward'].mean():.4f}\")\n",
                "print(f\"  Exponential baseline - Mean reward: {out_exp['reward'].mean():.4f}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "---\n",
                "## 5. PPO Training\n",
                "\n",
                "PPO performs multiple optimization epochs per batch with a clipped surrogate objective, often leading to more stable training."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "from logic.src.models.policies.critic import create_critic_from_actor\n",
                "from logic.src.pipeline.rl import PPO\n",
                "\n",
                "# Create fresh components\n",
                "env_ppo = get_env(\"vrpp\", num_loc=20)\n",
                "policy_ppo = AttentionModelPolicy(\n",
                "    env_name=\"vrpp\", embed_dim=64, n_encode_layers=2, n_decode_layers=2, n_heads=4,\n",
                ")\n",
                "\n",
                "# Create critic network (shares encoder architecture with policy)\n",
                "critic = create_critic_from_actor(\n",
                "    policy_ppo,\n",
                "    env_name=\"vrpp\",\n",
                "    embed_dim=64,\n",
                "    hidden_dim=64,\n",
                "    n_layers=2,\n",
                "    n_heads=4,\n",
                ")\n",
                "print(f\"Critic parameters: {sum(p.numel() for p in critic.parameters()):,}\")\n",
                "\n",
                "# Create PPO module\n",
                "model_ppo = PPO(\n",
                "    env=env_ppo,\n",
                "    policy=policy_ppo,\n",
                "    critic=critic,\n",
                "    ppo_epochs=3,\n",
                "    eps_clip=0.2,\n",
                "    baseline=\"no\",  # PPO uses critic instead of traditional baselines\n",
                "    optimizer=\"adam\",\n",
                "    optimizer_kwargs={\"lr\": 1e-4},\n",
                "    train_data_size=640,\n",
                "    val_data_size=128,\n",
                "    batch_size=64,\n",
                "    num_workers=0,\n",
                ")\n",
                "\n",
                "trainer_ppo = WSTrainer(\n",
                "    max_epochs=3, accelerator=\"cpu\", devices=1, precision=\"32-true\",\n",
                "    log_every_n_steps=5, enable_progress_bar=True, logger=False,\n",
                "    reload_dataloaders_every_n_epochs=1,\n",
                ")\n",
                "\n",
                "print(\"Training with PPO (3 epochs)...\")\n",
                "trainer_ppo.fit(model_ppo)\n",
                "\n",
                "with torch.no_grad():\n",
                "    out_ppo = policy_ppo(td_eval.clone(), env_ppo, strategy=\"greedy\", return_actions=True)\n",
                "\n",
                "print(f\"\\nPPO Results:\")\n",
                "print(f\"  Mean reward (greedy): {out_ppo['reward'].mean():.4f}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "---\n",
                "## 6. Using `create_model()` Helper\n",
                "\n",
                "For convenience, the `create_model()` function wires all components from a `Config` object."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "from logic.src.pipeline.features.train import create_model\n",
                "\n",
                "# Use the config we defined earlier\n",
                "cfg_auto = Config(\n",
                "    env=EnvConfig(name=\"vrpp\", num_loc=20),\n",
                "    model=ModelConfig(name=\"am\", embed_dim=64, num_encoder_layers=2, num_decoder_layers=2, n_heads=4),\n",
                "    train=TrainConfig(n_epochs=2, batch_size=64, train_data_size=640, val_data_size=128, num_workers=0, precision=\"32-true\"),\n",
                "    optim=OptimConfig(optimizer=\"adam\", lr=1e-4),\n",
                "    rl=RLConfig(algorithm=\"reinforce\", baseline=\"exponential\"),\n",
                "    device=\"cpu\",\n",
                ")\n",
                "\n",
                "model_auto = create_model(cfg_auto)\n",
                "print(f\"Auto-created model: {type(model_auto).__name__}\")\n",
                "print(f\"  Environment: {model_auto.env.name}\")\n",
                "print(f\"  Policy: {type(model_auto.policy).__name__}\")\n",
                "print(f\"  Baseline: {model_auto.baseline_type}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "---\n",
                "## 7. Saving and Loading Models"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import tempfile\n",
                "\n",
                "# Save trained model weights\n",
                "save_dir = tempfile.mkdtemp()\n",
                "save_path = os.path.join(save_dir, \"model.pt\")\n",
                "\n",
                "model.save_weights(save_path)\n",
                "print(f\"Saved model to: {save_path}\")\n",
                "print(f\"File size: {os.path.getsize(save_path) / 1024:.1f} KB\")\n",
                "\n",
                "# List saved files\n",
                "for f in os.listdir(save_dir):\n",
                "    print(f\"  {f}: {os.path.getsize(os.path.join(save_dir, f)) / 1024:.1f} KB\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "---\n",
                "## 8. Algorithm Comparison\n",
                "\n",
                "Let's visualize the performance of different algorithms trained in this tutorial."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Collect all results\n",
                "results = {\n",
                "    \"Untrained\": out_before[\"reward\"].mean().item(),\n",
                "    \"REINFORCE\\n(rollout)\": out_after[\"reward\"].mean().item(),\n",
                "    \"REINFORCE\\n(exponential)\": out_exp[\"reward\"].mean().item(),\n",
                "    \"PPO\": out_ppo[\"reward\"].mean().item(),\n",
                "}\n",
                "\n",
                "fig, ax = plt.subplots(figsize=(9, 5))\n",
                "names = list(results.keys())\n",
                "values = list(results.values())\n",
                "colors = [\"lightgray\", \"steelblue\", \"coral\", \"seagreen\"]\n",
                "\n",
                "bars = ax.bar(range(len(names)), values, color=colors, edgecolor=\"black\", alpha=0.85)\n",
                "ax.set_xticks(range(len(names)))\n",
                "ax.set_xticklabels(names, fontsize=10)\n",
                "ax.set_ylabel(\"Mean Reward (greedy)\")\n",
                "ax.set_title(\"Training Algorithm Comparison\\n(3 epochs, 20-node VRPP, CPU)\")\n",
                "ax.grid(True, alpha=0.3, axis=\"y\")\n",
                "\n",
                "for bar, val in zip(bars, values):\n",
                "    ax.text(bar.get_x() + bar.get_width() / 2, bar.get_height() + 0.005,\n",
                "            f\"{val:.3f}\", ha=\"center\", va=\"bottom\", fontsize=10)\n",
                "\n",
                "plt.tight_layout()\n",
                "plt.show()\n",
                "\n",
                "print(\"\\nNote: With more epochs and larger data, differences become more pronounced.\")\n",
                "print(\"For production training, use GPU with:\")\n",
                "print(\"  - num_loc=50-100\")\n",
                "print(\"  - embed_dim=128\")\n",
                "print(\"  - n_epochs=100+\")\n",
                "print(\"  - train_data_size=100000\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "---\n",
                "## Summary\n",
                "\n",
                "In this tutorial, you learned:\n",
                "\n",
                "- **Config dataclasses** (`Config`, `EnvConfig`, `ModelConfig`, `TrainConfig`, `RLConfig`, `OptimConfig`) control all training parameters\n",
                "- **REINFORCE** is the standard policy gradient algorithm with configurable baselines (`rollout`, `exponential`, `critic`)\n",
                "- **PPO** uses a critic network and clipped surrogate objective for stable training\n",
                "- **WSTrainer** wraps PyTorch Lightning Trainer with RL-specific optimizations\n",
                "- **`create_model(cfg)`** is the high-level helper that wires env + policy + RL module from config\n",
                "- Models can be **saved/loaded** with `model.save_weights()`\n",
                "\n",
                "### CLI Equivalents\n",
                "\n",
                "The training we did programmatically can also be run via CLI:\n",
                "\n",
                "```bash\n",
                "# REINFORCE with rollout baseline\n",
                "python main.py train_lightning model=am env.name=vrpp env.num_loc=50 train.n_epochs=100\n",
                "\n",
                "# PPO\n",
                "python main.py train_lightning model=am env.name=vrpp rl.algorithm=ppo train.n_epochs=100\n",
                "\n",
                "# With custom settings\n",
                "python main.py train_lightning model=deep_decoder env.name=wcvrp env.num_loc=100 \\\n",
                "    rl.algorithm=reinforce rl.baseline=exponential optim.lr=5e-5 train.batch_size=512\n",
                "```\n",
                "\n",
                "### Next Steps\n",
                "\n",
                "Continue to **[Tutorial 5: Evaluation and Decoding Strategies](05_evaluation_and_decoding.ipynb)** to learn how to evaluate trained models and compare decoding strategies."
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.10.12"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 5
}
