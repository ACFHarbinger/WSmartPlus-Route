{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Tutorial 7: Extending the Codebase\n",
                "\n",
                "**WSmart+ Route Tutorial Series**\n",
                "\n",
                "One of the core design goals of WSmart+ Route is extensibility. This tutorial demonstrates how to add new components without modifying the core library code. You'll learn how to:\n",
                "\n",
                "1. **Create a custom classical policy** (e.g., a geometric heuristic)\n",
                "2. **Implement a custom neural module** (e.g., a specialized encoder)\n",
                "3. **Integrate custom components** into the training pipeline\n",
                "\n",
                "**Previous**: [06_simulation_testing.ipynb](06_simulation_testing.ipynb)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import os\n",
                "import sys\n",
                "import warnings\n",
                "\n",
                "warnings.filterwarnings(\"ignore\")\n",
                "\n",
                "PROJECT_ROOT = os.path.abspath(os.path.join(os.getcwd(), \"..\", \"..\"))\n",
                "if PROJECT_ROOT not in sys.path:\n",
                "    sys.path.insert(0, PROJECT_ROOT)\n",
                "\n",
                "import math\n",
                "import matplotlib.pyplot as plt\n",
                "import numpy as np\n",
                "import torch\n",
                "import torch.nn as nn\n",
                "\n",
                "torch.manual_seed(42)\n",
                "np.random.seed(42)\n",
                "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
                "print(f\"Using device: {device}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "---\n",
                "## 1. Adding a Custom Classical Policy\n",
                "\n",
                "Policies in WSmart+ Route follow the **Adapter Pattern**. To add a new policy, you must:\n",
                "\n",
                "1. Inherit from the `IPolicy` interface (in `logic.src.policies.adapters`).\n",
                "2. Implement the `execute(self, **kwargs)` method.\n",
                "3. Register the class using the `@PolicyRegistry.register` decorator.\n",
                "\n",
                "Let's implement a **Spiral Policy** properly wrapped as an `IPolicy`."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "from typing import Any, List, Tuple\n",
                "from logic.src.policies.adapters import IPolicy, PolicyRegistry\n",
                "\n",
                "@PolicyRegistry.register(\"spiral\")\n",
                "class SpiralPolicy(IPolicy):\n",
                "    \"\"\"\n",
                "    Collects bins above threshold, ordering them by angle (spiral sweep).\n",
                "    \"\"\"\n",
                "    \n",
                "    def execute(self, **kwargs: Any) -> Tuple[List[int], float, Any]:\n",
                "        \"\"\"\n",
                "        Execute the spiral policy.\n",
                "        \n",
                "        Expected kwargs:\n",
                "            bins: Bins object with .fill_levels\n",
                "            dist_matrix: Distance matrix\n",
                "            coords: Coordinates array\n",
                "            config: Configuration dictionary (optional)\n",
                "        \"\"\"\n",
                "        # 1. Extract data from kwargs\n",
                "        bins = kwargs.get(\"bins\")\n",
                "        coords = kwargs.get(\"coords\")\n",
                "        dist_matrix = kwargs.get(\"distance_matrix\")\n",
                "        \n",
                "        # Parse config for threshold (default 50%)\n",
                "        config = kwargs.get(\"config\", {})\n",
                "        spiral_cfg = config.get(\"spiral\", {})\n",
                "        threshold = spiral_cfg.get(\"threshold\", 50.0)\n",
                "        \n",
                "        # 2. Policy Logic\n",
                "        fill_levels = bins.fill_levels\n",
                "        \n",
                "        # Identify candidates\n",
                "        candidates = np.where(fill_levels >= threshold)[0]\n",
                "        if len(candidates) == 0:\n",
                "            return [], 0.0, None\n",
                "            \n",
                "        # Adjust indices for coordinates (depot is 0, bin i is i+1)\n",
                "        # Note: bins 0..N-1 correspond to coords 1..N\n",
                "        candidate_coords = coords[candidates + 1]\n",
                "        depot_coord = coords[0]\n",
                "        \n",
                "        # Calculate angles relative to depot\n",
                "        diffs = candidate_coords - depot_coord\n",
                "        angles = np.arctan2(diffs[:, 1], diffs[:, 0])\n",
                "        \n",
                "        # Sort by angle\n",
                "        sort_indices = np.argsort(angles)\n",
                "        tour = list(candidates[sort_indices])\n",
                "        \n",
                "        # 3. Calculate cost (helper function or manual)\n",
                "        # Simple manual calculation for tutorial:\n",
                "        def calc_tour_cost(tour, dist_matrix):\n",
                "            if not tour: return 0.0\n",
                "            route = [0] + [t + 1 for t in tour] + [0]\n",
                "            cost = 0.0\n",
                "            for i in range(len(route) - 1):\n",
                "                cost += dist_matrix[route[i], route[i+1]]\n",
                "            return cost\n",
                "            \n",
                "        cost = calc_tour_cost(tour, dist_matrix)\n",
                "        \n",
                "        return tour, cost, None\n",
                "\n",
                "print(\"SpiralPolicy registered successfully!\")\n",
                "print(f\"Current Registry: {PolicyRegistry.list_policies()}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### Step 2: Test the Custom Policy\n",
                "\n",
                "Now we can instantiate and run it, simulating how the `Simulator` would call it."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Mock objects to simulate the environment state\n",
                "class MockBins:\n",
                "    def __init__(self, n=20):\n",
                "        self.n = n\n",
                "        self.fill_levels = np.random.uniform(0, 100, n)\n",
                "\n",
                "# Prepare simulation data\n",
                "n_bins = 20\n",
                "mock_bins = MockBins(n_bins)\n",
                "coords = np.random.rand(n_bins + 1, 2)\n",
                "dist_matrix = np.sqrt(((coords[:, None] - coords[None, :]) ** 2).sum(axis=-1))\n",
                "\n",
                "# Instantiate policy via Registry (or directly)\n",
                "policy_cls = PolicyRegistry.get(\"spiral\")\n",
                "policy_instance = policy_cls()\n",
                "\n",
                "# Execute\n",
                "tour, cost, _ = policy_instance.execute(\n",
                "    bins=mock_bins,\n",
                "    coords=coords,\n",
                "    distance_matrix=dist_matrix,\n",
                "    config={\"spiral\": {\"threshold\": 30.0}}\n",
                ")\n",
                "\n",
                "print(f\"Fill levels > 30%: {(mock_bins.fill_levels > 30).sum()}\")\n",
                "print(f\"Selected by Spiral Policy: {len(tour)}\")\n",
                "print(f\"Tour order: {tour}\")\n",
                "print(f\"Tour cost: {cost:.2f}\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Visualizing our custom policy\n",
                "def plot_tour(coords, tour, title=\"Policy Tour\"):\n",
                "    fig, ax = plt.subplots(figsize=(6, 6))\n",
                "    \n",
                "    # Plot all nodes\n",
                "    ax.scatter(coords[1:, 0], coords[1:, 1], c='lightgray', s=30, label='Bin')\n",
                "    ax.scatter(coords[0, 0], coords[0, 1], c='red', s=100, marker='*', label='Depot')\n",
                "    \n",
                "    if len(tour) > 0:\n",
                "        # Construct path: Depot -> Tour -> Depot\n",
                "        path_indices = [0] + [t + 1 for t in tour] + [0]\n",
                "        path_coords = coords[path_indices]\n",
                "        \n",
                "        ax.plot(path_coords[:, 0], path_coords[:, 1], c='steelblue', linewidth=1.5, alpha=0.8)\n",
                "        ax.scatter(coords[[t+1 for t in tour], 0], coords[[t+1 for t in tour], 1], \n",
                "                   c='steelblue', s=50, zorder=3, label='Collected')\n",
                "        \n",
                "        # Annotate order\n",
                "        for i, idx in enumerate(path_indices[1:-1]):\n",
                "            ax.text(coords[idx, 0], coords[idx, 1], str(i+1), fontsize=8, color='white', ha='center', va='center')\n",
                "\n",
                "    ax.legend()\n",
                "    ax.set_title(title)\n",
                "    plt.tight_layout()\n",
                "    plt.show()\n",
                "\n",
                "plot_tour(coords, tour, title=\"Custom Spiral Policy\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "---\n",
                "## 2. Adding a Custom Neural Encoder\n",
                "\n",
                "You can customize neural architectures by subclassing `nn.Module` and swapping components in existing models.\n",
                "\n",
                "Let's create a **Residual MLP Encoder**. Unlike the standard Graph Attention Encoder (GAT), this encoder treats nodes independently but uses a deep residual MLP to project features, which is faster but ignores graph topology."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "from logic.src.models.policies import AttentionModelPolicy\n",
                "\n",
                "class ResidualBlock(nn.Module):\n",
                "    def __init__(self, dim):\n",
                "        super().__init__()\n",
                "        self.linear1 = nn.Linear(dim, dim)\n",
                "        self.relu = nn.ReLU()\n",
                "        self.linear2 = nn.Linear(dim, dim)\n",
                "    \n",
                "    def forward(self, x):\n",
                "        residual = x\n",
                "        x = self.linear1(x)\n",
                "        x = self.relu(x)\n",
                "        x = self.linear2(x)\n",
                "        return x + residual  # Skip connection\n",
                "\n",
                "class ResidualMLPEncoder(nn.Module):\n",
                "    def __init__(self, input_dim=2, embed_dim=128, num_layers=3):\n",
                "        super().__init__()\n",
                "        self.init_embed = nn.Linear(input_dim, embed_dim)\n",
                "        self.layers = nn.ModuleList([ResidualBlock(embed_dim) for _ in range(num_layers)])\n",
                "    \n",
                "    def forward(self, x, mask=None):\n",
                "        \"\"\"\n",
                "        Args:\n",
                "            x: Input features (batch_size, num_nodes, input_dim)\n",
                "            mask: Unused here, kept for API compatibility\n",
                "        Returns:\n",
                "            (embeddings, mean_embedding) tuple\n",
                "        \"\"\"\n",
                "        h = self.init_embed(x)\n",
                "        for layer in self.layers:\n",
                "            h = layer(h)\n",
                "        \n",
                "        # Return (node_embeddings, graph_embedding)\n",
                "        # Graph embedding is just mean over nodes\n",
                "        graph_emb = h.mean(dim=1)\n",
                "        return h, graph_emb\n",
                "\n",
                "# Instantiate our custom encoder\n",
                "custom_encoder = ResidualMLPEncoder(input_dim=2, embed_dim=64, num_layers=4)\n",
                "print(\"Custom Encoder Created:\")\n",
                "print(custom_encoder)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### Integrating the Custom Encoder\n",
                "\n",
                "We can create a standard `AttentionModelPolicy` and simply replace its encoder."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# 1. Create standard policy\n",
                "policy = AttentionModelPolicy(\n",
                "    env_name=\"vrpp\",\n",
                "    embed_dim=64,  # Must match our custom encoder\n",
                "    n_encode_layers=2, \n",
                "    n_decode_layers=2,\n",
                "    n_heads=4\n",
                ")\n",
                "\n",
                "# 2. Swap the encoder\n",
                "print(f\"Original Encoder: {type(policy.encoder).__name__}\")\n",
                "policy.encoder = custom_encoder\n",
                "print(f\"New Encoder:      {type(policy.encoder).__name__}\")\n",
                "\n",
                "# 3. Verify it runs\n",
                "from logic.src.envs import get_env\n",
                "env = get_env(\"vrpp\", num_loc=20)\n",
                "td = env.reset(env.generator(batch_size=2))\n",
                "\n",
                "with torch.no_grad():\n",
                "    # Standard forward pass triggers our new encoder\n",
                "    out = policy(td, env, strategy=\"greedy\")\n",
                "    print(f\"Forward pass successful! Reward: {out['reward'].mean():.4f}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "---\n",
                "## 3. Creating a Subclass Model\n",
                "\n",
                "For cleaner integration, you can subclass `AttentionModelPolicy` to permanently use your architecture."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "class CustomMLPPolicy(AttentionModelPolicy):\n",
                "    def __init__(self, mlp_layers=3, **kwargs):\n",
                "        # Initialize parent standard components\n",
                "        super().__init__(**kwargs)\n",
                "        \n",
                "        # Overwrite encoder with our custom one\n",
                "        self.encoder = ResidualMLPEncoder(\n",
                "            input_dim=self.encoder.init_embed.in_features,\n",
                "            embed_dim=kwargs.get('embed_dim', 128),\n",
                "            num_layers=mlp_layers\n",
                "        )\n",
                "\n",
                "custom_policy = CustomMLPPolicy(\n",
                "    env_name=\"vrpp\", embed_dim=64, mlp_layers=5\n",
                ")\n",
                "\n",
                "print(f\"Custom Policy: {custom_policy}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "---\n",
                "## 4. Customizing the Loss Function (RL)\n",
                "\n",
                "WSmart+ Route uses PyTorch Lightning `LightningModule`s for training logic. You can subclass `RL4COLitModule` or `REINFORCE` to modify the loss calculation.\n",
                "\n",
                "Let's implement **Entropy-Regularized REINFORCE**. Standard REINFORCE maximizes reward. We will add an entropy term to encourage exploration."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "from logic.src.pipeline.rl import REINFORCE\n",
                "\n",
                "class EntropyREINFORCE(REINFORCE):\n",
                "    def __init__(self, entropy_coeff=0.01, **kwargs):\n",
                "        super().__init__(**kwargs)\n",
                "        self.entropy_coeff = entropy_coeff\n",
                "    \n",
                "    def training_step(self, batch, batch_idx):\n",
                "        # 1. Run policy\n",
                "        out = self.policy(batch, self.env, strategy=\"sampling\", return_actions=True)\n",
                "        \n",
                "        # 2. Compute advantage (Reward - Baseline)\n",
                "        # (Using internal baseline logic from parent)\n",
                "        bl_val, bl_loss = self.baseline.eval(batch, out['reward'])\n",
                "        advantage = out['reward'] - bl_val\n",
                "        \n",
                "        # 3. Policy Loss: -(Advantage * log_prob)\n",
                "        log_likelihood = out['log_likelihood']\n",
                "        reinforce_loss = -(advantage * log_likelihood).mean()\n",
                "        \n",
                "        # 4. Entropy Bonus: - (beta * entropy)\n",
                "        # Note: We subtract because we minimize loss. Entropy maximization -> negative loss.\n",
                "        entropy = out.get('entropy', torch.zeros(1, device=self.device))\n",
                "        entropy_loss = -self.entropy_coeff * entropy.mean()\n",
                "        \n",
                "        # 5. Total loss\n",
                "        loss = reinforce_loss + entropy_loss + bl_loss\n",
                "        \n",
                "        # Logging\n",
                "        self.log(\"train/loss\", loss)\n",
                "        self.log(\"train/entropy\", entropy.mean())\n",
                "        self.log(\"train/reward\", out['reward'].mean())\n",
                "        \n",
                "        return loss\n",
                "\n",
                "print(\"Custom Entropy-Regularized RL Module Defined.\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Brief training loop verification\n",
                "from logic.src.pipeline.rl.common.trainer import WSTrainer\n",
                "\n",
                "# Create module\n",
                "model_ent = EntropyREINFORCE(\n",
                "    entropy_coeff=0.1,  # High entropy for demonstration\n",
                "    env=env,\n",
                "    policy=custom_policy,\n",
                "    baseline=\"rollout\",\n",
                "    train_data_size=128,\n",
                "    val_data_size=32,\n",
                "    batch_size=32\n",
                ")\n",
                "\n",
                "# Train for 1 epoch\n",
                "trainer = WSTrainer(\n",
                "    max_epochs=1, \n",
                "    accelerator=\"cpu\", \n",
                "    devices=1, \n",
                "    logger=False, \n",
                "    enable_progress_bar=False\n",
                ")\n",
                "\n",
                "print(\"Running training step with custom loss...\")\n",
                "trainer.fit(model_ent)\n",
                "print(\"Training step successful!\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "---\n",
                "## Summary\n",
                "\n",
                "In this tutorial, we demonstrated how to extend WSmart+ Route:\n",
                "\n",
                "1.  **Custom Policies**: Implemented a `SpiralPolicy` using the `IPolicy` interface and `PolicyRegistry`.\n",
                "2.  **Custom Neural Modules**: Created a `ResidualMLPEncoder` and swapped it into a standard `AttentionModelPolicy`.\n",
                "3.  **Inheritance**: Created a proper `CustomMLPPolicy` class.\n",
                "4.  **Custom Loss**: Subclassed `REINFORCE` to add an entropy regularization term in the training step.\n",
                "\n",
                "These mechanisms allow you to test novel ideas without needing to fork the entire library."
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.10.12"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 5
}
