{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tutorial 3: Neural Models and Classical Policies\n",
    "\n",
    "**WSmart+ Route Tutorial Series**\n",
    "\n",
    "This tutorial covers the neural and classical routing policies available in WSmart+ Route. You'll learn:\n",
    "\n",
    "1. The **ConstructivePolicy** architecture (encoder + decoder)\n",
    "2. **AttentionModelPolicy** - the primary neural routing model\n",
    "3. Other neural models: **DeepDecoder**, **PointerNetwork**\n",
    "4. **Classical policies**: ALNS and HGS\n",
    "5. **Comparing** neural vs classical approaches\n",
    "\n",
    "**Previous**: [02_environments.ipynb](02_environments.ipynb) | **Next**: [04_training_with_lightning.ipynb](04_training_with_lightning.ipynb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "PROJECT_ROOT = os.path.abspath(os.path.join(os.getcwd(), \"..\", \"..\"))\n",
    "if PROJECT_ROOT not in sys.path:\n",
    "    sys.path.insert(0, PROJECT_ROOT)\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 1. Policy Architecture Overview\n",
    "\n",
    "WSmart+ Route uses a **constructive** approach to solving routing problems. A policy builds a solution step-by-step:\n",
    "\n",
    "1. **Encoder**: Process all node features into embeddings (runs once)\n",
    "2. **Decoder**: At each step, select the next node to visit based on current state and embeddings\n",
    "3. **Action Selection**: Sample or greedily pick from the probability distribution\n",
    "\n",
    "```\n",
    "Input Instance \u2192 [Encoder] \u2192 Node Embeddings \u2192 [Decoder (loop)] \u2192 Complete Tour\n",
    "                                                    \u2191\n",
    "                                              Current State\n",
    "                                              Action Mask\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from logic.src.envs import get_env\n",
    "from logic.src.models.policies import AttentionModelPolicy\n",
    "\n",
    "# Create environment and policy\n",
    "env = get_env(\"vrpp\", num_loc=20)\n",
    "policy = AttentionModelPolicy(\n",
    "    env_name=\"vrpp\",\n",
    "    embed_dim=64,           # Embedding dimension\n",
    "    n_encode_layers=2,      # Number of encoder layers\n",
    "    n_decode_layers=2,      # Number of decoder layers\n",
    "    n_heads=4,              # Attention heads\n",
    "    normalization=\"instance\",\n",
    "    activation=\"gelu\",\n",
    ")\n",
    "\n",
    "# Model summary\n",
    "total_params = sum(p.numel() for p in policy.parameters())\n",
    "trainable_params = sum(p.numel() for p in policy.parameters() if p.requires_grad)\n",
    "print(f\"AttentionModelPolicy:\")\n",
    "print(f\"  Total parameters:     {total_params:,}\")\n",
    "print(f\"  Trainable parameters: {trainable_params:,}\")\n",
    "print(f\"  Embedding dim: {64}\")\n",
    "print(f\"  Encoder layers: {2}\")\n",
    "print(f\"  Attention heads: {4}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate test instances and run forward pass\n",
    "td = env.generator(batch_size=8)\n",
    "td = env.reset(td)\n",
    "\n",
    "# Greedy decoding (deterministic - always picks highest probability node)\n",
    "with torch.no_grad():\n",
    "    out_greedy = policy(td.clone(), env, strategy=\"greedy\", return_actions=True)\n",
    "\n",
    "print(\"Greedy decoding output:\")\n",
    "print(f\"  Keys: {list(out_greedy.keys())}\")\n",
    "print(f\"  Reward shape: {out_greedy['reward'].shape}\")\n",
    "print(f\"  Mean reward: {out_greedy['reward'].mean():.4f}\")\n",
    "print(f\"  Actions shape: {out_greedy['actions'].shape}\")\n",
    "print(f\"  Log-likelihood shape: {out_greedy['log_likelihood'].shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sampling decoding (stochastic - samples from probability distribution)\n",
    "with torch.no_grad():\n",
    "    out_sampling = policy(td.clone(), env, strategy=\"sampling\", return_actions=True)\n",
    "\n",
    "print(\"Decoding Strategy Comparison (8 instances):\")\n",
    "print(f\"  Greedy   - Mean reward: {out_greedy['reward'].mean():.4f}\")\n",
    "print(f\"  Sampling - Mean reward: {out_sampling['reward'].mean():.4f}\")\n",
    "print()\n",
    "\n",
    "# Multiple samples to find better solutions\n",
    "rewards_multi = []\n",
    "n_samples_list = [1, 4, 8, 16, 32]\n",
    "for n in n_samples_list:\n",
    "    sample_rewards = []\n",
    "    for _ in range(n):\n",
    "        with torch.no_grad():\n",
    "            out = policy(td.clone(), env, strategy=\"sampling\", return_actions=True)\n",
    "        sample_rewards.append(out[\"reward\"])\n",
    "    # Take best reward across samples for each instance\n",
    "    stacked = torch.stack(sample_rewards, dim=0)\n",
    "    best_rewards = stacked.max(dim=0).values\n",
    "    rewards_multi.append(best_rewards.mean().item())\n",
    "\n",
    "print(\"Multi-sample improvement:\")\n",
    "for n, r in zip(n_samples_list, rewards_multi):\n",
    "    print(f\"  {n:3d} samples -> Mean best reward: {r:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(8, 5))\n",
    "ax.plot(n_samples_list, rewards_multi, \"o-\", linewidth=2, markersize=8, color=\"steelblue\")\n",
    "ax.axhline(y=out_greedy[\"reward\"].mean().item(), color=\"red\", linestyle=\"--\",\n",
    "           label=f\"Greedy baseline: {out_greedy['reward'].mean():.4f}\")\n",
    "ax.set_xlabel(\"Number of Samples\")\n",
    "ax.set_ylabel(\"Mean Best Reward\")\n",
    "ax.set_title(\"Reward vs Number of Samples (untrained model)\")\n",
    "ax.legend()\n",
    "ax.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 2. Other Neural Policies\n",
    "\n",
    "WSmart+ Route provides several neural architectures beyond the standard Attention Model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from logic.src.models.policies import (\n",
    "    AttentionModelPolicy,\n",
    "    DeepDecoderPolicy,\n",
    "    PointerNetworkPolicy,\n",
    ")\n",
    "\n",
    "# Create different policies with same embedding dimension\n",
    "models = {}\n",
    "\n",
    "models[\"AM (Attention Model)\"] = AttentionModelPolicy(\n",
    "    env_name=\"vrpp\", embed_dim=64, n_encode_layers=2, n_decode_layers=2, n_heads=4\n",
    ")\n",
    "\n",
    "models[\"DDAM (Deep Decoder)\"] = DeepDecoderPolicy(\n",
    "    env_name=\"vrpp\", embed_dim=64, n_encode_layers=2, n_decode_layers=4, n_heads=4\n",
    ")\n",
    "\n",
    "models[\"Pointer Network\"] = PointerNetworkPolicy(\n",
    "    env_name=\"vrpp\", embed_dim=64, n_heads=4\n",
    ")\n",
    "\n",
    "# Compare parameter counts\n",
    "print(\"Neural Policy Comparison:\")\n",
    "print(f\"{'Model':<25} {'Parameters':>12}\")\n",
    "print(\"-\" * 40)\n",
    "for name, model in models.items():\n",
    "    params = sum(p.numel() for p in model.parameters())\n",
    "    print(f\"{name:<25} {params:>12,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Benchmark all models on same instances\n",
    "td_bench = env.generator(batch_size=32)\n",
    "\n",
    "print(\"\\nPerformance Comparison (untrained, 32 instances, greedy):\")\n",
    "print(f\"{'Model':<25} {'Mean Reward':>12} {'Std':>8}\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "results = {}\n",
    "for name, model in models.items():\n",
    "    with torch.no_grad():\n",
    "        out = model(env.reset(td_bench.clone()), env, strategy=\"greedy\", return_actions=True)\n",
    "    results[name] = out[\"reward\"]\n",
    "    print(f\"{name:<25} {out['reward'].mean():>12.4f} {out['reward'].std():>8.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(9, 5))\n",
    "names = list(results.keys())\n",
    "means = [results[n].mean().item() for n in names]\n",
    "stds = [results[n].std().item() for n in names]\n",
    "\n",
    "bars = ax.bar(range(len(names)), means, yerr=stds, capsize=5,\n",
    "              color=[\"steelblue\", \"coral\", \"seagreen\"], alpha=0.8, edgecolor=\"black\")\n",
    "ax.set_xticks(range(len(names)))\n",
    "ax.set_xticklabels(names, rotation=15, ha=\"right\")\n",
    "ax.set_ylabel(\"Mean Reward (greedy)\")\n",
    "ax.set_title(\"Neural Policy Comparison (Untrained)\")\n",
    "ax.grid(True, alpha=0.3, axis=\"y\")\n",
    "\n",
    "for bar, mean in zip(bars, means):\n",
    "    ax.text(bar.get_x() + bar.get_width() / 2, bar.get_height() + 0.01,\n",
    "            f\"{mean:.3f}\", ha=\"center\", va=\"bottom\", fontsize=10)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 3. Classical Policies\n",
    "\n",
    "Classical policies use optimization algorithms rather than neural networks. They typically produce higher-quality solutions but are slower.\n",
    "\n",
    "| Policy | Type | Description |\n",
    "|--------|------|-------------|\n",
    "| `ALNSPolicy` | Metaheuristic | Adaptive Large Neighborhood Search - destroy and repair operators |\n",
    "| `HGSPolicy` | Genetic Algorithm | Hybrid Genetic Search - evolutionary approach with local search |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from logic.src.models.policies.classical.alns import ALNSPolicy\n",
    "\n",
    "# Create ALNS policy\n",
    "alns = ALNSPolicy(env_name=\"vrpp\")\n",
    "\n",
    "# Solve instances\n",
    "td_classical = env.generator(batch_size=8)\n",
    "td_classical_reset = env.reset(td_classical.clone())\n",
    "\n",
    "with torch.no_grad():\n",
    "    out_alns = alns(td_classical_reset.clone(), env, strategy=\"greedy\", return_actions=True)\n",
    "\n",
    "print(\"ALNS Policy Results (8 instances):\")\n",
    "print(f\"  Mean reward: {out_alns['reward'].mean():.4f}\")\n",
    "print(f\"  Std reward:  {out_alns['reward'].std():.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from logic.src.models.policies.classical.hgs import HGSPolicy\n",
    "\n",
    "# Create HGS policy\n",
    "hgs = HGSPolicy(env_name=\"vrpp\")\n",
    "\n",
    "with torch.no_grad():\n",
    "    out_hgs = hgs(td_classical_reset.clone(), env, strategy=\"greedy\", return_actions=True)\n",
    "\n",
    "print(\"HGS Policy Results (8 instances):\")\n",
    "print(f\"  Mean reward: {out_hgs['reward'].mean():.4f}\")\n",
    "print(f\"  Std reward:  {out_hgs['reward'].std():.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 4. Neural vs Classical Comparison\n",
    "\n",
    "Comparing untrained neural models against classical optimization algorithms shows the gap that training must close."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Comprehensive comparison\n",
    "all_results = {}\n",
    "\n",
    "# Neural models (untrained)\n",
    "for name, model in models.items():\n",
    "    with torch.no_grad():\n",
    "        out = model(env.reset(td_classical.clone()), env, strategy=\"greedy\", return_actions=True)\n",
    "    all_results[name] = out[\"reward\"]\n",
    "\n",
    "# Classical models\n",
    "all_results[\"ALNS\"] = out_alns[\"reward\"]\n",
    "all_results[\"HGS\"] = out_hgs[\"reward\"]\n",
    "\n",
    "print(\"Full Comparison (8 instances, greedy decoding):\")\n",
    "print(f\"{'Method':<25} {'Mean Reward':>12} {'Std':>8}\")\n",
    "print(\"-\" * 50)\n",
    "for name, rewards in all_results.items():\n",
    "    marker = \" *\" if name in [\"ALNS\", \"HGS\"] else \"\"\n",
    "    print(f\"{name:<25} {rewards.mean():>12.4f} {rewards.std():>8.4f}{marker}\")\n",
    "print(\"\\n* = Classical (no training required)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(10, 5))\n",
    "\n",
    "names = list(all_results.keys())\n",
    "means = [all_results[n].mean().item() for n in names]\n",
    "stds = [all_results[n].std().item() for n in names]\n",
    "colors = [\"steelblue\", \"coral\", \"seagreen\", \"orange\", \"purple\"]\n",
    "\n",
    "bars = ax.bar(range(len(names)), means, yerr=stds, capsize=4,\n",
    "              color=colors[:len(names)], alpha=0.8, edgecolor=\"black\")\n",
    "ax.set_xticks(range(len(names)))\n",
    "ax.set_xticklabels(names, rotation=20, ha=\"right\")\n",
    "ax.set_ylabel(\"Mean Reward\")\n",
    "ax.set_title(\"Neural (Untrained) vs Classical Policies\")\n",
    "ax.grid(True, alpha=0.3, axis=\"y\")\n",
    "\n",
    "# Add value labels\n",
    "for bar, mean in zip(bars, means):\n",
    "    ax.text(bar.get_x() + bar.get_width() / 2, bar.get_height() + 0.01,\n",
    "            f\"{mean:.3f}\", ha=\"center\", va=\"bottom\", fontsize=9)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 5. Model Internals\n",
    "\n",
    "Let's peek inside the Attention Model to understand its components."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inspect the AM policy structure\n",
    "am = models[\"AM (Attention Model)\"]\n",
    "\n",
    "print(\"AttentionModelPolicy Components:\")\n",
    "print(\"=\" * 50)\n",
    "for name, module in am.named_children():\n",
    "    n_params = sum(p.numel() for p in module.parameters())\n",
    "    print(f\"\\n  {name} ({type(module).__name__}):\")\n",
    "    print(f\"    Parameters: {n_params:,}\")\n",
    "    for sub_name, sub_module in module.named_children():\n",
    "        sub_params = sum(p.numel() for p in sub_module.parameters())\n",
    "        print(f\"      {sub_name}: {type(sub_module).__name__} ({sub_params:,} params)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize parameter distribution\n",
    "fig, axes = plt.subplots(1, 3, figsize=(15, 4))\n",
    "\n",
    "for ax, (name, model) in zip(axes, list(models.items())[:3]):\n",
    "    all_params = torch.cat([p.data.flatten() for p in model.parameters()])\n",
    "    ax.hist(all_params.numpy(), bins=50, alpha=0.7, color=\"steelblue\", edgecolor=\"white\")\n",
    "    ax.set_title(f\"{name}\\n({all_params.numel():,} params)\")\n",
    "    ax.set_xlabel(\"Parameter Value\")\n",
    "    ax.set_ylabel(\"Count\")\n",
    "    ax.axvline(x=0, color=\"red\", linestyle=\"--\", alpha=0.5)\n",
    "\n",
    "plt.suptitle(\"Parameter Value Distributions (Before Training)\", fontsize=13, y=1.02)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Summary\n",
    "\n",
    "In this tutorial, you learned:\n",
    "\n",
    "- **ConstructivePolicy** architecture: encoder processes all nodes once, decoder selects nodes step-by-step\n",
    "- **AttentionModelPolicy** is the primary neural model using multi-head attention\n",
    "- **DeepDecoderPolicy** uses more decoder layers for richer step-by-step reasoning\n",
    "- **PointerNetworkPolicy** uses classic RNN-based pointing mechanism\n",
    "- **ALNS** and **HGS** are classical optimization policies that need no training\n",
    "- **Untrained** neural models perform worse than classical methods - the gap is closed through RL training (next tutorial)\n",
    "- **Multi-sample** decoding can improve sampling-based solutions\n",
    "\n",
    "### Key Insight\n",
    "\n",
    "Neural models start with random weights and produce poor solutions. Through RL training (Tutorial 4), they learn to match or exceed classical methods while being much faster at inference time.\n",
    "\n",
    "### Next Steps\n",
    "\n",
    "Continue to **[Tutorial 4: Training with PyTorch Lightning](04_training_with_lightning.ipynb)** to train these neural models using reinforcement learning."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
