{
    "cells": [
        {
            "cell_type": "markdown",
            "id": "intro-header",
            "metadata": {},
            "source": [
                "# WSmart+ Route: Lightning-Based RL Training Tutorial\n",
                "\n",
                "**Version:** 2.0  \n",
                "**Last Updated:** January 2026\n",
                "\n",
                "This comprehensive tutorial covers the reinforcement learning training pipeline built on **PyTorch Lightning** and **Hydra** for combinatorial optimization problems.\n",
                "\n",
                "## Table of Contents\n",
                "\n",
                "1. [Overview & Architecture](#1-overview--architecture)\n",
                "2. [Hydra Configuration System](#2-hydra-configuration-system)\n",
                "3. [PyTorch Lightning Modules](#3-pytorch-lightning-modules)\n",
                "4. [Complete RL Algorithms Reference](#4-complete-rl-algorithms-reference)\n",
                "5. [Baselines for Variance Reduction](#5-baselines-for-variance-reduction)\n",
                "6. [Complete Neural Models Reference](#6-complete-neural-models-reference)\n",
                "7. [Environments & Data Generation](#7-environments--data-generation)\n",
                "8. [Meta-Learning](#8-meta-learning)\n",
                "9. [Hyperparameter Optimization](#9-hyperparameter-optimization)\n",
                "10. [Practical Examples](#10-practical-examples)\n",
                "11. [Adding New Components Guide](#11-adding-new-components-guide)\n",
                "12. [Hydra + Lightning + TorchRL Best Practices](#12-hydra--lightning--torchrl-best-practices)\n",
                "13. [Troubleshooting & Common Patterns](#13-troubleshooting--common-patterns)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 1,
            "id": "setup-imports",
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Setup completed - added home_dir to system path: /home/pkhunter/Repositories/WSmart-Route\n",
                        "PyTorch version: 2.2.2+cu121\n",
                        "CUDA available: True\n",
                        "CUDA device: NVIDIA GeForce RTX 3090 Ti\n"
                    ]
                }
            ],
            "source": [
                "# Standard notebook setup\n",
                "import warnings\n",
                "\n",
                "warnings.filterwarnings('ignore')\n",
                "\n",
                "from notebook_setup import setup_google_colab, setup_home_directory\n",
                "\n",
                "NOTEBOOK_NAME = \"lightning_rl_training_tutorial\"\n",
                "home_dir = setup_home_directory(NOTEBOOK_NAME)\n",
                "IN_COLAB, gdrive, gfiles = setup_google_colab(NOTEBOOK_NAME)\n",
                "\n",
                "# Core imports used throughout the notebook\n",
                "from typing import Any, Dict, List, Optional\n",
                "\n",
                "import pytorch_lightning as pl\n",
                "import torch\n",
                "import torch.nn as nn\n",
                "\n",
                "print(f\"PyTorch version: {torch.__version__}\")\n",
                "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
                "if torch.cuda.is_available():\n",
                "    print(f\"CUDA device: {torch.cuda.get_device_name(0)}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "section-1",
            "metadata": {},
            "source": [
                "---\n",
                "\n",
                "## 1. Overview & Architecture\n",
                "\n",
                "The WSmart+ Route training pipeline is built on three foundational technologies:\n",
                "\n",
                "### 1.1 Core Technologies\n",
                "\n",
                "| Technology | Purpose | Key Benefits |\n",
                "|------------|---------|-------------|\n",
                "| **PyTorch Lightning** | Training orchestration | Automatic GPU management, logging, checkpointing |\n",
                "| **Hydra** | Configuration management | Hierarchical configs, CLI overrides, experiment tracking |\n",
                "| **TensorDict** | State management | Efficient batched operations, device-agnostic tensors |\n",
                "\n",
                "### 1.2 Pipeline Architecture\n",
                "\n",
                "```\n",
                "┌─────────────────────────────────────────────────────────────────┐\n",
                "│                      train_lightning.py                         │\n",
                "│                    (Hydra CLI Entry Point)                      │\n",
                "└─────────────────────────────────────────────────────────────────┘\n",
                "                              │\n",
                "              ┌───────────────┼───────────────┐\n",
                "              │               │               │\n",
                "              ▼               ▼               ▼\n",
                "┌─────────────────┐ ┌─────────────────┐ ┌─────────────────┐\n",
                "│   Config        │ │   Environment   │ │   RL Module     │\n",
                "│   (Hydra)       │ │   (RL4COEnv)    │ │   (Lightning)   │\n",
                "└─────────────────┘ └─────────────────┘ └─────────────────┘\n",
                "                              │\n",
                "                              ▼\n",
                "              ┌───────────────────────────────┐\n",
                "              │        WSTrainer              │\n",
                "              │   (PyTorch Lightning)         │\n",
                "              └───────────────────────────────┘\n",
                "                              │\n",
                "                              ▼\n",
                "              ┌───────────────────────────────┐\n",
                "              │   Training Loop               │\n",
                "              │   - Data Generation           │\n",
                "              │   - Forward Pass              │\n",
                "              │   - Loss Computation          │\n",
                "              │   - Baseline Updates          │\n",
                "              └───────────────────────────────┘\n",
                "```\n",
                "\n",
                "### 1.3 Complete Directory Structure\n",
                "\n",
                "```\n",
                "logic/src/\n",
                "├── pipeline/rl/              # RL TRAINING PIPELINE\n",
                "│   ├── core/                 # Core RL algorithms\n",
                "│   │   ├── base.py          # RL4COLitModule (base Lightning module)\n",
                "│   │   ├── baselines.py     # All baseline implementations\n",
                "│   │   ├── reinforce.py     # REINFORCE algorithm\n",
                "│   │   ├── ppo.py           # Proximal Policy Optimization\n",
                "│   │   ├── sapo.py          # Self-Adaptive Policy Optimization\n",
                "│   │   ├── gspo.py          # Gradient-Scaled Proxy Optimization\n",
                "│   │   ├── gdpo.py          # Gradient-Divergence Policy Optimization\n",
                "│   │   ├── dr_grpo.py       # Divergence-Regularized GRPO\n",
                "│   │   ├── pomo.py          # Policy Optimization Multiple Optima\n",
                "│   │   ├── symnco.py        # Symmetry-aware NCO\n",
                "│   │   ├── imitation.py     # Imitation Learning\n",
                "│   │   └── adaptive_imitation.py  # IL-to-RL transition\n",
                "│   ├── meta/                 # Meta-learning strategies\n",
                "│   │   ├── module.py        # MetaRLModule wrapper\n",
                "│   │   ├── hrl.py           # Hierarchical RL\n",
                "│   │   ├── weight_optimizer.py\n",
                "│   │   ├── contextual_bandits.py\n",
                "│   │   ├── td_learning.py\n",
                "│   │   └── multi_objective.py\n",
                "│   ├── hpo/                  # Hyperparameter optimization\n",
                "│   │   ├── optuna_hpo.py    # Optuna-based HPO\n",
                "│   │   └── dehb.py          # DEHB multi-fidelity\n",
                "│   └── features/             # Training utilities\n",
                "│       ├── epoch.py         # Epoch preparation\n",
                "│       ├── post_processing.py\n",
                "│       └── time_training.py\n",
                "│\n",
                "├── models/                   # NEURAL NETWORK MODELS\n",
                "│   ├── attention_model.py   # Attention Model (AM)\n",
                "│   ├── deep_decoder_am.py   # Deep Decoder AM (DDAM)\n",
                "│   ├── temporal_am.py       # Temporal AM (TAM)\n",
                "│   ├── gat_lstm_manager.py  # HRL Manager network\n",
                "│   ├── pointer_network.py   # Pointer Network\n",
                "│   ├── critic_network.py    # Value network for PPO/baselines\n",
                "│   ├── hypernet.py          # Hypernetwork for meta-learning\n",
                "│   ├── meta_rnn.py          # Meta-learning RNN\n",
                "│   ├── moe_model.py         # Mixture of Experts\n",
                "│   ├── model_factory.py     # Factory for model instantiation\n",
                "│   ├── modules/             # Atomic building blocks\n",
                "│   │   ├── multi_head_attention.py\n",
                "│   │   ├── graph_convolution.py\n",
                "│   │   ├── feed_forward.py\n",
                "│   │   ├── normalization.py\n",
                "│   │   ├── activation_function.py\n",
                "│   │   ├── skip_connection.py\n",
                "│   │   └── ...\n",
                "│   └── subnets/             # Encoders & Decoders\n",
                "│       ├── gat_encoder.py   # Graph Attention Encoder\n",
                "│       ├── gac_encoder.py   # Graph Attention Conv Encoder\n",
                "│       ├── tgc_encoder.py   # Transformer Graph Conv Encoder\n",
                "│       ├── gcn_encoder.py   # GCN Encoder\n",
                "│       ├── attention_decoder.py\n",
                "│       └── ...\n",
                "│\n",
                "├── envs/                     # ENVIRONMENTS\n",
                "│   ├── base.py              # RL4COEnvBase\n",
                "│   ├── vrpp.py              # VRPP, CVRPP environments\n",
                "│   ├── wcvrp.py             # WCVRP, CWCVRP, SDWCVRP\n",
                "│   ├── swcvrp.py            # SCWCVRP\n",
                "│   └── generators.py        # Data generators\n",
                "│\n",
                "└── configs/                  # CONFIGURATION DATACLASSES\n",
                "    ├── __init__.py          # Root Config\n",
                "    ├── env.py               # EnvConfig\n",
                "    ├── model.py             # ModelConfig\n",
                "    ├── train.py             # TrainConfig\n",
                "    ├── optim.py             # OptimConfig\n",
                "    ├── rl.py                # RLConfig\n",
                "    ├── meta_rl.py           # MetaRLConfig\n",
                "    └── hpo.py               # HPOConfig\n",
                "```"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "section-2",
            "metadata": {},
            "source": [
                "---\n",
                "\n",
                "## 2. Hydra Configuration System\n",
                "\n",
                "The training pipeline uses **Hydra** for configuration management, enabling:\n",
                "- Hierarchical configuration with dataclasses\n",
                "- Command-line overrides\n",
                "- Experiment tracking and reproducibility\n",
                "\n",
                "### 2.1 Configuration Dataclasses\n",
                "\n",
                "All configurations are defined in `logic/src/configs/__init__.py`:"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 3,
            "id": "config-overview",
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Configuration Sections:\n",
                        "==================================================\n",
                        "\n",
                        "EnvConfig:\n",
                        "  name: <class 'str'> = vrpp\n",
                        "  num_loc: <class 'int'> = 50\n",
                        "  min_loc: <class 'float'> = 0.0\n",
                        "  max_loc: <class 'float'> = 1.0\n",
                        "  capacity: typing.Optional[float] = None\n",
                        "  overflow_penalty: <class 'float'> = 1.0\n",
                        "  collection_reward: <class 'float'> = 1.0\n",
                        "  cost_weight: <class 'float'> = 1.0\n",
                        "  prize_weight: <class 'float'> = 1.0\n",
                        "  area: <class 'str'> = riomaior\n",
                        "  waste_type: <class 'str'> = plastic\n",
                        "  focus_graph: typing.Optional[str] = None\n",
                        "  focus_size: <class 'int'> = 0\n",
                        "  eval_focus_size: <class 'int'> = 0\n",
                        "  distance_method: <class 'str'> = ogd\n",
                        "  dm_filepath: typing.Optional[str] = None\n",
                        "  waste_filepath: typing.Optional[str] = None\n",
                        "  vertex_method: <class 'str'> = mmn\n",
                        "  edge_threshold: <class 'float'> = 0.0\n",
                        "  edge_method: typing.Optional[str] = None\n",
                        "  data_distribution: typing.Optional[str] = None\n",
                        "  min_fill: <class 'float'> = 0.0\n",
                        "  max_fill: <class 'float'> = 1.0\n",
                        "  fill_distribution: <class 'str'> = uniform\n",
                        "\n",
                        "ModelConfig:\n",
                        "  name: <class 'str'> = am\n",
                        "  embed_dim: <class 'int'> = 128\n",
                        "  hidden_dim: <class 'int'> = 512\n",
                        "  num_encoder_layers: <class 'int'> = 3\n",
                        "  num_decoder_layers: <class 'int'> = 3\n",
                        "  n_heads: <class 'int'> = 8\n",
                        "  encoder_type: <class 'str'> = gat\n",
                        "  temporal_horizon: <class 'int'> = 0\n",
                        "  tanh_clipping: <class 'float'> = 10.0\n",
                        "  normalization: <class 'str'> = instance\n",
                        "  activation: <class 'str'> = gelu\n",
                        "  dropout: <class 'float'> = 0.1\n",
                        "  mask_inner: <class 'bool'> = True\n",
                        "  mask_logits: <class 'bool'> = True\n",
                        "  mask_graph: <class 'bool'> = False\n",
                        "  spatial_bias: <class 'bool'> = False\n",
                        "  connection_type: <class 'str'> = residual\n",
                        "  num_encoder_sublayers: typing.Optional[int] = None\n",
                        "  num_predictor_layers: typing.Optional[int] = None\n",
                        "  learn_affine: <class 'bool'> = True\n",
                        "  track_stats: <class 'bool'> = False\n",
                        "  epsilon_alpha: <class 'float'> = 1e-05\n",
                        "  momentum_beta: <class 'float'> = 0.1\n",
                        "  lrnorm_k: typing.Optional[float] = None\n",
                        "  gnorm_groups: <class 'int'> = 4\n",
                        "  activation_param: <class 'float'> = 1.0\n",
                        "  activation_threshold: typing.Optional[float] = None\n",
                        "  activation_replacement: typing.Optional[float] = None\n",
                        "  activation_num_parameters: <class 'int'> = 3\n",
                        "  activation_uniform_range: typing.List[float] = <no default>\n",
                        "  aggregation_graph: <class 'str'> = avg\n",
                        "  aggregation_node: <class 'str'> = sum\n",
                        "  spatial_bias_scale: <class 'float'> = 1.0\n",
                        "  hyper_expansion: <class 'int'> = 4\n",
                        "\n",
                        "TrainConfig:\n",
                        "  n_epochs: <class 'int'> = 100\n",
                        "  batch_size: <class 'int'> = 256\n",
                        "  train_data_size: <class 'int'> = 100000\n",
                        "  val_data_size: <class 'int'> = 10000\n",
                        "  val_dataset: typing.Optional[str] = None\n",
                        "  num_workers: <class 'int'> = 0\n",
                        "  precision: <class 'str'> = 16-mixed\n",
                        "  train_time: <class 'bool'> = False\n",
                        "  eval_time_days: <class 'int'> = 1\n",
                        "  accumulation_steps: <class 'int'> = 1\n",
                        "  enable_scaler: <class 'bool'> = False\n",
                        "  checkpoint_epochs: <class 'int'> = 1\n",
                        "  shrink_size: typing.Optional[int] = None\n",
                        "  post_processing_epochs: <class 'int'> = 0\n",
                        "  lr_post_processing: <class 'float'> = 0.001\n",
                        "  efficiency_weight: <class 'float'> = 0.8\n",
                        "  overflow_weight: <class 'float'> = 0.2\n",
                        "  log_step: <class 'int'> = 50\n",
                        "  epoch_start: <class 'int'> = 0\n",
                        "  eval_only: <class 'bool'> = False\n",
                        "  checkpoint_encoder: <class 'bool'> = False\n",
                        "  load_path: typing.Optional[str] = None\n",
                        "  resume: typing.Optional[str] = None\n",
                        "  logs_dir: typing.Optional[str] = None\n",
                        "  model_weights_path: typing.Optional[str] = None\n",
                        "  final_model_path: typing.Optional[str] = None\n",
                        "  eval_batch_size: <class 'int'> = 256\n",
                        "  persistent_workers: <class 'bool'> = True\n",
                        "  pin_memory: <class 'bool'> = False\n",
                        "\n",
                        "OptimConfig:\n",
                        "  optimizer: <class 'str'> = adam\n",
                        "  lr: <class 'float'> = 0.0001\n",
                        "  weight_decay: <class 'float'> = 0.0\n",
                        "  lr_scheduler: typing.Optional[str] = None\n",
                        "  lr_scheduler_kwargs: typing.Dict[str, typing.Any] = <no default>\n",
                        "  lr_critic: <class 'float'> = 0.0001\n",
                        "  lr_decay: <class 'float'> = 1.0\n",
                        "  lr_min_value: <class 'float'> = 0.0\n",
                        "  lr_min_decay: <class 'float'> = 1e-08\n",
                        "\n",
                        "RLConfig:\n",
                        "  algorithm: <class 'str'> = reinforce\n",
                        "  baseline: <class 'str'> = rollout\n",
                        "  bl_warmup_epochs: <class 'int'> = 0\n",
                        "  entropy_weight: <class 'float'> = 0.0\n",
                        "  max_grad_norm: <class 'float'> = 1.0\n",
                        "  ppo_epochs: <class 'int'> = 10\n",
                        "  eps_clip: <class 'float'> = 0.2\n",
                        "  value_loss_weight: <class 'float'> = 0.5\n",
                        "  mini_batch_size: <class 'float'> = 0.25\n",
                        "  sapo_tau_pos: <class 'float'> = 0.1\n",
                        "  sapo_tau_neg: <class 'float'> = 1.0\n",
                        "  dr_grpo_group_size: <class 'int'> = 8\n",
                        "  dr_grpo_epsilon: <class 'float'> = 0.2\n",
                        "  num_augment: <class 'int'> = 1\n",
                        "  num_starts: typing.Optional[int] = None\n",
                        "  augment_fn: <class 'str'> = dihedral8\n",
                        "  symnco_alpha: <class 'float'> = 0.2\n",
                        "  symnco_beta: <class 'float'> = 1.0\n",
                        "  imitation_mode: <class 'str'> = hgs\n",
                        "  imitation_weight: <class 'float'> = 0.0\n",
                        "  imitation_decay: <class 'float'> = 1.0\n",
                        "  imitation_threshold: <class 'float'> = 0.05\n",
                        "  reannealing_threshold: <class 'float'> = 0.05\n",
                        "  reannealing_patience: <class 'int'> = 5\n",
                        "  random_ls_iterations: <class 'int'> = 100\n",
                        "  random_ls_op_probs: typing.Optional[typing.Dict[str, float]] = None\n",
                        "  gdpo_objective_keys: typing.List[str] = <no default>\n",
                        "  gdpo_objective_weights: typing.Optional[typing.List[float]] = None\n",
                        "  gdpo_conditional_key: typing.Optional[str] = None\n",
                        "  gdpo_renormalize: <class 'bool'> = True\n",
                        "  gamma: <class 'float'> = 0.99\n",
                        "  gspo_epsilon: <class 'float'> = 0.2\n",
                        "  gspo_epochs: <class 'int'> = 3\n",
                        "  dr_grpo_epochs: <class 'int'> = 3\n",
                        "  exp_beta: <class 'float'> = 0.8\n",
                        "  bl_alpha: <class 'float'> = 0.05\n",
                        "  imitation_decay_step: <class 'int'> = 1\n",
                        "\n",
                        "HPOConfig:\n",
                        "  method: <class 'str'> = dehbo\n",
                        "  metric: <class 'str'> = reward\n",
                        "  n_trials: <class 'int'> = 20\n",
                        "  n_epochs_per_trial: <class 'int'> = 10\n",
                        "  num_workers: <class 'int'> = 4\n",
                        "  search_space: typing.Dict[str, typing.List[typing.Any]] = <no default>\n",
                        "  hop_range: typing.List[float] = <no default>\n",
                        "  fevals: <class 'int'> = 100\n",
                        "  timeout: typing.Optional[int] = None\n",
                        "  n_startup_trials: <class 'int'> = 5\n",
                        "  n_warmup_steps: <class 'int'> = 3\n",
                        "  min_fidelity: <class 'int'> = 1\n",
                        "  max_fidelity: <class 'int'> = 10\n",
                        "  interval_steps: <class 'int'> = 1\n",
                        "  eta: <class 'float'> = 10.0\n",
                        "  indpb: <class 'float'> = 0.2\n",
                        "  tournsize: <class 'int'> = 3\n",
                        "  cxpb: <class 'float'> = 0.7\n",
                        "  mutpb: <class 'float'> = 0.2\n",
                        "  n_pop: <class 'int'> = 20\n",
                        "  n_gen: <class 'int'> = 10\n",
                        "  cpu_cores: <class 'int'> = 1\n",
                        "  verbose: <class 'int'> = 2\n",
                        "  train_best: <class 'bool'> = True\n",
                        "  local_mode: <class 'bool'> = False\n",
                        "  num_samples: <class 'int'> = 20\n",
                        "  max_tres: <class 'int'> = 14\n",
                        "  reduction_factor: <class 'int'> = 3\n",
                        "  max_failures: <class 'int'> = 3\n",
                        "  grid: typing.List[float] = <no default>\n",
                        "  max_conc: <class 'int'> = 4\n"
                    ]
                }
            ],
            "source": [
                "from dataclasses import dataclass, field\n",
                "from typing import Any, Dict, List, Optional\n",
                "\n",
                "# Examine the configuration structure\n",
                "from logic.src.configs import (\n",
                "    Config,\n",
                "    EnvConfig,\n",
                "    HPOConfig,\n",
                "    ModelConfig,\n",
                "    OptimConfig,\n",
                "    RLConfig,\n",
                "    TrainConfig,\n",
                ")\n",
                "\n",
                "# Print available config sections\n",
                "print(\"Configuration Sections:\")\n",
                "print(\"=\" * 50)\n",
                "for config_cls in [EnvConfig, ModelConfig, TrainConfig, OptimConfig, RLConfig, HPOConfig]:\n",
                "    print(f\"\\n{config_cls.__name__}:\")\n",
                "    for field_name, field_type in config_cls.__annotations__.items():\n",
                "        default = getattr(config_cls, field_name, \"<no default>\")\n",
                "        if callable(default) and hasattr(default, \"__name__\"):\n",
                "            default = f\"<factory: {default.__name__}>\"\n",
                "        print(f\"  {field_name}: {field_type} = {default}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "config-sections",
            "metadata": {},
            "source": [
                "### 2.2 Configuration Sections Explained\n",
                "\n",
                "#### EnvConfig - Environment Settings\n",
                "```python\n",
                "@dataclass\n",
                "class EnvConfig:\n",
                "    name: str = \"vrpp\"           # Problem type: vrpp, wcvrp, cwcvrp, sdwcvrp\n",
                "    num_loc: int = 50             # Number of locations (nodes)\n",
                "    min_loc: float = 0.0          # Min coordinate value\n",
                "    max_loc: float = 1.0          # Max coordinate value\n",
                "    capacity: Optional[float] = None  # Vehicle capacity\n",
                "    overflow_penalty: float = 1.0     # Penalty for bin overflow\n",
                "    collection_reward: float = 1.0    # Reward for waste collection\n",
                "    cost_weight: float = 1.0          # Weight for travel cost\n",
                "    prize_weight: float = 1.0         # Weight for prizes (VRPP)\n",
                "```\n",
                "\n",
                "#### ModelConfig - Neural Network Architecture\n",
                "```python\n",
                "@dataclass\n",
                "class ModelConfig:\n",
                "    name: str = \"am\"              # Model: am, deep_decoder, temporal, pointer, symnco\n",
                "    embed_dim: int = 128          # Embedding dimension\n",
                "    hidden_dim: int = 512         # Hidden layer dimension\n",
                "    num_encoder_layers: int = 3   # Number of encoder layers\n",
                "    num_decoder_layers: int = 3   # Number of decoder layers (deep_decoder)\n",
                "    n_heads: int = 8            # Attention heads\n",
                "    encoder_type: str = \"gat\"     # Encoder type: gat, gcn, mlp\n",
                "    normalization: str = \"instance\"  # Normalization: instance, batch, layer\n",
                "    activation: str = \"gelu\"         # Activation function\n",
                "    dropout: float = 0.1             # Dropout rate\n",
                "```\n",
                "\n",
                "#### RLConfig - RL Algorithm Settings\n",
                "```python\n",
                "@dataclass\n",
                "class RLConfig:\n",
                "    algorithm: str = \"reinforce\"  # reinforce, ppo, sapo, gspo, pomo, symnco, hrl\n",
                "    baseline: str = \"rollout\"     # none, exponential, rollout, critic, pomo\n",
                "    entropy_weight: float = 0.0   # Entropy regularization\n",
                "    max_grad_norm: float = 1.0    # Gradient clipping\n",
                "    \n",
                "    # PPO-specific\n",
                "    ppo_epochs: int = 10          # Inner PPO epochs\n",
                "    eps_clip: float = 0.2         # Clipping epsilon\n",
                "    value_loss_weight: float = 0.5\n",
                "    \n",
                "    # Meta-learning\n",
                "    use_meta: bool = False        # Enable meta-learning wrapper\n",
                "    meta_strategy: str = \"rnn\"    # rnn, bandit, morl, tdl, hypernet\n",
                "```"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "cli-usage",
            "metadata": {},
            "source": [
                "### 2.3 Command-Line Interface Usage\n",
                "\n",
                "The main entry point is `train_lightning.py`, accessed via:\n",
                "\n",
                "```bash\n",
                "python main.py train_lightning [CONFIG_OVERRIDES]\n",
                "```\n",
                "\n",
                "#### Basic Training Examples\n",
                "\n",
                "```bash\n",
                "# Train Attention Model on VRPP with 50 nodes\n",
                "python main.py train_lightning model=am env.name=vrpp env.num_loc=50\n",
                "\n",
                "# Train with PPO algorithm\n",
                "python main.py train_lightning rl.algorithm=ppo rl.ppo_epochs=10\n",
                "\n",
                "# Use different baseline\n",
                "python main.py train_lightning rl.baseline=exponential\n",
                "\n",
                "# Custom training parameters\n",
                "python main.py train_lightning \\\n",
                "    train.n_epochs=100 \\\n",
                "    train.batch_size=256 \\\n",
                "    optim.lr=1e-4\n",
                "```\n",
                "\n",
                "#### Advanced Configurations\n",
                "\n",
                "```bash\n",
                "# POMO with data augmentation\n",
                "python main.py train_lightning \\\n",
                "    rl.algorithm=pomo \\\n",
                "    rl.num_augment=8 \\\n",
                "    rl.augment_fn=dihedral8\n",
                "\n",
                "# Hierarchical RL\n",
                "python main.py train_lightning \\\n",
                "    rl.algorithm=hrl \\\n",
                "    rl.meta_hidden_dim=128\n",
                "\n",
                "# With Meta-Learning wrapper\n",
                "python main.py train_lightning \\\n",
                "    rl.use_meta=true \\\n",
                "    rl.meta_strategy=rnn \\\n",
                "    rl.meta_lr=1e-3\n",
                "\n",
                "# Hyperparameter optimization\n",
                "python main.py train_lightning \\\n",
                "    hpo.n_trials=50 \\\n",
                "    hpo.method=tpe\n",
                "```"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 4,
            "id": "config-example",
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Configuration created:\n",
                        "  Environment: vrpp with 50 locations\n",
                        "  Model: am with 128d embeddings\n",
                        "  Algorithm: reinforce with rollout baseline\n",
                        "  Device: cuda\n"
                    ]
                }
            ],
            "source": [
                "# Programmatically create a configuration\n",
                "from logic.src.configs import Config, EnvConfig, ModelConfig, OptimConfig, RLConfig, TrainConfig\n",
                "\n",
                "# Create custom configuration\n",
                "cfg = Config(\n",
                "    env=EnvConfig(\n",
                "        name=\"vrpp\",\n",
                "        num_loc=50,\n",
                "        capacity=100.0,\n",
                "    ),\n",
                "    model=ModelConfig(\n",
                "        name=\"am\",\n",
                "        embed_dim=128,\n",
                "        num_encoder_layers=3,\n",
                "        n_heads=8,\n",
                "    ),\n",
                "    train=TrainConfig(\n",
                "        n_epochs=10,\n",
                "        batch_size=256,\n",
                "        train_data_size=10000,\n",
                "    ),\n",
                "    optim=OptimConfig(\n",
                "        optimizer=\"adam\",\n",
                "        lr=1e-4,\n",
                "    ),\n",
                "    rl=RLConfig(\n",
                "        algorithm=\"reinforce\",\n",
                "        baseline=\"rollout\",\n",
                "    ),\n",
                "    seed=42,\n",
                "    device=\"cuda\" if torch.cuda.is_available() else \"cpu\",\n",
                ")\n",
                "\n",
                "print(\"Configuration created:\")\n",
                "print(f\"  Environment: {cfg.env.name} with {cfg.env.num_loc} locations\")\n",
                "print(f\"  Model: {cfg.model.name} with {cfg.model.embed_dim}d embeddings\")\n",
                "print(f\"  Algorithm: {cfg.rl.algorithm} with {cfg.rl.baseline} baseline\")\n",
                "print(f\"  Device: {cfg.device}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "section-3",
            "metadata": {},
            "source": [
                "---\n",
                "\n",
                "## 3. PyTorch Lightning Modules\n",
                "\n",
                "All RL algorithms inherit from `RL4COLitModule`, which provides:\n",
                "- Training/validation/test loops\n",
                "- Automatic optimizer configuration\n",
                "- Data loading with generators\n",
                "- Baseline integration\n",
                "- Metric logging\n",
                "\n",
                "### 3.1 Base Module Architecture"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 4,
            "id": "lightning-module",
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "RL4COLitModule.__init__ signature:\n",
                        "(self, env: 'RL4COEnvBase', policy: 'ConstructivePolicy', baseline: 'Optional[str]' = 'rollout', optimizer: 'str' = 'adam', optimizer_kwargs: 'Optional[dict]' = None, lr_scheduler: 'Optional[str]' = None, lr_scheduler_kwargs: 'Optional[dict]' = None, train_data_size: 'int' = 100000, val_data_size: 'int' = 10000, val_dataset_path: 'Optional[str]' = None, batch_size: 'int' = 256, num_workers: 'int' = 4, persistent_workers: 'bool' = True, pin_memory: 'bool' = False, **kwargs)\n",
                        "\n",
                        "Key Methods:\n",
                        "  __init__: \n",
                        "  add_module: Add a child module to the current module.\n",
                        "  all_gather: Gather tensors or collections of tensors from multiple processes.\n",
                        "  apply: Apply ``fn`` recursively to every submodule (as returned by ``.children()``) as well as self.\n",
                        "  backward: Called to perform backward on the loss returned in :meth:`training_step`. Override this hook with your own\n",
                        "  bfloat16: Casts all floating point parameters and buffers to ``bfloat16`` datatype.\n",
                        "  buffers: Return an iterator over module buffers.\n",
                        "  calculate_loss: \n",
                        "  children: Return an iterator over immediate children modules.\n",
                        "  clip_gradients: Handles gradient clipping internally.\n",
                        "  compile: \n",
                        "  configure_callbacks: Configure model-specific callbacks. When the model gets attached, e.g., when ``.fit()`` or ``.test()`` gets\n",
                        "  configure_gradient_clipping: Perform gradient clipping for the optimizer parameters. Called before :meth:`optimizer_step`.\n",
                        "  configure_model: Hook to create modules in a strategy and precision aware context.\n",
                        "  configure_optimizers: Configure optimizer and optional scheduler.\n",
                        "  configure_sharded_model: Deprecated.\n",
                        "  cpu: See :meth:`torch.nn.Module.cpu`.\n",
                        "  cuda: Moves all model parameters and buffers to the GPU. This also makes associated parameters and buffers\n",
                        "  double: See :meth:`torch.nn.Module.double`.\n",
                        "  eval: Set the module in evaluation mode.\n",
                        "  extra_repr: Set the extra representation of the module.\n",
                        "  float: See :meth:`torch.nn.Module.float`.\n",
                        "  forward: Same as :meth:`torch.nn.Module.forward`.\n",
                        "  freeze: Freeze all params for inference.\n",
                        "  get_buffer: Return the buffer given by ``target`` if it exists, otherwise throw an error.\n",
                        "  get_extra_state: Return any extra state to include in the module's state_dict.\n",
                        "  get_parameter: Return the parameter given by ``target`` if it exists, otherwise throw an error.\n",
                        "  get_submodule: Return the submodule given by ``target`` if it exists, otherwise throw an error.\n",
                        "  half: See :meth:`torch.nn.Module.half`.\n",
                        "  ipu: Move all model parameters and buffers to the IPU.\n",
                        "  load_from_checkpoint: Primary way of loading a model from a checkpoint. When Lightning saves a checkpoint it stores the arguments\n",
                        "  load_state_dict: Copy parameters and buffers from :attr:`state_dict` into this module and its descendants.\n",
                        "  log: Log a key, value pair.\n",
                        "  log_dict: Log a dictionary of values at once.\n",
                        "  lr_scheduler_step: Override this method to adjust the default way the :class:`~pytorch_lightning.trainer.trainer.Trainer` calls\n",
                        "  lr_schedulers: Returns the learning rate scheduler(s) that are being used during training. Useful for manual optimization.\n",
                        "  manual_backward: Call this directly from your :meth:`training_step` when doing optimizations manually. By using this,\n",
                        "  modules: Return an iterator over all modules in the network.\n",
                        "  named_buffers: Return an iterator over module buffers, yielding both the name of the buffer as well as the buffer itself.\n",
                        "  named_children: Return an iterator over immediate children modules, yielding both the name of the module as well as the module itself.\n",
                        "  named_modules: Return an iterator over all modules in the network, yielding both the name of the module as well as the module itself.\n",
                        "  named_parameters: Return an iterator over module parameters, yielding both the name of the parameter as well as the parameter itself.\n",
                        "  on_after_backward: Called after ``loss.backward()`` and before optimizers are stepped.\n",
                        "  on_after_batch_transfer: Override to alter or apply batch augmentations to your batch after it is transferred to the device.\n",
                        "  on_before_backward: Called before ``loss.backward()``.\n",
                        "  on_before_batch_transfer: Override to alter or apply batch augmentations to your batch before it is transferred to the device.\n",
                        "  on_before_optimizer_step: Called before ``optimizer.step()``.\n",
                        "  on_before_zero_grad: Called after ``training_step()`` and before ``optimizer.zero_grad()``.\n",
                        "  on_fit_end: Called at the very end of fit.\n",
                        "  on_fit_start: Called at the very beginning of fit.\n",
                        "  on_load_checkpoint: Called by Lightning to restore your model. If you saved something with :meth:`on_save_checkpoint` this is\n",
                        "  on_predict_batch_end: Called in the predict loop after the batch.\n",
                        "  on_predict_batch_start: Called in the predict loop before anything happens for that batch.\n",
                        "  on_predict_end: Called at the end of predicting.\n",
                        "  on_predict_epoch_end: Called at the end of predicting.\n",
                        "  on_predict_epoch_start: Called at the beginning of predicting.\n",
                        "  on_predict_model_eval: Called when the predict loop starts.\n",
                        "  on_predict_start: Called at the beginning of predicting.\n",
                        "  on_save_checkpoint: Called by Lightning when saving a checkpoint to give you a chance to store anything else you might want to\n",
                        "  on_test_batch_end: Called in the test loop after the batch.\n",
                        "  on_test_batch_start: Called in the test loop before anything happens for that batch.\n",
                        "  on_test_end: Called at the end of testing.\n",
                        "  on_test_epoch_end: Called in the test loop at the very end of the epoch.\n",
                        "  on_test_epoch_start: Called in the test loop at the very beginning of the epoch.\n",
                        "  on_test_model_eval: Called when the test loop starts.\n",
                        "  on_test_model_train: Called when the test loop ends.\n",
                        "  on_test_start: Called at the beginning of testing.\n",
                        "  on_train_batch_end: Called in the training loop after the batch.\n",
                        "  on_train_batch_start: Called in the training loop before anything happens for that batch.\n",
                        "  on_train_end: Called at the end of training before logger experiment is closed.\n",
                        "  on_train_epoch_end: Update baseline and regenerate dataset.\n",
                        "  on_train_epoch_start: Prepare dataset for the new epoch (e.g. wrap with baseline).\n",
                        "  on_train_start: Called at the beginning of training after sanity check.\n",
                        "  on_validation_batch_end: Called in the validation loop after the batch.\n",
                        "  on_validation_batch_start: Called in the validation loop before anything happens for that batch.\n",
                        "  on_validation_end: Called at the end of validation.\n",
                        "  on_validation_epoch_end: Called in the validation loop at the very end of the epoch.\n",
                        "  on_validation_epoch_start: Called in the validation loop at the very beginning of the epoch.\n",
                        "  on_validation_model_eval: Called when the validation loop starts.\n",
                        "  on_validation_model_train: Called when the validation loop ends.\n",
                        "  on_validation_model_zero_grad: Called by the training loop to release gradients before entering the validation loop.\n",
                        "  on_validation_start: Called at the beginning of validation.\n",
                        "  optimizer_step: Override this method to adjust the default way the :class:`~pytorch_lightning.trainer.trainer.Trainer` calls\n",
                        "  optimizer_zero_grad: Override this method to change the default behaviour of ``optimizer.zero_grad()``.\n",
                        "  optimizers: Returns the optimizer(s) that are being used during training. Useful for manual optimization.\n",
                        "  parameters: Return an iterator over module parameters.\n",
                        "  predict_dataloader: An iterable or collection of iterables specifying prediction samples.\n",
                        "  predict_step: Step function called during :meth:`~pytorch_lightning.trainer.trainer.Trainer.predict`. By default, it calls\n",
                        "  prepare_data: Use this to download and prepare data. Downloading and saving data with multiple processes (distributed\n",
                        "  print: Prints only from process 0. Use this in any distributed mode to log only once.\n",
                        "  register_backward_hook: Register a backward hook on the module.\n",
                        "  register_buffer: Add a buffer to the module.\n",
                        "  register_forward_hook: Register a forward hook on the module.\n",
                        "  register_forward_pre_hook: Register a forward pre-hook on the module.\n",
                        "  register_full_backward_hook: Register a backward hook on the module.\n",
                        "  register_full_backward_pre_hook: Register a backward pre-hook on the module.\n",
                        "  register_load_state_dict_post_hook: Register a post hook to be run after module's ``load_state_dict`` is called.\n",
                        "  register_module: Alias for :func:`add_module`.\n",
                        "  register_parameter: Add a parameter to the module.\n",
                        "  register_state_dict_pre_hook: Register a pre-hook for the :meth:`~torch.nn.Module.load_state_dict` method.\n",
                        "  requires_grad_: Change if autograd should record operations on parameters in this module.\n",
                        "  save_hyperparameters: Save arguments to ``hparams`` attribute.\n",
                        "  save_weights: \n",
                        "  set_extra_state: Set extra state contained in the loaded `state_dict`.\n",
                        "  setup: \n",
                        "  share_memory: See :meth:`torch.Tensor.share_memory_`.\n",
                        "  shared_step: \n",
                        "  state_dict: Return a dictionary containing references to the whole state of the module.\n",
                        "  teardown: Called at the end of fit (train + validate), validate, test, or predict.\n",
                        "  test_dataloader: An iterable or collection of iterables specifying test samples.\n",
                        "  test_step: \n",
                        "  to: See :meth:`torch.nn.Module.to`.\n",
                        "  to_empty: Move the parameters and buffers to the specified device without copying storage.\n",
                        "  to_onnx: Saves the model in ONNX format.\n",
                        "  to_tensorrt: Export the model to ScriptModule or GraphModule using TensorRT compile backend.\n",
                        "  to_torchscript: By default compiles the whole model to a :class:`~torch.jit.ScriptModule`. If you want to use tracing,\n",
                        "  toggle_optimizer: Makes sure only the gradients of the current optimizer's parameters are calculated in the training step to\n",
                        "  toggled_optimizer: Makes sure only the gradients of the current optimizer's parameters are calculated in the training step to\n",
                        "  train: Set the module in training mode.\n",
                        "  train_dataloader: \n",
                        "  training_step: \n",
                        "  transfer_batch_to_device: Override this hook if your :class:`~torch.utils.data.DataLoader` returns tensors wrapped in a custom data\n",
                        "  type: See :meth:`torch.nn.Module.type`.\n",
                        "  unfreeze: Unfreeze all parameters for training.\n",
                        "  untoggle_optimizer: Resets the state of required gradients that were toggled with :meth:`toggle_optimizer`.\n",
                        "  val_dataloader: \n",
                        "  validation_step: \n",
                        "  xpu: Move all model parameters and buffers to the XPU.\n",
                        "  zero_grad: Reset gradients of all model parameters.\n"
                    ]
                }
            ],
            "source": [
                "import inspect\n",
                "\n",
                "from logic.src.pipeline.rl.core.base import RL4COLitModule\n",
                "\n",
                "# Show the base class signature\n",
                "print(\"RL4COLitModule.__init__ signature:\")\n",
                "print(inspect.signature(RL4COLitModule.__init__))\n",
                "\n",
                "print(\"\\nKey Methods:\")\n",
                "for name, method in inspect.getmembers(RL4COLitModule, predicate=inspect.isfunction):\n",
                "    if not name.startswith(\"_\") or name in [\"__init__\"]:\n",
                "        doc = method.__doc__\n",
                "        first_line = doc.split(\"\\n\")[0].strip() if doc else \"No docstring\"\n",
                "        print(f\"  {name}: {first_line}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "module-lifecycle",
            "metadata": {},
            "source": [
                "### 3.2 Training Lifecycle\n",
                "\n",
                "```python\n",
                "class RL4COLitModule(pl.LightningModule, ABC):\n",
                "    \"\"\"\n",
                "    Training Lifecycle:\n",
                "    \n",
                "    1. setup(stage='fit'):\n",
                "       - Create train_dataset (GeneratorDataset)\n",
                "       - Create val_dataset\n",
                "       \n",
                "    2. for epoch in range(n_epochs):\n",
                "       \n",
                "       3. on_train_epoch_start():\n",
                "          - Wrap dataset with baseline (if RolloutBaseline)\n",
                "          \n",
                "       4. for batch in train_dataloader():\n",
                "          \n",
                "          5. training_step(batch):\n",
                "             - Unwrap batch (baseline values)\n",
                "             - shared_step():\n",
                "               a. env.reset(batch)\n",
                "               b. policy(td, env, strategy=\"sampling\")\n",
                "               c. calculate_loss()  # Algorithm-specific\n",
                "             - return loss\n",
                "             \n",
                "          6. on_before_optimizer_step():\n",
                "             - Gradient clipping\n",
                "             \n",
                "       7. validation_step(batch):\n",
                "          - shared_step() with strategy=\"greedy\"\n",
                "          \n",
                "       8. on_train_epoch_end():\n",
                "          - baseline.epoch_callback()\n",
                "          - Optionally regenerate dataset\n",
                "    \"\"\"\n",
                "```\n",
                "\n",
                "### 3.3 Key Methods Explained\n",
                "\n",
                "#### `shared_step()` - Common Training Logic\n",
                "```python\n",
                "def shared_step(self, batch, batch_idx, phase):\n",
                "    # 1. Unwrap baseline values if present\n",
                "    batch, baseline_val = self.baseline.unwrap_batch(batch)\n",
                "    \n",
                "    # 2. Move to device\n",
                "    batch = batch.to(self.device)\n",
                "    \n",
                "    # 3. Reset environment with batch\n",
                "    td = self.env.reset(batch)\n",
                "    \n",
                "    # 4. Run policy\n",
                "    out = self.policy(\n",
                "        td, self.env,\n",
                "        strategy=\"sampling\" if phase == \"train\" else \"greedy\"\n",
                "    )\n",
                "    \n",
                "    # 5. Compute loss (training only)\n",
                "    if phase == \"train\":\n",
                "        out[\"loss\"] = self.calculate_loss(td, out, batch_idx)\n",
                "    \n",
                "    # 6. Log metrics\n",
                "    self.log(f\"{phase}/reward\", out[\"reward\"].mean())\n",
                "    \n",
                "    return out\n",
                "```\n",
                "\n",
                "#### `calculate_loss()` - Algorithm-Specific\n",
                "This is the **abstract method** that each RL algorithm must implement."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 5,
            "id": "show-calculate-loss",
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "REINFORCE.calculate_loss source:\n",
                        "    def calculate_loss(\n",
                        "        self,\n",
                        "        td: TensorDict,\n",
                        "        out: dict,\n",
                        "        batch_idx: int,\n",
                        "        env: Optional[\"RL4COEnvBase\"] = None,\n",
                        "    ) -> torch.Tensor:\n",
                        "        \"\"\"\n",
                        "        Compute REINFORCE loss.\n",
                        "\n",
                        "        Loss = -E[(R - b) * log π(a|s)]\n",
                        "        \"\"\"\n",
                        "        reward = out[\"reward\"]\n",
                        "        log_likelihood = out[\"log_likelihood\"]\n",
                        "\n",
                        "        # Get baseline\n",
                        "        if hasattr(self, \"_current_baseline_val\") and self._current_baseline_val is not None:\n",
                        "            baseline_val = self._current_baseline_val\n",
                        "        else:\n",
                        "            baseline_val = self.baseline.eval(td, reward, env=env)\n",
                        "\n",
                        "        # Advantage\n",
                        "        advantage = reward - baseline_val\n",
                        "\n",
                        "        # Normalize advantage (optional but helps stability)\n",
                        "        advantage = (advantage - advantage.mean()) / (advantage.std() + 1e-8)\n",
                        "\n",
                        "        # Policy gradient loss\n",
                        "        loss = -(advantage.detach() * log_likelihood).mean()\n",
                        "\n",
                        "        # Entropy bonus (if applicable)\n",
                        "        if self.entropy_weight > 0 and \"entropy\" in out:\n",
                        "            loss = loss - self.entropy_weight * out[\"entropy\"].mean()\n",
                        "\n",
                        "        # Log components\n",
                        "        self.log(\"train/advantage\", advantage.mean(), sync_dist=True)\n",
                        "        self.log(\"train/baseline\", baseline_val.mean(), sync_dist=True)\n",
                        "\n",
                        "        return loss\n",
                        "\n"
                    ]
                }
            ],
            "source": [
                "# Example: REINFORCE loss implementation\n",
                "from logic.src.pipeline.rl.core.reinforce import REINFORCE\n",
                "\n",
                "# Show the calculate_loss implementation\n",
                "print(\"REINFORCE.calculate_loss source:\")\n",
                "print(inspect.getsource(REINFORCE.calculate_loss))"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "section-4",
            "metadata": {},
            "source": [
                "---\n",
                "\n",
                "## 4. Complete RL Algorithms Reference\n",
                "\n",
                "The pipeline supports **11 RL algorithms**, each suited for different scenarios.\n",
                "\n",
                "### 4.1 Algorithm Overview\n",
                "\n",
                "| Algorithm | File | Loss Function | Best For |\n",
                "|-----------|------|---------------|----------|\n",
                "| **REINFORCE** | `reinforce.py` | $-\\mathbb{E}[(R-b) \\log \\pi]$ | Simple baseline, debugging |\n",
                "| **PPO** | `ppo.py` | Clipped surrogate + Value | Stable training |\n",
                "| **SAPO** | `sapo.py` | Soft adaptive clipping | When PPO is too aggressive |\n",
                "| **GSPO** | `gspo.py` | Gradient-scaled proxy | Sequence-level optimization |\n",
                "| **GDPO** | `gdpo.py` | Group distributional | Multi-objective optimization |\n",
                "| **DRGRPO** | `dr_grpo.py` | Divergence-regularized | Preventing policy collapse |\n",
                "| **POMO** | `pomo.py` | Multi-start baseline | Symmetric problems (TSP, VRP) |\n",
                "| **SymNCO** | `symnco.py` | Symmetry-aware loss | Exploiting problem symmetries |\n",
                "| **ImitationLearning** | `imitation.py` | Cross-entropy with expert | Bootstrapping from classical solvers |\n",
                "| **AdaptiveImitation** | `adaptive_imitation.py` | IL → RL transition | Curriculum learning |\n",
                "| **HRLModule** | `hrl.py` | Hierarchical PPO | Temporal decisions (when + where) |\n",
                "\n",
                "### 4.2 Algorithm Selection Guide\n",
                "\n",
                "```\n",
                "                        ┌─────────────────────────┐\n",
                "                        │   Starting a project?   │\n",
                "                        └───────────┬─────────────┘\n",
                "                                    │\n",
                "                    ┌───────────────┼───────────────┐\n",
                "                    │               │               │\n",
                "              Have expert      Need stable     Problem has\n",
                "               solutions?       training?       symmetry?\n",
                "                    │               │               │\n",
                "                    ▼               ▼               ▼\n",
                "            ┌───────────┐   ┌───────────┐   ┌───────────┐\n",
                "            │ Imitation │   │    PPO    │   │   POMO    │\n",
                "            │ Adaptive  │   │   SAPO    │   │  SymNCO   │\n",
                "            └───────────┘   │   GSPO    │   └───────────┘\n",
                "                            └───────────┘\n",
                "                                    │\n",
                "                            Simple case?\n",
                "                                    │\n",
                "                                    ▼\n",
                "                            ┌───────────┐\n",
                "                            │ REINFORCE │\n",
                "                            └───────────┘\n",
                "```\n",
                "\n",
                "### 4.3 Detailed Algorithm Descriptions\n",
                "\n",
                "#### REINFORCE (Vanilla Policy Gradient)\n",
                "```python\n",
                "# Loss: -E[(R - baseline) * log π(a|s)]\n",
                "from logic.src.pipeline.rl import REINFORCE\n",
                "\n",
                "module = REINFORCE(\n",
                "    env=env, policy=policy,\n",
                "    baseline=\"rollout\",        # none, exponential, rollout, critic\n",
                "    entropy_weight=0.01,       # Entropy regularization\n",
                "    max_grad_norm=1.0,         # Gradient clipping\n",
                ")\n",
                "```\n",
                "\n",
                "#### PPO (Proximal Policy Optimization)\n",
                "```python\n",
                "# Loss: min(r*A, clip(r, 1-ε, 1+ε)*A) + c*L_value - β*H(π)\n",
                "from logic.src.pipeline.rl import PPO\n",
                "\n",
                "module = PPO(\n",
                "    env=env, policy=policy, critic=critic,\n",
                "    ppo_epochs=10,             # Inner optimization epochs\n",
                "    eps_clip=0.2,              # Clipping epsilon\n",
                "    value_loss_weight=0.5,     # Critic loss weight\n",
                "    mini_batch_size=0.25,      # Fraction of batch per mini-batch\n",
                ")\n",
                "```\n",
                "\n",
                "#### SAPO (Self-Adaptive Policy Optimization)\n",
                "```python\n",
                "# Soft gating instead of hard clipping\n",
                "from logic.src.pipeline.rl import SAPO\n",
                "\n",
                "module = SAPO(\n",
                "    env=env, policy=policy, critic=critic,\n",
                "    sapo_tau_pos=0.1,          # Positive advantage temperature\n",
                "    sapo_tau_neg=1.0,          # Negative advantage temperature\n",
                ")\n",
                "```\n",
                "\n",
                "#### POMO (Policy Optimization with Multiple Optima)\n",
                "```python\n",
                "# Multi-start + data augmentation for symmetric problems\n",
                "from logic.src.pipeline.rl import POMO\n",
                "\n",
                "module = POMO(\n",
                "    env=env, policy=policy,\n",
                "    num_augment=8,             # Dihedral group D8\n",
                "    augment_fn=\"dihedral8\",    # Augmentation function\n",
                "    num_starts=None,           # Defaults to num_loc\n",
                ")\n",
                "```\n",
                "\n",
                "#### SymNCO (Symmetry-aware NCO)\n",
                "```python\n",
                "# Exploits invariances in CO problems\n",
                "from logic.src.pipeline.rl import SymNCO\n",
                "\n",
                "module = SymNCO(\n",
                "    env=env, policy=policy,\n",
                "    num_augment=8,\n",
                "    symnco_alpha=0.2,          # Symmetry loss weight\n",
                "    symnco_beta=1.0,           # Consistency loss weight\n",
                ")\n",
                "```\n",
                "\n",
                "#### ImitationLearning (Learn from Expert)\n",
                "```python\n",
                "# Supervised learning from classical solvers\n",
                "from logic.src.pipeline.rl import ImitationLearning\n",
                "\n",
                "module = ImitationLearning(\n",
                "    env=env, policy=policy,\n",
                "    imitation_mode=\"hgs\",      # Expert: hgs, alns, gurobi\n",
                "    imitation_weight=1.0,      # IL loss weight\n",
                ")\n",
                "```\n",
                "\n",
                "#### AdaptiveImitation (IL → RL Transition)\n",
                "```python\n",
                "# Curriculum: start with IL, transition to RL\n",
                "from logic.src.pipeline.rl import AdaptiveImitation\n",
                "\n",
                "module = AdaptiveImitation(\n",
                "    env=env, policy=policy,\n",
                "    imitation_weight=1.0,      # Initial IL weight\n",
                "    imitation_decay=0.95,      # Decay per epoch\n",
                "    imitation_threshold=0.05,  # Minimum IL weight\n",
                "    reannealing_threshold=0.05,# Re-anneal if gap exceeds this\n",
                "    reannealing_patience=5,    # Epochs before re-annealing\n",
                ")\n",
                "```\n",
                "\n",
                "#### HRLModule (Hierarchical RL)\n",
                "```python\n",
                "# Manager (when to collect) + Worker (route planning)\n",
                "from logic.src.pipeline.rl import HRLModule\n",
                "\n",
                "module = HRLModule(\n",
                "    env=env,\n",
                "    manager=manager_network,   # GATLSTManager\n",
                "    worker=worker_policy,      # AttentionModelPolicy\n",
                "    gamma=0.99,                # Discount factor\n",
                ")\n",
                "```"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 6,
            "id": "reinforce-example",
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "REINFORCE module created:\n",
                        "  Environment: VRPPEnv\n",
                        "  Policy parameters: 91,520\n",
                        "  Baseline: exponential\n"
                    ]
                }
            ],
            "source": [
                "from logic.src.envs import get_env\n",
                "from logic.src.models.policies import AttentionModelPolicy\n",
                "from logic.src.pipeline.rl import REINFORCE\n",
                "\n",
                "# Create environment\n",
                "env = get_env(\"vrpp\", num_loc=20, device=\"cpu\")\n",
                "\n",
                "# Create policy\n",
                "policy = AttentionModelPolicy(\n",
                "    env_name=\"vrpp\",\n",
                "    embed_dim=64,\n",
                "    hidden_dim=128,\n",
                "    n_encode_layers=2,\n",
                "    n_heads=4,\n",
                ")\n",
                "\n",
                "# Create REINFORCE module\n",
                "reinforce_module = REINFORCE(\n",
                "    env=env,\n",
                "    policy=policy,\n",
                "    baseline=\"exponential\",  # Using exponential baseline for demo\n",
                "    optimizer=\"adam\",\n",
                "    optimizer_kwargs={\"lr\": 1e-4},\n",
                "    train_data_size=1000,\n",
                "    val_data_size=100,\n",
                "    batch_size=64,\n",
                "    entropy_weight=0.01,\n",
                "    max_grad_norm=1.0,\n",
                ")\n",
                "\n",
                "print(\"REINFORCE module created:\")\n",
                "print(f\"  Environment: {env.__class__.__name__}\")\n",
                "print(f\"  Policy parameters: {sum(p.numel() for p in policy.parameters()):,}\")\n",
                "print(f\"  Baseline: {reinforce_module.baseline_type}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "ppo-section",
            "metadata": {},
            "source": [
                "### 4.2 PPO (Proximal Policy Optimization)\n",
                "\n",
                "PPO performs multiple optimization epochs per batch with clipped surrogate objective:\n",
                "\n",
                "$$\\mathcal{L}^{CLIP}(\\theta) = \\mathbb{E}\\left[\\min\\left(r_t(\\theta)\\hat{A}_t, \\text{clip}(r_t(\\theta), 1-\\epsilon, 1+\\epsilon)\\hat{A}_t\\right)\\right]$$\n",
                "\n",
                "where $r_t(\\theta) = \\frac{\\pi_\\theta(a_t|s_t)}{\\pi_{\\theta_{old}}(a_t|s_t)}$"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "ppo-example",
            "metadata": {},
            "outputs": [],
            "source": [
                "# PPO Example - requires environment and policy from previous cell\n",
                "from logic.src.envs import get_env\n",
                "from logic.src.models.policies import AttentionModelPolicy\n",
                "from logic.src.models.policies.critic import create_critic_from_actor\n",
                "from logic.src.pipeline.rl import PPO\n",
                "\n",
                "# Create environment\n",
                "env = get_env(\"vrpp\", num_loc=20, device=\"cpu\")\n",
                "\n",
                "# Create policy\n",
                "policy = AttentionModelPolicy(\n",
                "    env_name=\"vrpp\",\n",
                "    embed_dim=64,\n",
                "    hidden_dim=128,\n",
                "    n_encode_layers=2,\n",
                "    n_heads=4,\n",
                ")\n",
                "\n",
                "# Create critic from actor architecture\n",
                "critic = create_critic_from_actor(\n",
                "    policy,\n",
                "    env_name=\"vrpp\",\n",
                "    embed_dim=64,\n",
                "    hidden_dim=128,\n",
                "    n_layers=2,\n",
                "    n_heads=4,\n",
                ")\n",
                "\n",
                "# Create PPO module\n",
                "ppo_module = PPO(\n",
                "    env=env,\n",
                "    policy=policy,\n",
                "    critic=critic,\n",
                "    ppo_epochs=10,  # Inner optimization epochs\n",
                "    eps_clip=0.2,  # Clipping epsilon\n",
                "    value_loss_weight=0.5,  # Critic loss weight\n",
                "    mini_batch_size=0.25,  # 25% of batch per mini-batch\n",
                "    optimizer=\"adam\",\n",
                "    optimizer_kwargs={\"lr\": 1e-4},\n",
                "    train_data_size=1000,\n",
                "    val_data_size=100,\n",
                "    batch_size=64,\n",
                ")\n",
                "\n",
                "print(\"PPO module created:\")\n",
                "print(f\"  Actor parameters: {sum(p.numel() for p in policy.parameters()):,}\")\n",
                "print(f\"  Critic parameters: {sum(p.numel() for p in critic.parameters()):,}\")\n",
                "print(f\"  PPO epochs: {ppo_module.ppo_epochs}\")\n",
                "print(f\"  Clip epsilon: {ppo_module.eps_clip}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "pomo-section",
            "metadata": {},
            "source": [
                "### 4.3 POMO (Policy Optimization with Multiple Optima)\n",
                "\n",
                "POMO exploits problem symmetries through:\n",
                "1. **Data Augmentation**: Dihedral transformations (rotations, reflections)\n",
                "2. **Multi-start Decoding**: Try multiple starting nodes\n",
                "3. **Shared Baseline**: Mean reward across all augmentations/starts"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "pomo-example",
            "metadata": {},
            "outputs": [],
            "source": [
                "# POMO Example\n",
                "from logic.src.envs import get_env\n",
                "from logic.src.models.policies import AttentionModelPolicy\n",
                "from logic.src.pipeline.rl import POMO\n",
                "\n",
                "# Create environment\n",
                "env = get_env(\"vrpp\", num_loc=20, device=\"cpu\")\n",
                "\n",
                "# Create policy\n",
                "policy = AttentionModelPolicy(\n",
                "    env_name=\"vrpp\",\n",
                "    embed_dim=64,\n",
                "    hidden_dim=128,\n",
                "    n_encode_layers=2,\n",
                "    n_heads=4,\n",
                ")\n",
                "\n",
                "# Create POMO module\n",
                "pomo_module = POMO(\n",
                "    env=env,\n",
                "    policy=policy,\n",
                "    num_augment=8,  # Dihedral group D8\n",
                "    augment_fn=\"dihedral8\",  # Augmentation function\n",
                "    num_starts=None,  # Defaults to num_loc\n",
                "    optimizer=\"adam\",\n",
                "    optimizer_kwargs={\"lr\": 1e-4},\n",
                "    train_data_size=1000,\n",
                "    val_data_size=100,\n",
                "    batch_size=64,\n",
                ")\n",
                "\n",
                "print(\"POMO module created:\")\n",
                "print(f\"  Number of augmentations: {pomo_module.num_augment}\")\n",
                "print(f\"  Augmentation function: {pomo_module.augment_fn}\")\n",
                "print(f\"  Number of starts: {pomo_module.num_starts or 'auto (num_loc)'}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "algorithm-comparison",
            "metadata": {},
            "source": [
                "### 4.4 CLI Commands for All Algorithms\n",
                "\n",
                "```bash\n",
                "# REINFORCE (default)\n",
                "python main.py train_lightning rl.algorithm=reinforce rl.baseline=rollout\n",
                "\n",
                "# PPO\n",
                "python main.py train_lightning rl.algorithm=ppo rl.ppo_epochs=10 rl.eps_clip=0.2\n",
                "\n",
                "# SAPO\n",
                "python main.py train_lightning rl.algorithm=sapo rl.sapo_tau_pos=0.1 rl.sapo_tau_neg=1.0\n",
                "\n",
                "# GSPO\n",
                "python main.py train_lightning rl.algorithm=gspo rl.gspo_epsilon=0.2 rl.gspo_epochs=3\n",
                "\n",
                "# GDPO (multi-objective)\n",
                "python main.py train_lightning rl.algorithm=gdpo \\\n",
                "    'rl.gdpo_objective_keys=[\"cost\", \"overflow\"]' \\\n",
                "    'rl.gdpo_objective_weights=[0.8, 0.2]'\n",
                "\n",
                "# DR-GRPO\n",
                "python main.py train_lightning rl.algorithm=dr_grpo rl.dr_grpo_group_size=8 rl.dr_grpo_epsilon=0.2\n",
                "\n",
                "# POMO\n",
                "python main.py train_lightning rl.algorithm=pomo rl.num_augment=8 rl.augment_fn=dihedral8\n",
                "\n",
                "# SymNCO\n",
                "python main.py train_lightning rl.algorithm=symnco rl.num_augment=8 rl.symnco_alpha=0.2\n",
                "\n",
                "# Imitation Learning\n",
                "python main.py train_lightning rl.algorithm=imitation rl.imitation_mode=hgs\n",
                "\n",
                "# Adaptive Imitation\n",
                "python main.py train_lightning rl.algorithm=adaptive_imitation \\\n",
                "    rl.imitation_weight=1.0 rl.imitation_decay=0.95\n",
                "\n",
                "# HRL (Hierarchical)\n",
                "python main.py train_lightning rl.algorithm=hrl rl.gamma=0.99\n",
                "```"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "section-5",
            "metadata": {},
            "source": [
                "---\n",
                "\n",
                "## 5. Baselines for Variance Reduction\n",
                "\n",
                "Baselines reduce variance in policy gradient estimates:\n",
                "\n",
                "$$\\nabla_\\theta J(\\theta) = \\mathbb{E}\\left[(R - b(s)) \\nabla_\\theta \\log \\pi_\\theta(a|s)\\right]$$\n",
                "\n",
                "### 5.1 Available Baselines"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "baselines-overview",
            "metadata": {},
            "outputs": [],
            "source": [
                "# Baselines Overview\n",
                "from logic.src.pipeline.rl.core.baselines import (\n",
                "    BASELINE_REGISTRY,\n",
                "    CriticBaseline,\n",
                "    ExponentialBaseline,\n",
                "    NoBaseline,\n",
                "    POMOBaseline,\n",
                "    RolloutBaseline,\n",
                "    WarmupBaseline,\n",
                "    get_baseline,\n",
                ")\n",
                "\n",
                "print(\"Available Baselines:\")\n",
                "print(\"=\" * 60)\n",
                "for name, cls in BASELINE_REGISTRY.items():\n",
                "    doc = cls.__doc__ or \"No description\"\n",
                "    first_line = doc.split(\"\\n\")[0].strip()\n",
                "    print(f\"  {name:15} - {first_line}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "baseline-details",
            "metadata": {},
            "source": [
                "### 5.2 Baseline Comparison\n",
                "\n",
                "| Baseline | Formula | Pros | Cons |\n",
                "|----------|---------|------|------|\n",
                "| **none** | $b = 0$ | Simple | High variance |\n",
                "| **exponential** | $b = \\beta \\cdot b + (1-\\beta) \\cdot \\bar{R}$ | Low compute | Biased |\n",
                "| **rollout** | $b = R^{greedy}_{\\pi_{old}}$ | Unbiased | Expensive |\n",
                "| **critic** | $b = V_\\phi(s)$ | Learned | Requires critic network |\n",
                "| **pomo** | $b = \\text{mean}(R_{starts})$ | Multi-solution | POMO-specific |\n",
                "\n",
                "### 5.3 Rollout Baseline Deep Dive\n",
                "\n",
                "The **RolloutBaseline** is the most commonly used baseline:\n",
                "\n",
                "1. **Pre-compute**: At epoch start, run greedy rollout on training data\n",
                "2. **Store**: Keep baseline values alongside training samples\n",
                "3. **Use**: During training, advantage = reward - stored_baseline\n",
                "4. **Update**: Periodically update baseline policy if improved"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "rollout-baseline-example",
            "metadata": {},
            "outputs": [],
            "source": [
                "# Demonstrate RolloutBaseline usage\n",
                "from logic.src.envs import get_env\n",
                "from logic.src.models.policies import AttentionModelPolicy\n",
                "from logic.src.pipeline.rl.core.baselines import RolloutBaseline\n",
                "\n",
                "# Create policy for demonstration\n",
                "env = get_env(\"vrpp\", num_loc=20, device=\"cpu\")\n",
                "policy = AttentionModelPolicy(\n",
                "    env_name=\"vrpp\",\n",
                "    embed_dim=64,\n",
                "    hidden_dim=128,\n",
                "    n_encode_layers=2,\n",
                "    n_heads=4,\n",
                ")\n",
                "\n",
                "# Create baseline with policy\n",
                "rollout_bl = RolloutBaseline(\n",
                "    policy=policy,\n",
                "    update_every=1,  # Check for update every epoch\n",
                "    bl_alpha=0.05,  # Significance level for T-test\n",
                ")\n",
                "\n",
                "print(\"RolloutBaseline:\")\n",
                "print(f\"  Update frequency: every {rollout_bl.update_every} epoch(s)\")\n",
                "print(f\"  T-test alpha: {rollout_bl.bl_alpha}\")\n",
                "print(f\"  Has baseline policy: {rollout_bl.baseline_policy is not None}\")\n",
                "\n",
                "# Key methods\n",
                "print(\"\\nKey Methods:\")\n",
                "print(\"  wrap_dataset(dataset, policy, env) - Pre-compute baseline values\")\n",
                "print(\"  unwrap_batch(batch) -> (data, baseline_val) - Extract baseline\")\n",
                "print(\"  eval(td, reward, env) - Compute baseline on-the-fly\")\n",
                "print(\"  epoch_callback(policy, epoch, val_dataset, env) - Update check\")"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "warmup-baseline",
            "metadata": {},
            "source": [
                "### 5.4 Warmup Baseline\n",
                "\n",
                "The **WarmupBaseline** provides gradual transition from exponential to target baseline:\n",
                "\n",
                "$$b_{\\text{warmup}} = \\alpha \\cdot b_{\\text{target}} + (1 - \\alpha) \\cdot b_{\\text{exponential}}$$\n",
                "\n",
                "where $\\alpha$ increases from 0 to 1 over warmup epochs."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "warmup-example",
            "metadata": {},
            "outputs": [],
            "source": [
                "# WarmupBaseline Example\n",
                "from logic.src.envs import get_env\n",
                "from logic.src.models.policies import AttentionModelPolicy\n",
                "from logic.src.pipeline.rl.core.baselines import RolloutBaseline, WarmupBaseline\n",
                "\n",
                "# Create policy\n",
                "env = get_env(\"vrpp\", num_loc=20, device=\"cpu\")\n",
                "policy = AttentionModelPolicy(\n",
                "    env_name=\"vrpp\",\n",
                "    embed_dim=64,\n",
                "    hidden_dim=128,\n",
                "    n_encode_layers=2,\n",
                "    n_heads=4,\n",
                ")\n",
                "\n",
                "# Create warmup baseline wrapping rollout\n",
                "target_baseline = RolloutBaseline(policy=policy)\n",
                "warmup_bl = WarmupBaseline(\n",
                "    baseline=target_baseline,\n",
                "    warmup_epochs=5,\n",
                "    beta=0.8,  # Exponential baseline decay factor\n",
                ")\n",
                "\n",
                "print(\"WarmupBaseline Configuration:\")\n",
                "print(f\"  Target: {target_baseline.__class__.__name__}\")\n",
                "print(f\"  Warmup epochs: {warmup_bl.warmup_epochs}\")\n",
                "print(f\"  Current alpha: {warmup_bl.alpha}\")\n",
                "print(\"\\nBlending schedule:\")\n",
                "for epoch in range(6):\n",
                "    alpha = min(1.0, (epoch + 1) / warmup_bl.warmup_epochs)\n",
                "    print(f\"  Epoch {epoch}: α = {alpha:.2f} ({int(alpha * 100)}% target, {int((1 - alpha) * 100)}% exponential)\")"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "vo3uy3bqczf",
            "metadata": {},
            "source": [
                "---\n",
                "\n",
                "## 6. Complete Neural Models Reference\n",
                "\n",
                "### 6.1 Model Architecture Overview\n",
                "\n",
                "| Model | File | Architecture | Use Case |\n",
                "|-------|------|--------------|----------|\n",
                "| **AttentionModel (AM)** | `attention_model.py` | Transformer Encoder-Decoder | Standard routing, VRPP, WCVRP |\n",
                "| **DeepDecoderAM (DDAM)** | `deep_decoder_am.py` | Deep Transformer Decoder | Complex routing decisions |\n",
                "| **TemporalAM (TAM)** | `temporal_am.py` | Time-aware Transformer | Multi-day waste collection |\n",
                "| **GATLSTManager** | `gat_lstm_manager.py` | GAT + LSTM | HRL manager (dispatch decisions) |\n",
                "| **PointerNetwork** | `pointer_network.py` | RNN + Attention | Classic seq2seq routing |\n",
                "| **CriticNetwork** | `critic_network.py` | MLP Value Network | PPO baseline, actor-critic |\n",
                "| **Hypernetwork** | `hypernet.py` | Meta-learning | Weight generation |\n",
                "| **MetaRNN** | `meta_rnn.py` | LSTM | Adaptive weight adjustment |\n",
                "| **MOEModel** | `moe_model.py` | Mixture of Experts | Multi-task routing |\n",
                "\n",
                "### 6.2 Encoder Types\n",
                "\n",
                "All models support pluggable encoders via `encoder_type` config:\n",
                "\n",
                "| Encoder | Key | Architecture | Best For |\n",
                "|---------|-----|--------------|----------|\n",
                "| **GraphAttentionEncoder** | `gat` | Multi-head GAT | Default, general purpose |\n",
                "| **GraphAttConvEncoder** | `gac` | GAT + Edge features | When edge attributes matter |\n",
                "| **TransGraphConvEncoder** | `tgc` | Transformer-style GC | Large graphs |\n",
                "| **GatedGraphAttConvEncoder** | `ggac` | Gated GAT | Complex node-edge interactions |\n",
                "| **GCNEncoder** | `gcn` | Standard GCN | Simple baselines |\n",
                "| **MLPEncoder** | `mlp` | MLP only | No graph structure |\n",
                "| **PointerEncoder** | `ptr` | RNN-based | Pointer networks |\n",
                "| **MOEEncoder** | `moe` | Mixture of Experts | Multi-task learning |\n",
                "\n",
                "### 6.3 Module Building Blocks\n",
                "\n",
                "| Module | File | Description |\n",
                "|--------|------|-------------|\n",
                "| **MultiHeadAttention** | `multi_head_attention.py` | Standard scaled dot-product attention |\n",
                "| **GraphConvolution** | `graph_convolution.py` | Basic GCN message passing |\n",
                "| **DistanceGraphConvolution** | `distance_graph_convolution.py` | Distance-weighted convolution |\n",
                "| **GatedGraphConvolution** | `gated_graph_convolution.py` | GRU-style gating on graphs |\n",
                "| **EfficientGraphConvolution** | `efficient_graph_convolution.py` | Lightweight multi-head with aggregators |\n",
                "| **FeedForward** | `feed_forward.py` | 2-layer MLP block |\n",
                "| **Normalization** | `normalization.py` | Batch/Layer/Instance/Group norm |\n",
                "| **ActivationFunction** | `activation_function.py` | 21+ activations (ReLU, GELU, Mish, SwiGLU...) |\n",
                "| **SkipConnection** | `skip_connection.py` | Residual connections |\n",
                "| **HyperConnection** | `hyper_connection.py` | Dynamic depth mixing |\n",
                "| **MOE** | `moe.py` | Expert routing mechanism |\n",
                "\n",
                "### 6.4 Model Configuration Examples\n",
                "\n",
                "```python\n",
                "# Standard Attention Model\n",
                "ModelConfig(\n",
                "    name=\"am\",\n",
                "    embed_dim=128,\n",
                "    hidden_dim=512,\n",
                "    num_encoder_layers=3,\n",
                "    num_decoder_layers=1,\n",
                "    n_heads=8,\n",
                "    encoder_type=\"gat\",\n",
                "    normalization=\"instance\",\n",
                "    activation=\"gelu\",\n",
                "    dropout=0.1,\n",
                ")\n",
                "\n",
                "# Deep Decoder for complex problems\n",
                "ModelConfig(\n",
                "    name=\"deep_decoder\",\n",
                "    embed_dim=128,\n",
                "    hidden_dim=512,\n",
                "    num_encoder_layers=3,\n",
                "    num_decoder_layers=6,  # Deep decoder\n",
                "    n_heads=8,\n",
                ")\n",
                "\n",
                "# Temporal model for multi-day scenarios\n",
                "ModelConfig(\n",
                "    name=\"temporal\",\n",
                "    embed_dim=128,\n",
                "    temporal_horizon=7,  # 7-day lookahead\n",
                ")\n",
                "```\n",
                "\n",
                "### 6.5 CLI Model Selection\n",
                "\n",
                "```bash\n",
                "# Attention Model (default)\n",
                "python main.py train_lightning model=am model.embed_dim=128\n",
                "\n",
                "# Deep Decoder\n",
                "python main.py train_lightning model=deep_decoder model.num_decoder_layers=6\n",
                "\n",
                "# Temporal Model\n",
                "python main.py train_lightning model=temporal model.temporal_horizon=7\n",
                "\n",
                "# Change encoder type\n",
                "python main.py train_lightning model.encoder_type=gac  # Graph Attention Conv\n",
                "python main.py train_lightning model.encoder_type=tgc  # Transformer Graph Conv\n",
                "python main.py train_lightning model.encoder_type=gcn  # Standard GCN\n",
                "\n",
                "# Change normalization\n",
                "python main.py train_lightning model.normalization=layer  # or batch, instance, group\n",
                "\n",
                "# Change activation\n",
                "python main.py train_lightning model.activation=relu  # or gelu, mish, swish, swiglu\n",
                "```"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "section-6",
            "metadata": {},
            "source": [
                "---\n",
                "\n",
                "## 6. Environments & Data Generation\n",
                "\n",
                "The environment system provides:\n",
                "- Problem-specific state transitions\n",
                "- Reward computation\n",
                "- Action masking for valid moves\n",
                "- On-the-fly data generation\n",
                "\n",
                "### 6.1 Environment Registry"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "env-registry",
            "metadata": {},
            "outputs": [],
            "source": [
                "from logic.src.envs import ENV_REGISTRY, get_env\n",
                "\n",
                "print(\"Available Environments:\")\n",
                "print(\"=\" * 60)\n",
                "for name, cls in ENV_REGISTRY.items():\n",
                "    doc = cls.__doc__ or \"No description\"\n",
                "    first_line = doc.split(\"\\n\")[0].strip() if doc else \"No description\"\n",
                "    print(f\"  {name:10} - {cls.__name__}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "env-types",
            "metadata": {},
            "source": [
                "### 6.2 Environment Types\n",
                "\n",
                "| Environment | Description | Key Features |\n",
                "|-------------|-------------|-------------|\n",
                "| **vrpp** | Vehicle Routing with Profits | Prizes at nodes, maximize profit-cost |\n",
                "| **cvrpp** | Capacitated VRPP | + Vehicle capacity constraints |\n",
                "| **wcvrp** | Waste Collection VRP | Bin fill levels, accumulation |\n",
                "| **cwcvrp** | Capacitated WCVRP | + Vehicle capacity |\n",
                "| **sdwcvrp** | Stochastic Demand WCVRP | + Uncertain waste generation |\n",
                "\n",
                "### 6.3 TensorDict State Structure\n",
                "\n",
                "All environments use TensorDict for state management:"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "tensordict-example",
            "metadata": {},
            "outputs": [],
            "source": [
                "from logic.src.envs import get_env\n",
                "\n",
                "# Create VRPP environment\n",
                "env = get_env(\n",
                "    \"vrpp\",\n",
                "    num_loc=20,\n",
                "    min_loc=0.0,\n",
                "    max_loc=1.0,\n",
                "    device=\"cpu\",\n",
                ")\n",
                "\n",
                "# Generate a batch of instances\n",
                "batch_size = 4\n",
                "td = env.generator(batch_size)\n",
                "\n",
                "print(\"Generated TensorDict:\")\n",
                "print(f\"  Batch size: {batch_size}\")\n",
                "print(f\"  Keys: {list(td.keys())}\")\n",
                "print(\"\\nShapes:\")\n",
                "for key in td.keys():\n",
                "    print(f\"  {key}: {td[key].shape}\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "env-step-example",
            "metadata": {},
            "outputs": [],
            "source": [
                "# Demonstrate environment step\n",
                "td_reset = env.reset(td)\n",
                "\n",
                "print(\"After reset:\")\n",
                "print(f\"  Keys: {list(td_reset.keys())}\")\n",
                "print(f\"  Done: {td_reset['done'] if 'done' in td_reset.keys() else 'Not set'}\")\n",
                "\n",
                "# Show action mask\n",
                "if \"action_mask\" in td_reset.keys():\n",
                "    mask = td_reset[\"action_mask\"]\n",
                "    print(f\"\\nAction mask shape: {mask.shape}\")\n",
                "    print(f\"  Valid actions (first instance): {mask[0].sum().item()}/{mask[0].numel()}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "generator-section",
            "metadata": {},
            "source": [
                "### 6.4 Data Generators\n",
                "\n",
                "Generators create problem instances on-the-fly:"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "generator-example",
            "metadata": {},
            "outputs": [],
            "source": [
                "from logic.src.envs import GENERATOR_REGISTRY, get_generator\n",
                "\n",
                "print(\"Available Generators:\")\n",
                "for name in GENERATOR_REGISTRY.keys():\n",
                "    print(f\"  {name}\")\n",
                "\n",
                "# Create VRPP generator\n",
                "generator = get_generator(\n",
                "    \"vrpp\",\n",
                "    num_loc=50,\n",
                "    min_loc=0.0,\n",
                "    max_loc=1.0,\n",
                ")\n",
                "\n",
                "# Generate instances\n",
                "instances = generator(batch_size=8)\n",
                "print(f\"\\nGenerated {instances.batch_size[0]} instances with {instances['locs'].shape[-2]} locations\")"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "dataset-section",
            "metadata": {},
            "source": [
                "### 6.5 Dataset Classes\n",
                "\n",
                "The pipeline uses custom dataset classes:\n",
                "\n",
                "```python\n",
                "from logic.src.data.datasets import (\n",
                "    GeneratorDataset,      # On-the-fly generation\n",
                "    TensorDictDataset,     # Persistent storage\n",
                "    BaselineDataset,       # Wraps dataset with baseline values\n",
                "    tensordict_collate_fn, # Custom collation for TensorDict\n",
                ")\n",
                "```"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "dataset-example",
            "metadata": {},
            "outputs": [],
            "source": [
                "from logic.src.data.datasets import GeneratorDataset, tensordict_collate_fn\n",
                "from torch.utils.data import DataLoader\n",
                "\n",
                "# Create dataset\n",
                "dataset = GeneratorDataset(generator, size=1000)\n",
                "\n",
                "# Create dataloader\n",
                "dataloader = DataLoader(\n",
                "    dataset,\n",
                "    batch_size=64,\n",
                "    collate_fn=tensordict_collate_fn,\n",
                "    num_workers=0,\n",
                ")\n",
                "\n",
                "# Get a batch\n",
                "batch = next(iter(dataloader))\n",
                "print(f\"Batch keys: {list(batch.keys())}\")\n",
                "print(f\"Batch size: {batch.batch_size}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "section-7",
            "metadata": {},
            "source": [
                "---\n",
                "\n",
                "## 7. Meta-Learning\n",
                "\n",
                "Meta-learning enables automatic adaptation of training parameters (e.g., reward weights) through bi-level optimization.\n",
                "\n",
                "### 7.1 Meta-Learning Wrapper"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "meta-overview",
            "metadata": {},
            "outputs": [],
            "source": [
                "# Meta-Learning Example\n",
                "from logic.src.envs import get_env\n",
                "from logic.src.models.policies import AttentionModelPolicy\n",
                "from logic.src.pipeline.rl import REINFORCE, MetaRLModule\n",
                "\n",
                "# Create environment\n",
                "env = get_env(\"vrpp\", num_loc=20, device=\"cpu\")\n",
                "\n",
                "# Create policy\n",
                "policy = AttentionModelPolicy(\n",
                "    env_name=\"vrpp\",\n",
                "    embed_dim=64,\n",
                "    hidden_dim=128,\n",
                "    n_encode_layers=2,\n",
                "    n_heads=4,\n",
                ")\n",
                "\n",
                "# Create base RL module\n",
                "base_module = REINFORCE(\n",
                "    env=env,\n",
                "    policy=policy,\n",
                "    baseline=\"exponential\",\n",
                "    optimizer=\"adam\",\n",
                "    optimizer_kwargs={\"lr\": 1e-4},\n",
                "    train_data_size=1000,\n",
                "    val_data_size=100,\n",
                "    batch_size=64,\n",
                ")\n",
                "\n",
                "# Wrap with meta-learning\n",
                "meta_module = MetaRLModule(\n",
                "    agent=base_module,\n",
                "    meta_lr=1e-3,\n",
                "    history_length=10,\n",
                "    hidden_size=64,\n",
                ")\n",
                "\n",
                "print(\"MetaRLModule created:\")\n",
                "print(f\"  Inner agent: {base_module.__class__.__name__}\")\n",
                "print(f\"  Meta learning rate: {meta_module.meta_lr}\")\n",
                "print(f\"  History length: {meta_module.history_length}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "meta-strategies",
            "metadata": {},
            "source": [
                "### 7.2 Meta-Learning Strategies\n",
                "\n",
                "| Strategy | Description | Best For |\n",
                "|----------|-------------|----------|\n",
                "| **rnn** | Recurrent network processes reward history | General adaptation |\n",
                "| **bandit** | UCB/Thompson sampling for weight selection | Discrete weight choices |\n",
                "| **morl** | Multi-objective Pareto optimization | Multiple objectives |\n",
                "| **tdl** | Temporal difference learning | Online adaptation |\n",
                "| **hypernet** | Hypernetwork generates weights | Problem-conditioned |\n",
                "\n",
                "### 7.3 Bi-Level Optimization Flow\n",
                "\n",
                "```\n",
                "┌─────────────────────────────────────────────────────────────────┐\n",
                "│                    Meta-Learning Training Loop                   │\n",
                "└─────────────────────────────────────────────────────────────────┘\n",
                "\n",
                "for epoch in range(n_epochs):\n",
                "    \n",
                "    ┌─────────────────────────────────────────────────────────────┐\n",
                "    │ INNER LOOP: RL Training                                      │\n",
                "    │                                                              │\n",
                "    │   for batch in dataloader:                                   │\n",
                "    │       loss = agent.training_step(batch)                      │\n",
                "    │       optimizer.step()                                       │\n",
                "    │                                                              │\n",
                "    │   reward_signal = epoch_reward                               │\n",
                "    └─────────────────────────────────────────────────────────────┘\n",
                "                              │\n",
                "                              ▼\n",
                "    ┌─────────────────────────────────────────────────────────────┐\n",
                "    │ OUTER LOOP: Meta-Strategy Update                             │\n",
                "    │                                                              │\n",
                "    │   meta_strategy.update(reward_signal)                        │\n",
                "    │   new_weights = meta_strategy.propose_weights()              │\n",
                "    │   env.update_weights(new_weights)                            │\n",
                "    └─────────────────────────────────────────────────────────────┘\n",
                "```"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "section-8",
            "metadata": {},
            "source": [
                "---\n",
                "\n",
                "## 8. Hyperparameter Optimization\n",
                "\n",
                "The pipeline supports two HPO methods:\n",
                "\n",
                "### 8.1 Optuna-Based HPO\n",
                "\n",
                "Multiple sampling strategies:\n",
                "- **TPE** (Tree-structured Parzen Estimator) - Default\n",
                "- **Grid Search**\n",
                "- **Random Search**\n",
                "- **Hyperband** - Multi-fidelity"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "hpo-config",
            "metadata": {},
            "outputs": [],
            "source": [
                "from logic.src.configs import HPOConfig\n",
                "\n",
                "# HPO configuration\n",
                "hpo_cfg = HPOConfig(\n",
                "    method=\"tpe\",  # tpe, grid, random, hyperband\n",
                "    n_trials=50,  # Number of trials\n",
                "    n_epochs_per_trial=10,  # Epochs per trial\n",
                "    search_space={\n",
                "        \"optim.lr\": [1e-5, 1e-3],  # Log-uniform\n",
                "        \"train.batch_size\": [64, 512],  # Integer\n",
                "        \"model.embed_dim\": [64, 128, 256],  # Categorical\n",
                "        \"rl.entropy_weight\": [0.0, 0.1],  # Uniform\n",
                "    },\n",
                ")\n",
                "\n",
                "print(\"HPO Configuration:\")\n",
                "print(f\"  Method: {hpo_cfg.method}\")\n",
                "print(f\"  Trials: {hpo_cfg.n_trials}\")\n",
                "print(f\"  Epochs per trial: {hpo_cfg.n_epochs_per_trial}\")\n",
                "print(\"\\nSearch Space:\")\n",
                "for param, range_val in hpo_cfg.search_space.items():\n",
                "    print(f\"  {param}: {range_val}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "hpo-cli",
            "metadata": {},
            "source": [
                "### 8.2 HPO via CLI\n",
                "\n",
                "```bash\n",
                "# TPE optimization with 50 trials\n",
                "python main.py train_lightning \\\n",
                "    hpo.n_trials=50 \\\n",
                "    hpo.method=tpe \\\n",
                "    hpo.n_epochs_per_trial=10 \\\n",
                "    'hpo.search_space={\"optim.lr\": [1e-5, 1e-3], \"train.batch_size\": [64, 512]}'\n",
                "\n",
                "# Grid search\n",
                "python main.py train_lightning \\\n",
                "    hpo.n_trials=100 \\\n",
                "    hpo.method=grid \\\n",
                "    'hpo.search_space={\"model.embed_dim\": [64, 128, 256], \"model.n_heads\": [4, 8]}'\n",
                "\n",
                "# DEHB (multi-fidelity)\n",
                "python main.py train_lightning \\\n",
                "    hpo.method=dehb \\\n",
                "    hpo.min_fidelity=1 \\\n",
                "    hpo.max_fidelity=50 \\\n",
                "    hpo.fevals=100\n",
                "```\n",
                "\n",
                "### 8.3 DEHB (Differential Evolution Hyperband)\n",
                "\n",
                "DEHB combines:\n",
                "- **Differential Evolution** for global search\n",
                "- **Hyperband** for multi-fidelity scheduling\n",
                "\n",
                "Benefits:\n",
                "- Early stopping of poor configurations\n",
                "- Efficient use of compute budget"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "section-9",
            "metadata": {},
            "source": [
                "---\n",
                "\n",
                "## 9. Practical Examples\n",
                "\n",
                "### 9.1 Complete Training Script"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "complete-example",
            "metadata": {},
            "outputs": [],
            "source": [
                "import pytorch_lightning as pl\n",
                "from logic.src.callbacks import SpeedMonitor\n",
                "from logic.src.envs import get_env\n",
                "from logic.src.models.policies import AttentionModelPolicy\n",
                "from logic.src.pipeline.rl import REINFORCE\n",
                "from logic.src.pipeline.trainer import WSTrainer\n",
                "from pytorch_lightning import seed_everything\n",
                "from pytorch_lightning.loggers import CSVLogger\n",
                "\n",
                "# Set seed for reproducibility\n",
                "seed_everything(42)\n",
                "\n",
                "# 1. Create Environment\n",
                "env = get_env(\n",
                "    \"vrpp\",\n",
                "    num_loc=20,\n",
                "    device=\"cpu\",  # Use \"cuda\" if available\n",
                ")\n",
                "\n",
                "# 2. Create Policy\n",
                "policy = AttentionModelPolicy(\n",
                "    env_name=\"vrpp\",\n",
                "    embed_dim=64,\n",
                "    hidden_dim=128,\n",
                "    n_encode_layers=2,\n",
                "    n_heads=4,\n",
                ")\n",
                "\n",
                "# 3. Create RL Module\n",
                "model = REINFORCE(\n",
                "    env=env,\n",
                "    policy=policy,\n",
                "    baseline=\"exponential\",\n",
                "    optimizer=\"adam\",\n",
                "    optimizer_kwargs={\"lr\": 1e-4},\n",
                "    train_data_size=5000,\n",
                "    val_data_size=500,\n",
                "    batch_size=64,\n",
                "    num_workers=0,  # Use 0 for notebooks\n",
                ")\n",
                "\n",
                "# 4. Create Trainer\n",
                "trainer = WSTrainer(\n",
                "    max_epochs=3,  # Short training for demo\n",
                "    accelerator=\"auto\",\n",
                "    devices=1,\n",
                "    logger=CSVLogger(\"logs\", name=\"tutorial_demo\"),\n",
                "    callbacks=[SpeedMonitor(epoch_time=True)],\n",
                "    enable_progress_bar=True,\n",
                ")\n",
                "\n",
                "print(\"Training configuration ready!\")\n",
                "print(f\"  Environment: {env.__class__.__name__}\")\n",
                "print(f\"  Policy: {policy.__class__.__name__}\")\n",
                "print(f\"  Algorithm: REINFORCE with {model.baseline_type} baseline\")\n",
                "print(f\"  Epochs: {trainer.max_epochs}\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "run-training",
            "metadata": {},
            "outputs": [],
            "source": [
                "# Run training (uncomment to execute)\n",
                "# trainer.fit(model)\n",
                "\n",
                "# Get final metrics\n",
                "# print(f\"\\nFinal validation reward: {trainer.callback_metrics.get('val/reward', 'N/A')}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "cli-examples",
            "metadata": {},
            "source": [
                "### 9.2 CLI Examples Reference\n",
                "\n",
                "#### Basic Training\n",
                "```bash\n",
                "# VRPP with Attention Model\n",
                "python main.py train_lightning \\\n",
                "    model=am \\\n",
                "    env.name=vrpp \\\n",
                "    env.num_loc=50 \\\n",
                "    train.n_epochs=100\n",
                "\n",
                "# WCVRP (Waste Collection)\n",
                "python main.py train_lightning \\\n",
                "    model=am \\\n",
                "    env.name=wcvrp \\\n",
                "    env.num_loc=50 \\\n",
                "    env.capacity=100\n",
                "```\n",
                "\n",
                "#### Algorithm Variants\n",
                "```bash\n",
                "# PPO with custom parameters\n",
                "python main.py train_lightning \\\n",
                "    rl.algorithm=ppo \\\n",
                "    rl.ppo_epochs=10 \\\n",
                "    rl.eps_clip=0.2 \\\n",
                "    rl.value_loss_weight=0.5\n",
                "\n",
                "# POMO with augmentation\n",
                "python main.py train_lightning \\\n",
                "    rl.algorithm=pomo \\\n",
                "    rl.num_augment=8 \\\n",
                "    rl.num_starts=50\n",
                "\n",
                "# Imitation learning from HGS\n",
                "python main.py train_lightning \\\n",
                "    rl.algorithm=imitation \\\n",
                "    rl.expert=hgs\n",
                "```\n",
                "\n",
                "#### Advanced Features\n",
                "```bash\n",
                "# Meta-learning with RNN strategy\n",
                "python main.py train_lightning \\\n",
                "    rl.use_meta=true \\\n",
                "    rl.meta_strategy=rnn \\\n",
                "    rl.meta_lr=1e-3 \\\n",
                "    rl.meta_history_length=10\n",
                "\n",
                "# Learning rate scheduling\n",
                "python main.py train_lightning \\\n",
                "    optim.lr_scheduler=cosine \\\n",
                "    'optim.lr_scheduler_kwargs={\"T_max\": 100}'\n",
                "\n",
                "# Mixed precision training\n",
                "python main.py train_lightning \\\n",
                "    train.precision=16-mixed\n",
                "```"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "section-10",
            "metadata": {},
            "source": [
                "---\n",
                "\n",
                "## 10. Advanced Topics\n",
                "\n",
                "### 10.1 Custom RL Algorithm\n",
                "\n",
                "To implement a custom RL algorithm, inherit from `RL4COLitModule`:"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "custom-algorithm",
            "metadata": {},
            "outputs": [],
            "source": [
                "from typing import Optional\n",
                "\n",
                "import torch\n",
                "from logic.src.pipeline.rl.core.base import RL4COLitModule\n",
                "from tensordict import TensorDict\n",
                "\n",
                "\n",
                "class CustomRL(RL4COLitModule):\n",
                "    \"\"\"\n",
                "    Example custom RL algorithm.\n",
                "\n",
                "    Implements a simple variant with custom advantage normalization.\n",
                "    \"\"\"\n",
                "\n",
                "    def __init__(\n",
                "        self,\n",
                "        temperature: float = 1.0,\n",
                "        **kwargs,\n",
                "    ):\n",
                "        super().__init__(**kwargs)\n",
                "        self.temperature = temperature\n",
                "\n",
                "    def calculate_loss(\n",
                "        self,\n",
                "        td: TensorDict,\n",
                "        out: dict,\n",
                "        batch_idx: int,\n",
                "        env: Optional[\"RL4COEnvBase\"] = None,\n",
                "    ) -> torch.Tensor:\n",
                "        \"\"\"\n",
                "        Custom loss computation.\n",
                "\n",
                "        Uses temperature-scaled advantage.\n",
                "        \"\"\"\n",
                "        reward = out[\"reward\"]\n",
                "        log_likelihood = out[\"log_likelihood\"]\n",
                "\n",
                "        # Get baseline\n",
                "        if hasattr(self, \"_current_baseline_val\") and self._current_baseline_val is not None:\n",
                "            baseline_val = self._current_baseline_val\n",
                "        else:\n",
                "            baseline_val = self.baseline.eval(td, reward, env=env)\n",
                "\n",
                "        # Temperature-scaled advantage\n",
                "        advantage = (reward - baseline_val) / self.temperature\n",
                "        advantage = (advantage - advantage.mean()) / (advantage.std() + 1e-8)\n",
                "\n",
                "        # Policy gradient loss\n",
                "        loss = -(advantage.detach() * log_likelihood).mean()\n",
                "\n",
                "        return loss\n",
                "\n",
                "\n",
                "print(\"CustomRL algorithm defined successfully!\")\n",
                "print(f\"  Key parameter: temperature={1.0}\")\n",
                "print(\"  Inherits from: RL4COLitModule\")"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "custom-baseline",
            "metadata": {},
            "source": [
                "### 10.2 Custom Baseline\n",
                "\n",
                "To implement a custom baseline:"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "custom-baseline-code",
            "metadata": {},
            "outputs": [],
            "source": [
                "import torch\n",
                "import torch.nn as nn\n",
                "from logic.src.pipeline.rl.core.baselines import Baseline\n",
                "from tensordict import TensorDict\n",
                "\n",
                "\n",
                "class PercentileBaseline(Baseline):\n",
                "    \"\"\"\n",
                "    Baseline using batch percentile.\n",
                "\n",
                "    Returns the q-th percentile of rewards as baseline.\n",
                "    \"\"\"\n",
                "\n",
                "    def __init__(self, percentile: float = 50.0):\n",
                "        super().__init__()\n",
                "        self.percentile = percentile\n",
                "\n",
                "    def eval(\n",
                "        self,\n",
                "        td: TensorDict,\n",
                "        reward: torch.Tensor,\n",
                "        env=None,\n",
                "    ) -> torch.Tensor:\n",
                "        \"\"\"\n",
                "        Compute percentile baseline.\n",
                "\n",
                "        Args:\n",
                "            td: TensorDict (unused)\n",
                "            reward: Batch rewards\n",
                "            env: Environment (unused)\n",
                "\n",
                "        Returns:\n",
                "            Percentile value expanded to reward shape\n",
                "        \"\"\"\n",
                "        # Compute percentile\n",
                "        baseline_val = torch.quantile(reward.float(), self.percentile / 100.0)\n",
                "        return baseline_val.expand_as(reward)\n",
                "\n",
                "\n",
                "# Test the custom baseline\n",
                "baseline = PercentileBaseline(percentile=75.0)\n",
                "test_rewards = torch.randn(64)\n",
                "baseline_val = baseline.eval(None, test_rewards)\n",
                "\n",
                "print(\"PercentileBaseline (75th):\")\n",
                "print(f\"  Input rewards shape: {test_rewards.shape}\")\n",
                "print(f\"  Baseline value: {baseline_val[0].item():.4f}\")\n",
                "print(f\"  Actual 75th percentile: {torch.quantile(test_rewards, 0.75).item():.4f}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "multi-gpu",
            "metadata": {},
            "source": [
                "### 10.3 Multi-GPU Training\n",
                "\n",
                "PyTorch Lightning automatically handles distributed training:\n",
                "\n",
                "```bash\n",
                "# Single GPU\n",
                "python main.py train_lightning device=cuda\n",
                "\n",
                "# Multiple GPUs (DDP)\n",
                "python main.py train_lightning \\\n",
                "    device=cuda \\\n",
                "    --trainer.devices=4 \\\n",
                "    --trainer.strategy=ddp\n",
                "\n",
                "# DeepSpeed (for large models)\n",
                "python main.py train_lightning \\\n",
                "    --trainer.strategy=deepspeed_stage_2\n",
                "```\n",
                "\n",
                "### 10.4 Checkpoint Management\n",
                "\n",
                "```bash\n",
                "# Resume from checkpoint\n",
                "python main.py train_lightning \\\n",
                "    --ckpt_path=/path/to/checkpoint.ckpt\n",
                "\n",
                "# Custom checkpoint directory\n",
                "python main.py train_lightning \\\n",
                "    output_dir=assets/model_weights/experiment1\n",
                "```\n",
                "\n",
                "### 10.5 Debugging Tips\n",
                "\n",
                "```bash\n",
                "# Fast dev run (1 batch per epoch)\n",
                "python main.py train_lightning \\\n",
                "    --trainer.fast_dev_run=true\n",
                "\n",
                "# Limit batches for debugging\n",
                "python main.py train_lightning \\\n",
                "    --trainer.limit_train_batches=10 \\\n",
                "    --trainer.limit_val_batches=5\n",
                "\n",
                "# Profiling\n",
                "python main.py train_lightning \\\n",
                "    --trainer.profiler=simple\n",
                "```"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "summary",
            "metadata": {},
            "source": [
                "---\n",
                "\n",
                "## Summary\n",
                "\n",
                "This comprehensive tutorial covered the Lightning-based RL training pipeline for WSmart+ Route.\n",
                "\n",
                "### Key Components Covered\n",
                "\n",
                "| Category | Count | Description |\n",
                "|----------|-------|-------------|\n",
                "| **RL Algorithms** | 11 | REINFORCE, PPO, SAPO, GSPO, GDPO, DRGRPO, POMO, SymNCO, Imitation, AdaptiveImitation, HRL |\n",
                "| **Baselines** | 7 | None, Exponential, Rollout, Critic, Warmup, POMO |\n",
                "| **Environments** | 6 | VRPP, CVRPP, WCVRP, CWCVRP, SDWCVRP, SCWCVRP |\n",
                "| **Models** | 9 | AM, DDAM, TAM, Pointer, Critic, HyperNet, MetaRNN, MOE, GATLSTManager |\n",
                "| **Encoders** | 8 | GAT, GAC, TGC, GGAC, GCN, MLP, Pointer, MOE |\n",
                "| **Modules** | 11 | Attention, GraphConv, FeedForward, Normalization, Activation, Skip, Hyper, MOE |\n",
                "\n",
                "### Quick Reference Commands\n",
                "\n",
                "```bash\n",
                "# Basic training\n",
                "python main.py train_lightning model=am env.name=vrpp\n",
                "\n",
                "# PPO with rollout baseline\n",
                "python main.py train_lightning rl.algorithm=ppo rl.baseline=rollout\n",
                "\n",
                "# POMO with augmentation\n",
                "python main.py train_lightning rl.algorithm=pomo rl.num_augment=8\n",
                "\n",
                "# Imitation learning from HGS expert\n",
                "python main.py train_lightning rl.algorithm=imitation rl.imitation_mode=hgs\n",
                "\n",
                "# Adaptive IL → RL transition\n",
                "python main.py train_lightning rl.algorithm=adaptive_imitation rl.imitation_decay=0.95\n",
                "\n",
                "# Meta-learning\n",
                "python main.py train_lightning meta_rl.enabled=true meta_rl.strategy=rnn\n",
                "\n",
                "# Hyperparameter optimization\n",
                "python main.py train_lightning hpo.n_trials=50 hpo.method=tpe\n",
                "\n",
                "# Multi-GPU training\n",
                "python main.py train_lightning --trainer.devices=4 --trainer.strategy=ddp\n",
                "```\n",
                "\n",
                "### Extension Points\n",
                "\n",
                "This tutorial covered how to add:\n",
                "1. **New Environment/Task** - Inherit from `RL4COEnvBase`\n",
                "2. **New Encoder** - Create module in `subnets/`, register in factory\n",
                "3. **New RL Algorithm** - Inherit from `RL4COLitModule`, implement `calculate_loss()`\n",
                "4. **New Baseline** - Inherit from `Baseline`, implement `eval()`\n",
                "5. **New Configuration** - Create dataclass, add to root `Config`\n",
                "\n",
                "### Further Reading\n",
                "\n",
                "| Resource | Location |\n",
                "|----------|----------|\n",
                "| Project Overview | `CLAUDE.md` |\n",
                "| Architecture | `ARCHITECTURE.md` |\n",
                "| CLI Reference | `python main.py --help` |\n",
                "| RL Pipeline Source | `logic/src/pipeline/rl/` |\n",
                "| Configuration | `logic/src/configs/` |\n",
                "| Environments | `logic/src/envs/` |\n",
                "| Models | `logic/src/models/` |\n",
                "\n",
                "### Version History\n",
                "\n",
                "| Version | Date | Changes |\n",
                "|---------|------|---------|\n",
                "| 2.0 | Jan 2026 | Added all algorithms, models, extension guides, best practices |\n",
                "| 1.0 | Jan 2026 | Initial tutorial |"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "0ye9uwzjbq2k",
            "metadata": {},
            "source": [
                "---\n",
                "\n",
                "## 11. Adding New Components Guide\n",
                "\n",
                "This section provides step-by-step guides for extending the framework with new components.\n",
                "\n",
                "### 11.1 Adding a New Environment/Task\n",
                "\n",
                "**Location:** `logic/src/envs/`\n",
                "\n",
                "**Step 1: Create the environment file**\n",
                "\n",
                "```python\n",
                "# logic/src/envs/my_problem.py\n",
                "\n",
                "import torch\n",
                "from tensordict import TensorDict\n",
                "from logic.src.envs.base import RL4COEnvBase\n",
                "from logic.src.envs.generators import Generator\n",
                "\n",
                "\n",
                "class MyProblemGenerator(Generator):\n",
                "    \"\"\"Generator for MyProblem instances.\"\"\"\n",
                "    \n",
                "    def __init__(\n",
                "        self,\n",
                "        num_loc: int = 50,\n",
                "        min_loc: float = 0.0,\n",
                "        max_loc: float = 1.0,\n",
                "        **kwargs\n",
                "    ):\n",
                "        super().__init__(**kwargs)\n",
                "        self.num_loc = num_loc\n",
                "        self.min_loc = min_loc\n",
                "        self.max_loc = max_loc\n",
                "    \n",
                "    def _generate(self, batch_size: int) -> TensorDict:\n",
                "        \"\"\"Generate random problem instances.\"\"\"\n",
                "        # Locations (batch, num_loc, 2)\n",
                "        locs = torch.rand(batch_size, self.num_loc, 2)\n",
                "        locs = locs * (self.max_loc - self.min_loc) + self.min_loc\n",
                "        \n",
                "        # Depot (batch, 2)\n",
                "        depot = torch.rand(batch_size, 2)\n",
                "        \n",
                "        # Problem-specific features\n",
                "        demands = torch.rand(batch_size, self.num_loc)\n",
                "        \n",
                "        return TensorDict({\n",
                "            \"locs\": locs,\n",
                "            \"depot\": depot,\n",
                "            \"demand\": demands,\n",
                "        }, batch_size=[batch_size])\n",
                "\n",
                "\n",
                "class MyProblemEnv(RL4COEnvBase):\n",
                "    \"\"\"Environment for MyProblem.\"\"\"\n",
                "    \n",
                "    name = \"myproblem\"\n",
                "    \n",
                "    def __init__(\n",
                "        self,\n",
                "        num_loc: int = 50,\n",
                "        capacity: float = 1.0,\n",
                "        **kwargs\n",
                "    ):\n",
                "        super().__init__(**kwargs)\n",
                "        self.num_loc = num_loc\n",
                "        self.capacity = capacity\n",
                "        self.generator = MyProblemGenerator(num_loc=num_loc, **kwargs)\n",
                "    \n",
                "    def _reset(self, td: TensorDict) -> TensorDict:\n",
                "        \"\"\"Reset environment to initial state.\"\"\"\n",
                "        batch_size = td.batch_size[0]\n",
                "        device = td.device\n",
                "        \n",
                "        # Initialize state\n",
                "        td[\"current_node\"] = torch.zeros(batch_size, dtype=torch.long, device=device)\n",
                "        td[\"visited\"] = torch.zeros(batch_size, self.num_loc, dtype=torch.bool, device=device)\n",
                "        td[\"current_load\"] = torch.zeros(batch_size, device=device)\n",
                "        td[\"done\"] = torch.zeros(batch_size, dtype=torch.bool, device=device)\n",
                "        \n",
                "        # Compute action mask\n",
                "        td[\"action_mask\"] = self._get_action_mask(td)\n",
                "        \n",
                "        return td\n",
                "    \n",
                "    def _step(self, td: TensorDict) -> TensorDict:\n",
                "        \"\"\"Execute one step in the environment.\"\"\"\n",
                "        action = td[\"action\"]\n",
                "        batch_size = td.batch_size[0]\n",
                "        \n",
                "        # Update visited\n",
                "        td[\"visited\"].scatter_(1, action.unsqueeze(-1), True)\n",
                "        \n",
                "        # Update current node\n",
                "        td[\"current_node\"] = action\n",
                "        \n",
                "        # Update load\n",
                "        demand = td[\"demand\"].gather(1, action.unsqueeze(-1)).squeeze(-1)\n",
                "        td[\"current_load\"] = td[\"current_load\"] + demand\n",
                "        \n",
                "        # Check if done\n",
                "        td[\"done\"] = td[\"visited\"].all(dim=-1)\n",
                "        \n",
                "        # Update action mask\n",
                "        td[\"action_mask\"] = self._get_action_mask(td)\n",
                "        \n",
                "        return td\n",
                "    \n",
                "    def _get_action_mask(self, td: TensorDict) -> torch.Tensor:\n",
                "        \"\"\"Compute valid actions mask.\"\"\"\n",
                "        mask = ~td[\"visited\"]\n",
                "        # Add capacity constraints, etc.\n",
                "        return mask\n",
                "    \n",
                "    def _get_reward(self, td: TensorDict, actions: torch.Tensor) -> torch.Tensor:\n",
                "        \"\"\"Compute reward (negative cost).\"\"\"\n",
                "        # Compute tour length\n",
                "        locs = td[\"locs\"]\n",
                "        depot = td[\"depot\"]\n",
                "        \n",
                "        # Simplified: compute total distance\n",
                "        # ... actual implementation\n",
                "        \n",
                "        return -cost  # Negative because we minimize\n",
                "```\n",
                "\n",
                "**Step 2: Register in `__init__.py`**\n",
                "\n",
                "```python\n",
                "# logic/src/envs/__init__.py\n",
                "\n",
                "from logic.src.envs.my_problem import MyProblemEnv, MyProblemGenerator\n",
                "\n",
                "# Add to registry\n",
                "ENV_REGISTRY = {\n",
                "    # ... existing envs\n",
                "    \"myproblem\": MyProblemEnv,\n",
                "}\n",
                "\n",
                "GENERATOR_REGISTRY = {\n",
                "    # ... existing generators\n",
                "    \"myproblem\": MyProblemGenerator,\n",
                "}\n",
                "```\n",
                "\n",
                "**Step 3: Add configuration**\n",
                "\n",
                "```python\n",
                "# logic/src/configs/env.py\n",
                "\n",
                "@dataclass\n",
                "class EnvConfig:\n",
                "    name: str = \"vrpp\"  # Add \"myproblem\" as valid option\n",
                "    # ... add any new config fields\n",
                "```"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "3nfbgvaaoo3",
            "metadata": {},
            "source": [
                "### 11.2 Adding a New Encoder\n",
                "\n",
                "**Location:** `logic/src/models/subnets/`\n",
                "\n",
                "**Step 1: Create the encoder file**\n",
                "\n",
                "```python\n",
                "# logic/src/models/subnets/my_encoder.py\n",
                "\n",
                "import torch\n",
                "import torch.nn as nn\n",
                "from logic.src.models.modules import Normalization, ActivationFunction\n",
                "\n",
                "\n",
                "class MyCustomEncoder(nn.Module):\n",
                "    \"\"\"\n",
                "    Custom encoder for node embedding.\n",
                "    \n",
                "    Args:\n",
                "        embed_dim: Embedding dimension\n",
                "        hidden_dim: Hidden layer dimension\n",
                "        n_layers: Number of encoder layers\n",
                "        n_heads: Number of attention heads\n",
                "        normalization: Type of normalization\n",
                "        activation: Activation function\n",
                "    \"\"\"\n",
                "    \n",
                "    def __init__(\n",
                "        self,\n",
                "        embed_dim: int = 128,\n",
                "        hidden_dim: int = 512,\n",
                "        n_layers: int = 3,\n",
                "        n_heads: int = 8,\n",
                "        normalization: str = \"instance\",\n",
                "        activation: str = \"gelu\",\n",
                "        dropout: float = 0.1,\n",
                "    ):\n",
                "        super().__init__()\n",
                "        \n",
                "        self.embed_dim = embed_dim\n",
                "        self.n_layers = n_layers\n",
                "        \n",
                "        # Initial embedding\n",
                "        self.init_embed = nn.Linear(2, embed_dim)  # 2D coordinates\n",
                "        \n",
                "        # Encoder layers\n",
                "        self.layers = nn.ModuleList([\n",
                "            self._make_layer(embed_dim, hidden_dim, n_heads, normalization, activation, dropout)\n",
                "            for _ in range(n_layers)\n",
                "        ])\n",
                "        \n",
                "    def _make_layer(self, embed_dim, hidden_dim, n_heads, normalization, activation, dropout):\n",
                "        \"\"\"Create a single encoder layer.\"\"\"\n",
                "        return nn.ModuleDict({\n",
                "            \"attention\": nn.MultiheadAttention(embed_dim, n_heads, dropout=dropout, batch_first=True),\n",
                "            \"norm1\": Normalization(embed_dim, normalization),\n",
                "            \"ff\": nn.Sequential(\n",
                "                nn.Linear(embed_dim, hidden_dim),\n",
                "                ActivationFunction(activation),\n",
                "                nn.Dropout(dropout),\n",
                "                nn.Linear(hidden_dim, embed_dim),\n",
                "            ),\n",
                "            \"norm2\": Normalization(embed_dim, normalization),\n",
                "        })\n",
                "    \n",
                "    def forward(self, x: torch.Tensor, mask: torch.Tensor = None) -> torch.Tensor:\n",
                "        \"\"\"\n",
                "        Forward pass.\n",
                "        \n",
                "        Args:\n",
                "            x: Node features (batch, n_nodes, input_dim)\n",
                "            mask: Attention mask (batch, n_nodes)\n",
                "            \n",
                "        Returns:\n",
                "            Node embeddings (batch, n_nodes, embed_dim)\n",
                "        \"\"\"\n",
                "        # Initial embedding\n",
                "        h = self.init_embed(x)\n",
                "        \n",
                "        # Encoder layers\n",
                "        for layer in self.layers:\n",
                "            # Self-attention\n",
                "            attn_out, _ = layer[\"attention\"](h, h, h, key_padding_mask=mask)\n",
                "            h = layer[\"norm1\"](h + attn_out)\n",
                "            \n",
                "            # Feed-forward\n",
                "            ff_out = layer[\"ff\"](h)\n",
                "            h = layer[\"norm2\"](h + ff_out)\n",
                "        \n",
                "        return h\n",
                "```\n",
                "\n",
                "**Step 2: Register in `__init__.py`**\n",
                "\n",
                "```python\n",
                "# logic/src/models/subnets/__init__.py\n",
                "\n",
                "from .my_encoder import MyCustomEncoder as MyCustomEncoder\n",
                "```\n",
                "\n",
                "**Step 3: Add to model factory**\n",
                "\n",
                "```python\n",
                "# logic/src/models/model_factory.py\n",
                "\n",
                "ENCODER_REGISTRY = {\n",
                "    \"gat\": GraphAttentionEncoder,\n",
                "    \"gcn\": GraphConvolutionEncoder,\n",
                "    \"my_encoder\": MyCustomEncoder,  # Add here\n",
                "}\n",
                "```\n",
                "\n",
                "### 11.3 Adding a New RL Algorithm\n",
                "\n",
                "**Location:** `logic/src/pipeline/rl/core/`\n",
                "\n",
                "**Step 1: Create the algorithm file**\n",
                "\n",
                "```python\n",
                "# logic/src/pipeline/rl/core/my_algorithm.py\n",
                "\n",
                "from typing import Optional\n",
                "import torch\n",
                "from tensordict import TensorDict\n",
                "from logic.src.pipeline.rl.core.base import RL4COLitModule\n",
                "\n",
                "\n",
                "class MyAlgorithm(RL4COLitModule):\n",
                "    \"\"\"\n",
                "    Custom RL algorithm.\n",
                "    \n",
                "    Implements a novel policy gradient variant with custom loss.\n",
                "    \"\"\"\n",
                "    \n",
                "    def __init__(\n",
                "        self,\n",
                "        # Algorithm-specific parameters\n",
                "        my_param: float = 1.0,\n",
                "        temperature: float = 1.0,\n",
                "        **kwargs  # Pass to parent\n",
                "    ):\n",
                "        super().__init__(**kwargs)\n",
                "        self.my_param = my_param\n",
                "        self.temperature = temperature\n",
                "        \n",
                "        # Save hyperparameters for checkpointing\n",
                "        self.save_hyperparameters(ignore=[\"env\", \"policy\"])\n",
                "    \n",
                "    def calculate_loss(\n",
                "        self,\n",
                "        td: TensorDict,\n",
                "        out: dict,\n",
                "        batch_idx: int,\n",
                "        env: Optional[\"RL4COEnvBase\"] = None,\n",
                "    ) -> torch.Tensor:\n",
                "        \"\"\"\n",
                "        Compute the custom loss.\n",
                "        \n",
                "        Args:\n",
                "            td: TensorDict with environment state\n",
                "            out: Dict with policy outputs (reward, log_likelihood, etc.)\n",
                "            batch_idx: Current batch index\n",
                "            env: Environment instance\n",
                "            \n",
                "        Returns:\n",
                "            Scalar loss tensor\n",
                "        \"\"\"\n",
                "        reward = out[\"reward\"]\n",
                "        log_likelihood = out[\"log_likelihood\"]\n",
                "        \n",
                "        # Get baseline value\n",
                "        if hasattr(self, \"_current_baseline_val\") and self._current_baseline_val is not None:\n",
                "            baseline_val = self._current_baseline_val\n",
                "        else:\n",
                "            baseline_val = self.baseline.eval(td, reward, env=env)\n",
                "        \n",
                "        # Compute advantage\n",
                "        advantage = reward - baseline_val\n",
                "        \n",
                "        # Custom loss computation\n",
                "        # Example: temperature-scaled policy gradient\n",
                "        scaled_advantage = advantage / self.temperature\n",
                "        scaled_advantage = (scaled_advantage - scaled_advantage.mean()) / (scaled_advantage.std() + 1e-8)\n",
                "        \n",
                "        # Policy gradient loss\n",
                "        loss = -(scaled_advantage.detach() * log_likelihood).mean()\n",
                "        \n",
                "        # Optional: add regularization\n",
                "        loss = loss * self.my_param\n",
                "        \n",
                "        # Log metrics\n",
                "        self.log(\"train/advantage\", advantage.mean(), prog_bar=False)\n",
                "        self.log(\"train/baseline\", baseline_val.mean(), prog_bar=False)\n",
                "        \n",
                "        return loss\n",
                "```\n",
                "\n",
                "**Step 2: Register in `__init__.py`**\n",
                "\n",
                "```python\n",
                "# logic/src/pipeline/rl/core/__init__.py\n",
                "\n",
                "from logic.src.pipeline.rl.core.my_algorithm import MyAlgorithm\n",
                "\n",
                "__all__ = [\n",
                "    # ... existing\n",
                "    \"MyAlgorithm\",\n",
                "]\n",
                "```\n",
                "\n",
                "**Step 3: Add to config**\n",
                "\n",
                "```python\n",
                "# logic/src/configs/rl.py\n",
                "\n",
                "@dataclass\n",
                "class RLConfig:\n",
                "    algorithm: str = \"reinforce\"  # Add \"my_algorithm\" as valid option\n",
                "    # Add algorithm-specific parameters\n",
                "    my_param: float = 1.0\n",
                "    temperature: float = 1.0\n",
                "```\n",
                "\n",
                "### 11.4 Adding a New Baseline\n",
                "\n",
                "**Location:** `logic/src/pipeline/rl/core/baselines.py`\n",
                "\n",
                "```python\n",
                "# Add to logic/src/pipeline/rl/core/baselines.py\n",
                "\n",
                "class MyCustomBaseline(Baseline):\n",
                "    \"\"\"\n",
                "    Custom baseline implementation.\n",
                "    \n",
                "    Example: Percentile-based baseline.\n",
                "    \"\"\"\n",
                "    \n",
                "    def __init__(self, percentile: float = 50.0):\n",
                "        super().__init__()\n",
                "        self.percentile = percentile\n",
                "    \n",
                "    def setup(\n",
                "        self,\n",
                "        policy: nn.Module,\n",
                "        env: \"RL4COEnvBase\",\n",
                "        **kwargs\n",
                "    ) -> \"Baseline\":\n",
                "        \"\"\"Initialize baseline (called once at start).\"\"\"\n",
                "        self.policy = policy\n",
                "        self.env = env\n",
                "        return self\n",
                "    \n",
                "    def eval(\n",
                "        self,\n",
                "        td: TensorDict,\n",
                "        reward: torch.Tensor,\n",
                "        env: Optional[\"RL4COEnvBase\"] = None,\n",
                "    ) -> torch.Tensor:\n",
                "        \"\"\"\n",
                "        Compute baseline value.\n",
                "        \n",
                "        Args:\n",
                "            td: TensorDict with state\n",
                "            reward: Batch rewards\n",
                "            env: Environment (optional)\n",
                "            \n",
                "        Returns:\n",
                "            Baseline values (same shape as reward)\n",
                "        \"\"\"\n",
                "        # Compute percentile\n",
                "        baseline_val = torch.quantile(reward.float(), self.percentile / 100.0)\n",
                "        return baseline_val.expand_as(reward)\n",
                "    \n",
                "    def epoch_callback(\n",
                "        self,\n",
                "        policy: nn.Module,\n",
                "        epoch: int,\n",
                "        val_dataset: \"TensorDictDataset\",\n",
                "        env: \"RL4COEnvBase\",\n",
                "        **kwargs\n",
                "    ) -> dict:\n",
                "        \"\"\"Called at end of each epoch (optional).\"\"\"\n",
                "        return {}  # Return any metrics to log\n",
                "\n",
                "\n",
                "# Register in BASELINE_REGISTRY\n",
                "BASELINE_REGISTRY = {\n",
                "    # ... existing\n",
                "    \"percentile\": MyCustomBaseline,\n",
                "}\n",
                "```"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "98g3h8bu2q8",
            "metadata": {},
            "source": [
                "### 11.5 Adding a New Configuration Section\n",
                "\n",
                "**Location:** `logic/src/configs/`\n",
                "\n",
                "```python\n",
                "# logic/src/configs/my_config.py\n",
                "\n",
                "from dataclasses import dataclass, field\n",
                "from typing import Optional, List, Dict, Any\n",
                "\n",
                "\n",
                "@dataclass\n",
                "class MyFeatureConfig:\n",
                "    \"\"\"Configuration for my new feature.\n",
                "    \n",
                "    Attributes:\n",
                "        enabled: Whether feature is enabled\n",
                "        param1: First parameter\n",
                "        param2: Second parameter\n",
                "        options: List of options\n",
                "    \"\"\"\n",
                "    \n",
                "    enabled: bool = False\n",
                "    param1: float = 1.0\n",
                "    param2: int = 10\n",
                "    options: List[str] = field(default_factory=lambda: [\"option1\", \"option2\"])\n",
                "    advanced_settings: Dict[str, Any] = field(default_factory=dict)\n",
                "```\n",
                "\n",
                "**Add to root config:**\n",
                "\n",
                "```python\n",
                "# logic/src/configs/__init__.py\n",
                "\n",
                "from .my_config import MyFeatureConfig\n",
                "\n",
                "@dataclass\n",
                "class Config:\n",
                "    # ... existing fields\n",
                "    my_feature: MyFeatureConfig = field(default_factory=MyFeatureConfig)\n",
                "```\n",
                "\n",
                "---\n",
                "\n",
                "## 12. Hydra + Lightning + TorchRL Best Practices\n",
                "\n",
                "### 12.1 Hydra Configuration Best Practices\n",
                "\n",
                "#### Structured Configs with Dataclasses\n",
                "\n",
                "```python\n",
                "from dataclasses import dataclass, field\n",
                "from omegaconf import MISSING  # For required fields\n",
                "\n",
                "@dataclass\n",
                "class MyConfig:\n",
                "    # Required field (must be set)\n",
                "    name: str = MISSING\n",
                "    \n",
                "    # Optional with default\n",
                "    learning_rate: float = 1e-4\n",
                "    \n",
                "    # List with factory\n",
                "    layers: List[int] = field(default_factory=lambda: [128, 256, 128])\n",
                "    \n",
                "    # Nested config\n",
                "    optimizer: OptimizerConfig = field(default_factory=OptimizerConfig)\n",
                "```\n",
                "\n",
                "#### CLI Override Patterns\n",
                "\n",
                "```bash\n",
                "# Simple override\n",
                "python main.py train_lightning model.embed_dim=256\n",
                "\n",
                "# Nested override\n",
                "python main.py train_lightning optim.lr_scheduler_kwargs.T_max=100\n",
                "\n",
                "# List override (use quotes)\n",
                "python main.py train_lightning 'model.layers=[64,128,64]'\n",
                "\n",
                "# Dict override\n",
                "python main.py train_lightning 'rl.gdpo_objective_keys=[\"cost\",\"overflow\"]'\n",
                "\n",
                "# Boolean flags\n",
                "python main.py train_lightning train.eval_only=true\n",
                "\n",
                "# Override with None\n",
                "python main.py train_lightning train.val_dataset=null\n",
                "\n",
                "# Multiple overrides\n",
                "python main.py train_lightning model=am env.name=vrpp train.n_epochs=50\n",
                "```\n",
                "\n",
                "#### Multi-run and Sweeps\n",
                "\n",
                "```bash\n",
                "# Grid search over multiple values\n",
                "python main.py train_lightning -m model.embed_dim=64,128,256 optim.lr=1e-3,1e-4\n",
                "\n",
                "# Range sweep\n",
                "python main.py train_lightning -m 'train.batch_size=range(64,512,64)'\n",
                "\n",
                "# Glob pattern for files\n",
                "python main.py train_lightning -m 'train.val_dataset=glob(data/*.pkl)'\n",
                "```\n",
                "\n",
                "### 12.2 PyTorch Lightning Best Practices\n",
                "\n",
                "#### Logging Metrics\n",
                "\n",
                "```python\n",
                "class MyModule(RL4COLitModule):\n",
                "    def calculate_loss(self, td, out, batch_idx, env=None):\n",
                "        # Log scalar metrics\n",
                "        self.log(\"train/loss\", loss, prog_bar=True)\n",
                "        self.log(\"train/reward\", reward.mean(), sync_dist=True)\n",
                "        \n",
                "        # Log multiple metrics at once\n",
                "        self.log_dict({\n",
                "            \"train/advantage\": advantage.mean(),\n",
                "            \"train/baseline\": baseline_val.mean(),\n",
                "            \"train/entropy\": entropy.mean(),\n",
                "        })\n",
                "        \n",
                "        # Log with specific settings\n",
                "        self.log(\n",
                "            \"train/custom\",\n",
                "            value,\n",
                "            on_step=True,      # Log at each step\n",
                "            on_epoch=True,     # Also log epoch average\n",
                "            prog_bar=True,     # Show in progress bar\n",
                "            logger=True,       # Send to logger\n",
                "            sync_dist=True,    # Sync across GPUs\n",
                "        )\n",
                "        \n",
                "        return loss\n",
                "```\n",
                "\n",
                "#### Checkpointing\n",
                "\n",
                "```python\n",
                "from pytorch_lightning.callbacks import ModelCheckpoint\n",
                "\n",
                "# Save best model by validation reward\n",
                "checkpoint_callback = ModelCheckpoint(\n",
                "    dirpath=\"checkpoints/\",\n",
                "    filename=\"best-{epoch:02d}-{val_reward:.4f}\",\n",
                "    monitor=\"val/reward\",\n",
                "    mode=\"max\",\n",
                "    save_top_k=3,\n",
                "    save_last=True,\n",
                ")\n",
                "\n",
                "# Resume from checkpoint\n",
                "trainer.fit(model, ckpt_path=\"checkpoints/last.ckpt\")\n",
                "```\n",
                "\n",
                "#### Learning Rate Scheduling\n",
                "\n",
                "```python\n",
                "class MyModule(RL4COLitModule):\n",
                "    def configure_optimizers(self):\n",
                "        optimizer = torch.optim.Adam(self.parameters(), lr=self.lr)\n",
                "        \n",
                "        # Cosine annealing\n",
                "        scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(\n",
                "            optimizer, T_max=self.trainer.max_epochs\n",
                "        )\n",
                "        \n",
                "        return {\n",
                "            \"optimizer\": optimizer,\n",
                "            \"lr_scheduler\": {\n",
                "                \"scheduler\": scheduler,\n",
                "                \"interval\": \"epoch\",  # or \"step\"\n",
                "                \"frequency\": 1,\n",
                "                \"monitor\": \"val/reward\",\n",
                "            }\n",
                "        }\n",
                "```\n",
                "\n",
                "### 12.3 TensorDict Best Practices\n",
                "\n",
                "#### Efficient Batched Operations\n",
                "\n",
                "```python\n",
                "from tensordict import TensorDict\n",
                "\n",
                "# Create TensorDict\n",
                "td = TensorDict({\n",
                "    \"locs\": torch.rand(batch_size, n_nodes, 2),\n",
                "    \"demand\": torch.rand(batch_size, n_nodes),\n",
                "}, batch_size=[batch_size])\n",
                "\n",
                "# Move to device (moves all tensors)\n",
                "td = td.to(device)\n",
                "\n",
                "# Clone for modification\n",
                "td_clone = td.clone()\n",
                "\n",
                "# Update in-place\n",
                "td[\"visited\"] = torch.zeros(batch_size, n_nodes, dtype=torch.bool)\n",
                "\n",
                "# Batch indexing\n",
                "subset = td[0:10]  # First 10 samples\n",
                "\n",
                "# Apply function to all tensors\n",
                "td = td.apply(lambda x: x.float())\n",
                "```\n",
                "\n",
                "#### Environment State Management\n",
                "\n",
                "```python\n",
                "# Reset returns TensorDict with initial state\n",
                "td = env.reset(batch)\n",
                "\n",
                "# Step modifies state in-place\n",
                "td[\"action\"] = action\n",
                "td = env.step(td)\n",
                "\n",
                "# Access state components\n",
                "current_node = td[\"current_node\"]\n",
                "action_mask = td[\"action_mask\"]\n",
                "done = td[\"done\"]\n",
                "```\n",
                "\n",
                "### 12.4 Common Pitfalls and Solutions\n",
                "\n",
                "| Pitfall | Solution |\n",
                "|---------|----------|\n",
                "| **OOM on large batches** | Use `accumulate_grad_batches` in trainer |\n",
                "| **Slow data loading** | Increase `num_workers`, use `pin_memory=True` |\n",
                "| **NaN in loss** | Check gradient clipping, reduce learning rate |\n",
                "| **Baseline not updating** | Verify `epoch_callback` is called |\n",
                "| **Config not merging** | Use `OmegaConf.merge()` for nested configs |\n",
                "| **GPU memory leak** | Call `.detach()` on baseline values |\n",
                "\n",
                "### 12.5 Debugging Tips\n",
                "\n",
                "```python\n",
                "# Fast dev run (1 batch)\n",
                "trainer = Trainer(fast_dev_run=True)\n",
                "\n",
                "# Limit batches for debugging\n",
                "trainer = Trainer(\n",
                "    limit_train_batches=10,\n",
                "    limit_val_batches=5,\n",
                ")\n",
                "\n",
                "# Enable gradient anomaly detection\n",
                "torch.autograd.set_detect_anomaly(True)\n",
                "\n",
                "# Profile training\n",
                "trainer = Trainer(profiler=\"simple\")  # or \"advanced\"\n",
                "\n",
                "# Check model device\n",
                "print(f\"Model device: {next(model.parameters()).device}\")\n",
                "\n",
                "# Check TensorDict device\n",
                "print(f\"TD device: {td.device}\")\n",
                "```\n",
                "\n",
                "### 12.6 Performance Optimization\n",
                "\n",
                "```python\n",
                "# Mixed precision training (faster on modern GPUs)\n",
                "trainer = Trainer(precision=\"16-mixed\")\n",
                "\n",
                "# Gradient accumulation for effective larger batches\n",
                "trainer = Trainer(accumulate_grad_batches=4)\n",
                "\n",
                "# Compile model (PyTorch 2.0+)\n",
                "model = torch.compile(model)\n",
                "\n",
                "# Use persistent workers\n",
                "DataLoader(..., persistent_workers=True, num_workers=4)\n",
                "\n",
                "# Pin memory for faster GPU transfer\n",
                "DataLoader(..., pin_memory=True)\n",
                "```"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "ds2r9ixa7nq",
            "metadata": {},
            "source": [
                "---\n",
                "\n",
                "## 13. Troubleshooting & Common Patterns\n",
                "\n",
                "### 13.1 Common Errors and Solutions\n",
                "\n",
                "#### \"CUDA out of memory\"\n",
                "```python\n",
                "# Solution 1: Reduce batch size\n",
                "train.batch_size=128  # Instead of 256\n",
                "\n",
                "# Solution 2: Use gradient accumulation\n",
                "trainer = Trainer(accumulate_grad_batches=2)\n",
                "\n",
                "# Solution 3: Use mixed precision\n",
                "trainer = Trainer(precision=\"16-mixed\")\n",
                "\n",
                "# Solution 4: Clear cache between batches\n",
                "torch.cuda.empty_cache()\n",
                "```\n",
                "\n",
                "#### \"NaN in loss\"\n",
                "```python\n",
                "# Check 1: Gradient clipping\n",
                "rl.max_grad_norm=0.5  # Reduce from 1.0\n",
                "\n",
                "# Check 2: Learning rate\n",
                "optim.lr=1e-5  # Reduce from 1e-4\n",
                "\n",
                "# Check 3: Baseline values\n",
                "baseline_val = baseline_val.detach()  # Must detach!\n",
                "\n",
                "# Check 4: Log likelihood explosion\n",
                "log_likelihood = log_likelihood.clamp(min=-100, max=0)\n",
                "```\n",
                "\n",
                "#### \"Baseline not improving\"\n",
                "```python\n",
                "# Solution: Check T-test threshold\n",
                "rl.bl_alpha=0.1  # More lenient (default 0.05)\n",
                "\n",
                "# Or use exponential baseline instead\n",
                "rl.baseline=\"exponential\"\n",
                "rl.exp_beta=0.9\n",
                "```\n",
                "\n",
                "#### \"Reward not improving\"\n",
                "```python\n",
                "# Check 1: Increase exploration\n",
                "rl.entropy_weight=0.01\n",
                "\n",
                "# Check 2: Use warmup baseline\n",
                "rl.bl_warmup_epochs=5\n",
                "\n",
                "# Check 3: Try different algorithm\n",
                "rl.algorithm=pomo  # If problem has symmetry\n",
                "```\n",
                "\n",
                "### 13.2 Recommended Configurations by Problem\n",
                "\n",
                "#### VRPP (Vehicle Routing with Profits)\n",
                "```bash\n",
                "python main.py train_lightning \\\n",
                "    env.name=vrpp \\\n",
                "    env.num_loc=50 \\\n",
                "    model=am \\\n",
                "    model.embed_dim=128 \\\n",
                "    rl.algorithm=reinforce \\\n",
                "    rl.baseline=rollout \\\n",
                "    train.n_epochs=100 \\\n",
                "    train.batch_size=256\n",
                "```\n",
                "\n",
                "#### WCVRP (Waste Collection VRP)\n",
                "```bash\n",
                "python main.py train_lightning \\\n",
                "    env.name=cwcvrp \\\n",
                "    env.num_loc=50 \\\n",
                "    env.capacity=100 \\\n",
                "    model=am \\\n",
                "    model.encoder_type=gat \\\n",
                "    rl.algorithm=pomo \\\n",
                "    rl.num_augment=8 \\\n",
                "    train.n_epochs=100\n",
                "```\n",
                "\n",
                "#### Large-scale (100+ nodes)\n",
                "```bash\n",
                "python main.py train_lightning \\\n",
                "    env.num_loc=100 \\\n",
                "    model.embed_dim=256 \\\n",
                "    model.num_encoder_layers=6 \\\n",
                "    train.batch_size=128 \\\n",
                "    train.precision=16-mixed \\\n",
                "    optim.lr=5e-5\n",
                "```\n",
                "\n",
                "#### Multi-day temporal\n",
                "```bash\n",
                "python main.py train_lightning \\\n",
                "    env.name=sdwcvrp \\\n",
                "    model=temporal \\\n",
                "    model.temporal_horizon=7 \\\n",
                "    train.train_time=true \\\n",
                "    train.eval_time_days=7\n",
                "```\n",
                "\n",
                "### 13.3 Quick Reference Cheatsheet\n",
                "\n",
                "#### All Available Algorithms\n",
                "| Key | Class | Best For |\n",
                "|-----|-------|----------|\n",
                "| `reinforce` | REINFORCE | Simple baseline |\n",
                "| `ppo` | PPO | Stable training |\n",
                "| `sapo` | SAPO | Adaptive clipping |\n",
                "| `gspo` | GSPO | Sequence-level |\n",
                "| `gdpo` | GDPO | Multi-objective |\n",
                "| `dr_grpo` | DRGRPO | Divergence-regularized |\n",
                "| `pomo` | POMO | Symmetric problems |\n",
                "| `symnco` | SymNCO | Symmetry exploitation |\n",
                "| `imitation` | ImitationLearning | Expert guidance |\n",
                "| `adaptive_imitation` | AdaptiveImitation | IL → RL |\n",
                "| `hrl` | HRLModule | Hierarchical |\n",
                "\n",
                "#### All Available Baselines\n",
                "| Key | Class | Description |\n",
                "|-----|-------|-------------|\n",
                "| `none` | NoBaseline | No baseline (high variance) |\n",
                "| `exponential` | ExponentialBaseline | Moving average |\n",
                "| `rollout` | RolloutBaseline | Greedy rollout |\n",
                "| `critic` | CriticBaseline | Learned value network |\n",
                "| `warmup` | WarmupBaseline | Gradual transition |\n",
                "| `pomo` | POMOBaseline | Multi-start mean |\n",
                "\n",
                "#### All Available Environments\n",
                "| Key | Class | Description |\n",
                "|-----|-------|-------------|\n",
                "| `vrpp` | VRPPEnv | Vehicle Routing with Profits |\n",
                "| `cvrpp` | CVRPPEnv | Capacitated VRPP |\n",
                "| `wcvrp` | WCVRPEnv | Waste Collection VRP |\n",
                "| `cwcvrp` | CWCVRPEnv | Capacitated WCVRP |\n",
                "| `sdwcvrp` | SDWCVRPEnv | Stochastic Demand WCVRP |\n",
                "| `scwcvrp` | SCWCVRPEnv | Selective Capacitated WCVRP |\n",
                "\n",
                "#### All Available Encoders\n",
                "| Key | Class | Architecture |\n",
                "|-----|-------|--------------|\n",
                "| `gat` | GraphAttentionEncoder | Multi-head GAT |\n",
                "| `gac` | GraphAttConvEncoder | GAT + edge features |\n",
                "| `tgc` | TransGraphConvEncoder | Transformer-style |\n",
                "| `ggac` | GatedGraphAttConvEncoder | Gated GAT |\n",
                "| `gcn` | GCNEncoder | Standard GCN |\n",
                "| `mlp` | MLPEncoder | No graph structure |\n",
                "| `ptr` | PointerEncoder | RNN-based |\n",
                "| `moe` | MOEEncoder | Mixture of Experts |\n",
                "\n",
                "#### All Available Models\n",
                "| Key | Class | Use Case |\n",
                "|-----|-------|----------|\n",
                "| `am` | AttentionModel | Standard routing |\n",
                "| `deep_decoder` | DeepDecoderAM | Complex problems |\n",
                "| `temporal` | TemporalAM | Multi-day scenarios |\n",
                "| `pointer` | PointerNetwork | Classic seq2seq |"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": ".venv",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.9.25"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 5
}
