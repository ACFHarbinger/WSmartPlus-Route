{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "intro-header",
   "metadata": {},
   "source": [
    "# WSmart+ Route: Lightning-Based RL Training Tutorial\n",
    "\n",
    "**Version:** 1.0  \n",
    "**Last Updated:** January 2026\n",
    "\n",
    "This comprehensive tutorial covers the reinforcement learning training pipeline built on **PyTorch Lightning** and **Hydra** for combinatorial optimization problems.\n",
    "\n",
    "## Table of Contents\n",
    "\n",
    "1. [Overview & Architecture](#1-overview--architecture)\n",
    "2. [Hydra Configuration System](#2-hydra-configuration-system)\n",
    "3. [PyTorch Lightning Modules](#3-pytorch-lightning-modules)\n",
    "4. [RL Algorithms](#4-rl-algorithms)\n",
    "5. [Baselines for Variance Reduction](#5-baselines-for-variance-reduction)\n",
    "6. [Environments & Data Generation](#6-environments--data-generation)\n",
    "7. [Meta-Learning](#7-meta-learning)\n",
    "8. [Hyperparameter Optimization](#8-hyperparameter-optimization)\n",
    "9. [Practical Examples](#9-practical-examples)\n",
    "10. [Advanced Topics](#10-advanced-topics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "setup-imports",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard notebook setup\n",
    "from notebook_setup import setup_google_colab, setup_home_directory\n",
    "\n",
    "NOTEBOOK_NAME = \"lightning_rl_training_tutorial\"\n",
    "home_dir = setup_home_directory(NOTEBOOK_NAME)\n",
    "IN_COLAB, gdrive, gfiles = setup_google_colab(NOTEBOOK_NAME)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "section-1",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 1. Overview & Architecture\n",
    "\n",
    "The WSmart+ Route training pipeline is built on three foundational technologies:\n",
    "\n",
    "### 1.1 Core Technologies\n",
    "\n",
    "| Technology | Purpose | Key Benefits |\n",
    "|------------|---------|-------------|\n",
    "| **PyTorch Lightning** | Training orchestration | Automatic GPU management, logging, checkpointing |\n",
    "| **Hydra** | Configuration management | Hierarchical configs, CLI overrides, experiment tracking |\n",
    "| **TensorDict** | State management | Efficient batched operations, device-agnostic tensors |\n",
    "\n",
    "### 1.2 Pipeline Architecture\n",
    "\n",
    "```\n",
    "┌─────────────────────────────────────────────────────────────────┐\n",
    "│                      train_lightning.py                         │\n",
    "│                    (Hydra CLI Entry Point)                      │\n",
    "└─────────────────────────────────────────────────────────────────┘\n",
    "                              │\n",
    "              ┌───────────────┼───────────────┐\n",
    "              │               │               │\n",
    "              ▼               ▼               ▼\n",
    "┌─────────────────┐ ┌─────────────────┐ ┌─────────────────┐\n",
    "│   Config        │ │   Environment   │ │   RL Module     │\n",
    "│   (Hydra)       │ │   (RL4COEnv)    │ │   (Lightning)   │\n",
    "└─────────────────┘ └─────────────────┘ └─────────────────┘\n",
    "                              │\n",
    "                              ▼\n",
    "              ┌───────────────────────────────┐\n",
    "              │        WSTrainer              │\n",
    "              │   (PyTorch Lightning)         │\n",
    "              └───────────────────────────────┘\n",
    "                              │\n",
    "                              ▼\n",
    "              ┌───────────────────────────────┐\n",
    "              │   Training Loop               │\n",
    "              │   - Data Generation           │\n",
    "              │   - Forward Pass              │\n",
    "              │   - Loss Computation          │\n",
    "              │   - Baseline Updates          │\n",
    "              └───────────────────────────────┘\n",
    "```\n",
    "\n",
    "### 1.3 Directory Structure\n",
    "\n",
    "```\n",
    "logic/src/pipeline/rl/\n",
    "├── core/                    # RL algorithms\n",
    "│   ├── base.py             # RL4COLitModule (base Lightning module)\n",
    "│   ├── baselines.py        # Variance reduction baselines\n",
    "│   ├── reinforce.py        # REINFORCE algorithm\n",
    "│   ├── ppo.py              # Proximal Policy Optimization\n",
    "│   ├── sapo.py             # Soft Adaptive PPO\n",
    "│   ├── gspo.py             # Group Sequence PO\n",
    "│   ├── dr_grpo.py          # Divergence-Regularized GRPO\n",
    "│   ├── gdpo.py             # Group Distributional PO\n",
    "│   ├── pomo.py             # Policy Optimization Multiple Optima\n",
    "│   ├── symnco.py           # Symmetry-aware NCO\n",
    "│   ├── hrl.py              # Hierarchical RL\n",
    "│   ├── imitation.py        # Imitation Learning\n",
    "│   └── adaptive_imitation.py\n",
    "├── meta/                    # Meta-learning\n",
    "│   ├── module.py           # MetaRLModule wrapper\n",
    "│   ├── weight_optimizer.py\n",
    "│   ├── contextual_bandits.py\n",
    "│   ├── td_learning.py\n",
    "│   └── multi_objective.py\n",
    "├── hpo/                     # Hyperparameter optimization\n",
    "│   ├── optuna_hpo.py\n",
    "│   └── dehb.py\n",
    "└── features/                # Training utilities\n",
    "    ├── epoch.py\n",
    "    ├── post_processing.py\n",
    "    └── time_training.py\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "section-2",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 2. Hydra Configuration System\n",
    "\n",
    "The training pipeline uses **Hydra** for configuration management, enabling:\n",
    "- Hierarchical configuration with dataclasses\n",
    "- Command-line overrides\n",
    "- Experiment tracking and reproducibility\n",
    "\n",
    "### 2.1 Configuration Dataclasses\n",
    "\n",
    "All configurations are defined in `logic/src/configs/__init__.py`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "config-overview",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass, field\n",
    "from typing import Any, Dict, List, Optional\n",
    "\n",
    "# Examine the configuration structure\n",
    "from logic.src.configs import (\n",
    "    Config,\n",
    "    EnvConfig,\n",
    "    ModelConfig,\n",
    "    TrainConfig,\n",
    "    OptimConfig,\n",
    "    RLConfig,\n",
    "    HPOConfig,\n",
    ")\n",
    "\n",
    "# Print available config sections\n",
    "print(\"Configuration Sections:\")\n",
    "print(\"=\"*50)\n",
    "for config_cls in [EnvConfig, ModelConfig, TrainConfig, OptimConfig, RLConfig, HPOConfig]:\n",
    "    print(f\"\\n{config_cls.__name__}:\")\n",
    "    for field_name, field_type in config_cls.__annotations__.items():\n",
    "        default = getattr(config_cls, field_name, \"<no default>\")\n",
    "        if callable(default) and hasattr(default, \"__name__\"):\n",
    "            default = f\"<factory: {default.__name__}>\"\n",
    "        print(f\"  {field_name}: {field_type} = {default}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "config-sections",
   "metadata": {},
   "source": [
    "### 2.2 Configuration Sections Explained\n",
    "\n",
    "#### EnvConfig - Environment Settings\n",
    "```python\n",
    "@dataclass\n",
    "class EnvConfig:\n",
    "    name: str = \"vrpp\"           # Problem type: vrpp, wcvrp, cwcvrp, sdwcvrp\n",
    "    num_loc: int = 50             # Number of locations (nodes)\n",
    "    min_loc: float = 0.0          # Min coordinate value\n",
    "    max_loc: float = 1.0          # Max coordinate value\n",
    "    capacity: Optional[float] = None  # Vehicle capacity\n",
    "    overflow_penalty: float = 1.0     # Penalty for bin overflow\n",
    "    collection_reward: float = 1.0    # Reward for waste collection\n",
    "    cost_weight: float = 1.0          # Weight for travel cost\n",
    "    prize_weight: float = 1.0         # Weight for prizes (VRPP)\n",
    "```\n",
    "\n",
    "#### ModelConfig - Neural Network Architecture\n",
    "```python\n",
    "@dataclass\n",
    "class ModelConfig:\n",
    "    name: str = \"am\"              # Model: am, deep_decoder, temporal, pointer, symnco\n",
    "    embed_dim: int = 128          # Embedding dimension\n",
    "    hidden_dim: int = 512         # Hidden layer dimension\n",
    "    num_encoder_layers: int = 3   # Number of encoder layers\n",
    "    num_decoder_layers: int = 3   # Number of decoder layers (deep_decoder)\n",
    "    num_heads: int = 8            # Attention heads\n",
    "    encoder_type: str = \"gat\"     # Encoder type: gat, gcn, mlp\n",
    "    normalization: str = \"instance\"  # Normalization: instance, batch, layer\n",
    "    activation: str = \"gelu\"         # Activation function\n",
    "    dropout: float = 0.1             # Dropout rate\n",
    "```\n",
    "\n",
    "#### RLConfig - RL Algorithm Settings\n",
    "```python\n",
    "@dataclass\n",
    "class RLConfig:\n",
    "    algorithm: str = \"reinforce\"  # reinforce, ppo, sapo, gspo, pomo, symnco, hrl\n",
    "    baseline: str = \"rollout\"     # none, exponential, rollout, critic, pomo\n",
    "    entropy_weight: float = 0.0   # Entropy regularization\n",
    "    max_grad_norm: float = 1.0    # Gradient clipping\n",
    "    \n",
    "    # PPO-specific\n",
    "    ppo_epochs: int = 10          # Inner PPO epochs\n",
    "    eps_clip: float = 0.2         # Clipping epsilon\n",
    "    value_loss_weight: float = 0.5\n",
    "    \n",
    "    # Meta-learning\n",
    "    use_meta: bool = False        # Enable meta-learning wrapper\n",
    "    meta_strategy: str = \"rnn\"    # rnn, bandit, morl, tdl, hypernet\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cli-usage",
   "metadata": {},
   "source": [
    "### 2.3 Command-Line Interface Usage\n",
    "\n",
    "The main entry point is `train_lightning.py`, accessed via:\n",
    "\n",
    "```bash\n",
    "python main.py train_lightning [CONFIG_OVERRIDES]\n",
    "```\n",
    "\n",
    "#### Basic Training Examples\n",
    "\n",
    "```bash\n",
    "# Train Attention Model on VRPP with 50 nodes\n",
    "python main.py train_lightning model=am env.name=vrpp env.num_loc=50\n",
    "\n",
    "# Train with PPO algorithm\n",
    "python main.py train_lightning rl.algorithm=ppo rl.ppo_epochs=10\n",
    "\n",
    "# Use different baseline\n",
    "python main.py train_lightning rl.baseline=exponential\n",
    "\n",
    "# Custom training parameters\n",
    "python main.py train_lightning \\\n",
    "    train.n_epochs=100 \\\n",
    "    train.batch_size=256 \\\n",
    "    optim.lr=1e-4\n",
    "```\n",
    "\n",
    "#### Advanced Configurations\n",
    "\n",
    "```bash\n",
    "# POMO with data augmentation\n",
    "python main.py train_lightning \\\n",
    "    rl.algorithm=pomo \\\n",
    "    rl.num_augment=8 \\\n",
    "    rl.augment_fn=dihedral8\n",
    "\n",
    "# Hierarchical RL\n",
    "python main.py train_lightning \\\n",
    "    rl.algorithm=hrl \\\n",
    "    rl.meta_hidden_dim=128\n",
    "\n",
    "# With Meta-Learning wrapper\n",
    "python main.py train_lightning \\\n",
    "    rl.use_meta=true \\\n",
    "    rl.meta_strategy=rnn \\\n",
    "    rl.meta_lr=1e-3\n",
    "\n",
    "# Hyperparameter optimization\n",
    "python main.py train_lightning \\\n",
    "    hpo.n_trials=50 \\\n",
    "    hpo.method=tpe\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "config-example",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Programmatically create a configuration\n",
    "from logic.src.configs import Config, EnvConfig, ModelConfig, TrainConfig, OptimConfig, RLConfig\n",
    "\n",
    "# Create custom configuration\n",
    "cfg = Config(\n",
    "    env=EnvConfig(\n",
    "        name=\"vrpp\",\n",
    "        num_loc=50,\n",
    "        capacity=100.0,\n",
    "    ),\n",
    "    model=ModelConfig(\n",
    "        name=\"am\",\n",
    "        embed_dim=128,\n",
    "        num_encoder_layers=3,\n",
    "        num_heads=8,\n",
    "    ),\n",
    "    train=TrainConfig(\n",
    "        n_epochs=10,\n",
    "        batch_size=256,\n",
    "        train_data_size=10000,\n",
    "    ),\n",
    "    optim=OptimConfig(\n",
    "        optimizer=\"adam\",\n",
    "        lr=1e-4,\n",
    "    ),\n",
    "    rl=RLConfig(\n",
    "        algorithm=\"reinforce\",\n",
    "        baseline=\"rollout\",\n",
    "    ),\n",
    "    seed=42,\n",
    "    device=\"cuda\" if torch.cuda.is_available() else \"cpu\",\n",
    ")\n",
    "\n",
    "print(f\"Configuration created:\")\n",
    "print(f\"  Environment: {cfg.env.name} with {cfg.env.num_loc} locations\")\n",
    "print(f\"  Model: {cfg.model.name} with {cfg.model.embed_dim}d embeddings\")\n",
    "print(f\"  Algorithm: {cfg.rl.algorithm} with {cfg.rl.baseline} baseline\")\n",
    "print(f\"  Device: {cfg.device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "section-3",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 3. PyTorch Lightning Modules\n",
    "\n",
    "All RL algorithms inherit from `RL4COLitModule`, which provides:\n",
    "- Training/validation/test loops\n",
    "- Automatic optimizer configuration\n",
    "- Data loading with generators\n",
    "- Baseline integration\n",
    "- Metric logging\n",
    "\n",
    "### 3.1 Base Module Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "lightning-module",
   "metadata": {},
   "outputs": [],
   "source": [
    "from logic.src.pipeline.rl.core.base import RL4COLitModule\n",
    "import inspect\n",
    "\n",
    "# Show the base class signature\n",
    "print(\"RL4COLitModule.__init__ signature:\")\n",
    "print(inspect.signature(RL4COLitModule.__init__))\n",
    "\n",
    "print(\"\\nKey Methods:\")\n",
    "for name, method in inspect.getmembers(RL4COLitModule, predicate=inspect.isfunction):\n",
    "    if not name.startswith('_') or name in ['__init__']:\n",
    "        doc = method.__doc__\n",
    "        first_line = doc.split('\\n')[0].strip() if doc else \"No docstring\"\n",
    "        print(f\"  {name}: {first_line}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "module-lifecycle",
   "metadata": {},
   "source": [
    "### 3.2 Training Lifecycle\n",
    "\n",
    "```python\n",
    "class RL4COLitModule(pl.LightningModule, ABC):\n",
    "    \"\"\"\n",
    "    Training Lifecycle:\n",
    "    \n",
    "    1. setup(stage='fit'):\n",
    "       - Create train_dataset (GeneratorDataset)\n",
    "       - Create val_dataset\n",
    "       \n",
    "    2. for epoch in range(n_epochs):\n",
    "       \n",
    "       3. on_train_epoch_start():\n",
    "          - Wrap dataset with baseline (if RolloutBaseline)\n",
    "          \n",
    "       4. for batch in train_dataloader():\n",
    "          \n",
    "          5. training_step(batch):\n",
    "             - Unwrap batch (baseline values)\n",
    "             - shared_step():\n",
    "               a. env.reset(batch)\n",
    "               b. policy(td, env, decode_type=\"sampling\")\n",
    "               c. calculate_loss()  # Algorithm-specific\n",
    "             - return loss\n",
    "             \n",
    "          6. on_before_optimizer_step():\n",
    "             - Gradient clipping\n",
    "             \n",
    "       7. validation_step(batch):\n",
    "          - shared_step() with decode_type=\"greedy\"\n",
    "          \n",
    "       8. on_train_epoch_end():\n",
    "          - baseline.epoch_callback()\n",
    "          - Optionally regenerate dataset\n",
    "    \"\"\"\n",
    "```\n",
    "\n",
    "### 3.3 Key Methods Explained\n",
    "\n",
    "#### `shared_step()` - Common Training Logic\n",
    "```python\n",
    "def shared_step(self, batch, batch_idx, phase):\n",
    "    # 1. Unwrap baseline values if present\n",
    "    batch, baseline_val = self.baseline.unwrap_batch(batch)\n",
    "    \n",
    "    # 2. Move to device\n",
    "    batch = batch.to(self.device)\n",
    "    \n",
    "    # 3. Reset environment with batch\n",
    "    td = self.env.reset(batch)\n",
    "    \n",
    "    # 4. Run policy\n",
    "    out = self.policy(\n",
    "        td, self.env,\n",
    "        decode_type=\"sampling\" if phase == \"train\" else \"greedy\"\n",
    "    )\n",
    "    \n",
    "    # 5. Compute loss (training only)\n",
    "    if phase == \"train\":\n",
    "        out[\"loss\"] = self.calculate_loss(td, out, batch_idx)\n",
    "    \n",
    "    # 6. Log metrics\n",
    "    self.log(f\"{phase}/reward\", out[\"reward\"].mean())\n",
    "    \n",
    "    return out\n",
    "```\n",
    "\n",
    "#### `calculate_loss()` - Algorithm-Specific\n",
    "This is the **abstract method** that each RL algorithm must implement."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "show-calculate-loss",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: REINFORCE loss implementation\n",
    "from logic.src.pipeline.rl.core.reinforce import REINFORCE\n",
    "\n",
    "# Show the calculate_loss implementation\n",
    "print(\"REINFORCE.calculate_loss source:\")\n",
    "print(inspect.getsource(REINFORCE.calculate_loss))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "section-4",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 4. RL Algorithms\n",
    "\n",
    "The pipeline supports multiple RL algorithms, each suited for different scenarios:\n",
    "\n",
    "| Algorithm | Best For | Key Features |\n",
    "|-----------|----------|-------------|\n",
    "| **REINFORCE** | Simple baseline | Standard policy gradient |\n",
    "| **PPO** | Stable training | Clipped surrogate, multiple epochs |\n",
    "| **SAPO** | Adaptive clipping | Soft gating instead of hard clip |\n",
    "| **GSPO** | Sequence-level | Group-based importance ratios |\n",
    "| **POMO** | Multiple solutions | Data augmentation, multi-start |\n",
    "| **SymNCO** | Symmetry exploitation | Invariance-aware training |\n",
    "| **HRL** | Hierarchical decisions | Manager-Worker architecture |\n",
    "| **Imitation** | Expert guidance | Learn from HGS/ALNS experts |\n",
    "\n",
    "### 4.1 REINFORCE with Baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "reinforce-example",
   "metadata": {},
   "outputs": [],
   "source": [
    "from logic.src.pipeline.rl import REINFORCE\n",
    "from logic.src.envs import get_env\n",
    "from logic.src.models.policies import AttentionModelPolicy\n",
    "\n",
    "# Create environment\n",
    "env = get_env(\"vrpp\", num_loc=20, device=\"cpu\")\n",
    "\n",
    "# Create policy\n",
    "policy = AttentionModelPolicy(\n",
    "    env_name=\"vrpp\",\n",
    "    embed_dim=64,\n",
    "    hidden_dim=128,\n",
    "    n_encode_layers=2,\n",
    "    n_heads=4,\n",
    ")\n",
    "\n",
    "# Create REINFORCE module\n",
    "reinforce_module = REINFORCE(\n",
    "    env=env,\n",
    "    policy=policy,\n",
    "    baseline=\"exponential\",  # Using exponential baseline for demo\n",
    "    optimizer=\"adam\",\n",
    "    optimizer_kwargs={\"lr\": 1e-4},\n",
    "    train_data_size=1000,\n",
    "    val_data_size=100,\n",
    "    batch_size=64,\n",
    "    entropy_weight=0.01,\n",
    "    max_grad_norm=1.0,\n",
    ")\n",
    "\n",
    "print(f\"REINFORCE module created:\")\n",
    "print(f\"  Environment: {env.__class__.__name__}\")\n",
    "print(f\"  Policy parameters: {sum(p.numel() for p in policy.parameters()):,}\")\n",
    "print(f\"  Baseline: {reinforce_module.baseline_type}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ppo-section",
   "metadata": {},
   "source": [
    "### 4.2 PPO (Proximal Policy Optimization)\n",
    "\n",
    "PPO performs multiple optimization epochs per batch with clipped surrogate objective:\n",
    "\n",
    "$$\\mathcal{L}^{CLIP}(\\theta) = \\mathbb{E}\\left[\\min\\left(r_t(\\theta)\\hat{A}_t, \\text{clip}(r_t(\\theta), 1-\\epsilon, 1+\\epsilon)\\hat{A}_t\\right)\\right]$$\n",
    "\n",
    "where $r_t(\\theta) = \\frac{\\pi_\\theta(a_t|s_t)}{\\pi_{\\theta_{old}}(a_t|s_t)}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ppo-example",
   "metadata": {},
   "outputs": [],
   "source": [
    "from logic.src.pipeline.rl import PPO\n",
    "from logic.src.models.policies.critic import create_critic_from_actor\n",
    "\n",
    "# Create critic from actor architecture\n",
    "critic = create_critic_from_actor(\n",
    "    policy,\n",
    "    env_name=\"vrpp\",\n",
    "    embed_dim=64,\n",
    "    hidden_dim=128,\n",
    "    n_layers=2,\n",
    "    n_heads=4,\n",
    ")\n",
    "\n",
    "# Create PPO module\n",
    "ppo_module = PPO(\n",
    "    env=env,\n",
    "    policy=policy,\n",
    "    critic=critic,\n",
    "    ppo_epochs=10,           # Inner optimization epochs\n",
    "    eps_clip=0.2,            # Clipping epsilon\n",
    "    value_loss_weight=0.5,   # Critic loss weight\n",
    "    mini_batch_size=0.25,    # 25% of batch per mini-batch\n",
    "    optimizer=\"adam\",\n",
    "    optimizer_kwargs={\"lr\": 1e-4},\n",
    "    train_data_size=1000,\n",
    "    val_data_size=100,\n",
    "    batch_size=64,\n",
    ")\n",
    "\n",
    "print(f\"PPO module created:\")\n",
    "print(f\"  Actor parameters: {sum(p.numel() for p in policy.parameters()):,}\")\n",
    "print(f\"  Critic parameters: {sum(p.numel() for p in critic.parameters()):,}\")\n",
    "print(f\"  PPO epochs: {ppo_module.ppo_epochs}\")\n",
    "print(f\"  Clip epsilon: {ppo_module.eps_clip}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "pomo-section",
   "metadata": {},
   "source": [
    "### 4.3 POMO (Policy Optimization with Multiple Optima)\n",
    "\n",
    "POMO exploits problem symmetries through:\n",
    "1. **Data Augmentation**: Dihedral transformations (rotations, reflections)\n",
    "2. **Multi-start Decoding**: Try multiple starting nodes\n",
    "3. **Shared Baseline**: Mean reward across all augmentations/starts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "pomo-example",
   "metadata": {},
   "outputs": [],
   "source": [
    "from logic.src.pipeline.rl import POMO\n",
    "\n",
    "# Create POMO module\n",
    "pomo_module = POMO(\n",
    "    env=env,\n",
    "    policy=policy,\n",
    "    num_augment=8,           # Dihedral group D8\n",
    "    augment_fn=\"dihedral8\",  # Augmentation function\n",
    "    num_starts=None,         # Defaults to num_loc\n",
    "    optimizer=\"adam\",\n",
    "    optimizer_kwargs={\"lr\": 1e-4},\n",
    "    train_data_size=1000,\n",
    "    val_data_size=100,\n",
    "    batch_size=64,\n",
    ")\n",
    "\n",
    "print(f\"POMO module created:\")\n",
    "print(f\"  Number of augmentations: {pomo_module.num_augment}\")\n",
    "print(f\"  Augmentation function: {pomo_module.augment_fn}\")\n",
    "print(f\"  Number of starts: {pomo_module.num_starts or 'auto (num_loc)'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "algorithm-comparison",
   "metadata": {},
   "source": [
    "### 4.4 Algorithm Selection Guide\n",
    "\n",
    "```\n",
    "┌─────────────────────────────────────────────────────────────────┐\n",
    "│                    Algorithm Selection Tree                      │\n",
    "└─────────────────────────────────────────────────────────────────┘\n",
    "                              │\n",
    "          ┌───────────────────┼───────────────────┐\n",
    "          │                   │                   │\n",
    "    Need stability?    Need multiple      Learning from\n",
    "          │              solutions?          expert?\n",
    "          │                   │                   │\n",
    "          ▼                   ▼                   ▼\n",
    "    ┌─────────┐        ┌─────────┐        ┌─────────────┐\n",
    "    │   PPO   │        │  POMO   │        │  Imitation  │\n",
    "    │  SAPO   │        │ SymNCO  │        │  Adaptive   │\n",
    "    │  GSPO   │        └─────────┘        └─────────────┘\n",
    "    └─────────┘\n",
    "          │\n",
    "          │ Simple case?\n",
    "          ▼\n",
    "    ┌─────────┐\n",
    "    │REINFORCE│\n",
    "    └─────────┘\n",
    "```\n",
    "\n",
    "**Recommendations:**\n",
    "- **Start with REINFORCE + Rollout baseline** for initial experiments\n",
    "- **Use PPO** when training is unstable or gradients are noisy\n",
    "- **Use POMO** for problems with symmetric solutions (TSP, VRP)\n",
    "- **Use Imitation + Adaptive** to bootstrap from classical solvers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "section-5",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 5. Baselines for Variance Reduction\n",
    "\n",
    "Baselines reduce variance in policy gradient estimates:\n",
    "\n",
    "$$\\nabla_\\theta J(\\theta) = \\mathbb{E}\\left[(R - b(s)) \\nabla_\\theta \\log \\pi_\\theta(a|s)\\right]$$\n",
    "\n",
    "### 5.1 Available Baselines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "baselines-overview",
   "metadata": {},
   "outputs": [],
   "source": [
    "from logic.src.pipeline.rl.core.baselines import (\n",
    "    BASELINE_REGISTRY,\n",
    "    NoBaseline,\n",
    "    ExponentialBaseline,\n",
    "    RolloutBaseline,\n",
    "    CriticBaseline,\n",
    "    WarmupBaseline,\n",
    "    POMOBaseline,\n",
    "    get_baseline,\n",
    ")\n",
    "\n",
    "print(\"Available Baselines:\")\n",
    "print(\"=\"*60)\n",
    "for name, cls in BASELINE_REGISTRY.items():\n",
    "    doc = cls.__doc__ or \"No description\"\n",
    "    first_line = doc.split('\\n')[0].strip()\n",
    "    print(f\"  {name:15} - {first_line}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "baseline-details",
   "metadata": {},
   "source": [
    "### 5.2 Baseline Comparison\n",
    "\n",
    "| Baseline | Formula | Pros | Cons |\n",
    "|----------|---------|------|------|\n",
    "| **none** | $b = 0$ | Simple | High variance |\n",
    "| **exponential** | $b = \\beta \\cdot b + (1-\\beta) \\cdot \\bar{R}$ | Low compute | Biased |\n",
    "| **rollout** | $b = R^{greedy}_{\\pi_{old}}$ | Unbiased | Expensive |\n",
    "| **critic** | $b = V_\\phi(s)$ | Learned | Requires critic network |\n",
    "| **pomo** | $b = \\text{mean}(R_{starts})$ | Multi-solution | POMO-specific |\n",
    "\n",
    "### 5.3 Rollout Baseline Deep Dive\n",
    "\n",
    "The **RolloutBaseline** is the most commonly used baseline:\n",
    "\n",
    "1. **Pre-compute**: At epoch start, run greedy rollout on training data\n",
    "2. **Store**: Keep baseline values alongside training samples\n",
    "3. **Use**: During training, advantage = reward - stored_baseline\n",
    "4. **Update**: Periodically update baseline policy if improved"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "rollout-baseline-example",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Demonstrate RolloutBaseline usage\n",
    "from logic.src.pipeline.rl.core.baselines import RolloutBaseline\n",
    "\n",
    "# Create baseline with policy\n",
    "rollout_bl = RolloutBaseline(\n",
    "    policy=policy,\n",
    "    update_every=1,   # Check for update every epoch\n",
    "    bl_alpha=0.05,    # Significance level for T-test\n",
    ")\n",
    "\n",
    "print(\"RolloutBaseline:\")\n",
    "print(f\"  Update frequency: every {rollout_bl.update_every} epoch(s)\")\n",
    "print(f\"  T-test alpha: {rollout_bl.bl_alpha}\")\n",
    "print(f\"  Has baseline policy: {rollout_bl.baseline_policy is not None}\")\n",
    "\n",
    "# Key methods\n",
    "print(\"\\nKey Methods:\")\n",
    "print(\"  wrap_dataset(dataset, policy, env) - Pre-compute baseline values\")\n",
    "print(\"  unwrap_batch(batch) -> (data, baseline_val) - Extract baseline\")\n",
    "print(\"  eval(td, reward, env) - Compute baseline on-the-fly\")\n",
    "print(\"  epoch_callback(policy, epoch, val_dataset, env) - Update check\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "warmup-baseline",
   "metadata": {},
   "source": [
    "### 5.4 Warmup Baseline\n",
    "\n",
    "The **WarmupBaseline** provides gradual transition from exponential to target baseline:\n",
    "\n",
    "$$b_{\\text{warmup}} = \\alpha \\cdot b_{\\text{target}} + (1 - \\alpha) \\cdot b_{\\text{exponential}}$$\n",
    "\n",
    "where $\\alpha$ increases from 0 to 1 over warmup epochs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "warmup-example",
   "metadata": {},
   "outputs": [],
   "source": [
    "from logic.src.pipeline.rl.core.baselines import WarmupBaseline, RolloutBaseline\n",
    "\n",
    "# Create warmup baseline wrapping rollout\n",
    "target_baseline = RolloutBaseline(policy=policy)\n",
    "warmup_bl = WarmupBaseline(\n",
    "    baseline=target_baseline,\n",
    "    warmup_epochs=5,\n",
    "    beta=0.8,  # Exponential baseline decay factor\n",
    ")\n",
    "\n",
    "print(f\"WarmupBaseline Configuration:\")\n",
    "print(f\"  Target: {target_baseline.__class__.__name__}\")\n",
    "print(f\"  Warmup epochs: {warmup_bl.warmup_epochs}\")\n",
    "print(f\"  Current alpha: {warmup_bl.alpha}\")\n",
    "print(f\"\\nBlending schedule:\")\n",
    "for epoch in range(6):\n",
    "    alpha = min(1.0, (epoch + 1) / warmup_bl.warmup_epochs)\n",
    "    print(f\"  Epoch {epoch}: α = {alpha:.2f} ({int(alpha*100)}% target, {int((1-alpha)*100)}% exponential)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "section-6",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 6. Environments & Data Generation\n",
    "\n",
    "The environment system provides:\n",
    "- Problem-specific state transitions\n",
    "- Reward computation\n",
    "- Action masking for valid moves\n",
    "- On-the-fly data generation\n",
    "\n",
    "### 6.1 Environment Registry"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "env-registry",
   "metadata": {},
   "outputs": [],
   "source": [
    "from logic.src.envs import ENV_REGISTRY, get_env\n",
    "\n",
    "print(\"Available Environments:\")\n",
    "print(\"=\"*60)\n",
    "for name, cls in ENV_REGISTRY.items():\n",
    "    doc = cls.__doc__ or \"No description\"\n",
    "    first_line = doc.split('\\n')[0].strip() if doc else \"No description\"\n",
    "    print(f\"  {name:10} - {cls.__name__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "env-types",
   "metadata": {},
   "source": [
    "### 6.2 Environment Types\n",
    "\n",
    "| Environment | Description | Key Features |\n",
    "|-------------|-------------|-------------|\n",
    "| **vrpp** | Vehicle Routing with Profits | Prizes at nodes, maximize profit-cost |\n",
    "| **cvrpp** | Capacitated VRPP | + Vehicle capacity constraints |\n",
    "| **wcvrp** | Waste Collection VRP | Bin fill levels, accumulation |\n",
    "| **cwcvrp** | Capacitated WCVRP | + Vehicle capacity |\n",
    "| **sdwcvrp** | Stochastic Demand WCVRP | + Uncertain waste generation |\n",
    "\n",
    "### 6.3 TensorDict State Structure\n",
    "\n",
    "All environments use TensorDict for state management:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "tensordict-example",
   "metadata": {},
   "outputs": [],
   "source": [
    "from logic.src.envs import get_env\n",
    "\n",
    "# Create VRPP environment\n",
    "env = get_env(\n",
    "    \"vrpp\",\n",
    "    num_loc=20,\n",
    "    min_loc=0.0,\n",
    "    max_loc=1.0,\n",
    "    device=\"cpu\",\n",
    ")\n",
    "\n",
    "# Generate a batch of instances\n",
    "batch_size = 4\n",
    "td = env.generator(batch_size)\n",
    "\n",
    "print(\"Generated TensorDict:\")\n",
    "print(f\"  Batch size: {batch_size}\")\n",
    "print(f\"  Keys: {list(td.keys())}\")\n",
    "print(f\"\\nShapes:\")\n",
    "for key in td.keys():\n",
    "    print(f\"  {key}: {td[key].shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "env-step-example",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Demonstrate environment step\n",
    "td_reset = env.reset(td)\n",
    "\n",
    "print(\"After reset:\")\n",
    "print(f\"  Keys: {list(td_reset.keys())}\")\n",
    "print(f\"  Done: {td_reset['done'] if 'done' in td_reset.keys() else 'Not set'}\")\n",
    "\n",
    "# Show action mask\n",
    "if 'action_mask' in td_reset.keys():\n",
    "    mask = td_reset['action_mask']\n",
    "    print(f\"\\nAction mask shape: {mask.shape}\")\n",
    "    print(f\"  Valid actions (first instance): {mask[0].sum().item()}/{mask[0].numel()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "generator-section",
   "metadata": {},
   "source": [
    "### 6.4 Data Generators\n",
    "\n",
    "Generators create problem instances on-the-fly:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "generator-example",
   "metadata": {},
   "outputs": [],
   "source": [
    "from logic.src.envs import GENERATOR_REGISTRY, get_generator\n",
    "\n",
    "print(\"Available Generators:\")\n",
    "for name in GENERATOR_REGISTRY.keys():\n",
    "    print(f\"  {name}\")\n",
    "\n",
    "# Create VRPP generator\n",
    "generator = get_generator(\n",
    "    \"vrpp\",\n",
    "    num_loc=50,\n",
    "    min_loc=0.0,\n",
    "    max_loc=1.0,\n",
    ")\n",
    "\n",
    "# Generate instances\n",
    "instances = generator(batch_size=8)\n",
    "print(f\"\\nGenerated {instances.batch_size[0]} instances with {instances['locs'].shape[-2]} locations\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dataset-section",
   "metadata": {},
   "source": [
    "### 6.5 Dataset Classes\n",
    "\n",
    "The pipeline uses custom dataset classes:\n",
    "\n",
    "```python\n",
    "from logic.src.data.datasets import (\n",
    "    GeneratorDataset,      # On-the-fly generation\n",
    "    TensorDictDataset,     # Persistent storage\n",
    "    BaselineDataset,       # Wraps dataset with baseline values\n",
    "    tensordict_collate_fn, # Custom collation for TensorDict\n",
    ")\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dataset-example",
   "metadata": {},
   "outputs": [],
   "source": [
    "from logic.src.data.datasets import GeneratorDataset, tensordict_collate_fn\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# Create dataset\n",
    "dataset = GeneratorDataset(generator, size=1000)\n",
    "\n",
    "# Create dataloader\n",
    "dataloader = DataLoader(\n",
    "    dataset,\n",
    "    batch_size=64,\n",
    "    collate_fn=tensordict_collate_fn,\n",
    "    num_workers=0,\n",
    ")\n",
    "\n",
    "# Get a batch\n",
    "batch = next(iter(dataloader))\n",
    "print(f\"Batch keys: {list(batch.keys())}\")\n",
    "print(f\"Batch size: {batch.batch_size}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "section-7",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 7. Meta-Learning\n",
    "\n",
    "Meta-learning enables automatic adaptation of training parameters (e.g., reward weights) through bi-level optimization.\n",
    "\n",
    "### 7.1 Meta-Learning Wrapper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "meta-overview",
   "metadata": {},
   "outputs": [],
   "source": [
    "from logic.src.pipeline.rl import MetaRLModule\n",
    "\n",
    "# Create base RL module\n",
    "base_module = REINFORCE(\n",
    "    env=env,\n",
    "    policy=policy,\n",
    "    baseline=\"exponential\",\n",
    "    optimizer=\"adam\",\n",
    "    optimizer_kwargs={\"lr\": 1e-4},\n",
    "    train_data_size=1000,\n",
    "    val_data_size=100,\n",
    "    batch_size=64,\n",
    ")\n",
    "\n",
    "# Wrap with meta-learning\n",
    "meta_module = MetaRLModule(\n",
    "    agent=base_module,\n",
    "    meta_lr=1e-3,\n",
    "    history_length=10,\n",
    "    hidden_size=64,\n",
    ")\n",
    "\n",
    "print(\"MetaRLModule created:\")\n",
    "print(f\"  Inner agent: {base_module.__class__.__name__}\")\n",
    "print(f\"  Meta learning rate: {meta_module.meta_lr}\")\n",
    "print(f\"  History length: {meta_module.history_length}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "meta-strategies",
   "metadata": {},
   "source": [
    "### 7.2 Meta-Learning Strategies\n",
    "\n",
    "| Strategy | Description | Best For |\n",
    "|----------|-------------|----------|\n",
    "| **rnn** | Recurrent network processes reward history | General adaptation |\n",
    "| **bandit** | UCB/Thompson sampling for weight selection | Discrete weight choices |\n",
    "| **morl** | Multi-objective Pareto optimization | Multiple objectives |\n",
    "| **tdl** | Temporal difference learning | Online adaptation |\n",
    "| **hypernet** | Hypernetwork generates weights | Problem-conditioned |\n",
    "\n",
    "### 7.3 Bi-Level Optimization Flow\n",
    "\n",
    "```\n",
    "┌─────────────────────────────────────────────────────────────────┐\n",
    "│                    Meta-Learning Training Loop                   │\n",
    "└─────────────────────────────────────────────────────────────────┘\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    \n",
    "    ┌─────────────────────────────────────────────────────────────┐\n",
    "    │ INNER LOOP: RL Training                                      │\n",
    "    │                                                              │\n",
    "    │   for batch in dataloader:                                   │\n",
    "    │       loss = agent.training_step(batch)                      │\n",
    "    │       optimizer.step()                                       │\n",
    "    │                                                              │\n",
    "    │   reward_signal = epoch_reward                               │\n",
    "    └─────────────────────────────────────────────────────────────┘\n",
    "                              │\n",
    "                              ▼\n",
    "    ┌─────────────────────────────────────────────────────────────┐\n",
    "    │ OUTER LOOP: Meta-Strategy Update                             │\n",
    "    │                                                              │\n",
    "    │   meta_strategy.update(reward_signal)                        │\n",
    "    │   new_weights = meta_strategy.propose_weights()              │\n",
    "    │   env.update_weights(new_weights)                            │\n",
    "    └─────────────────────────────────────────────────────────────┘\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "section-8",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 8. Hyperparameter Optimization\n",
    "\n",
    "The pipeline supports two HPO methods:\n",
    "\n",
    "### 8.1 Optuna-Based HPO\n",
    "\n",
    "Multiple sampling strategies:\n",
    "- **TPE** (Tree-structured Parzen Estimator) - Default\n",
    "- **Grid Search**\n",
    "- **Random Search**\n",
    "- **Hyperband** - Multi-fidelity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "hpo-config",
   "metadata": {},
   "outputs": [],
   "source": [
    "from logic.src.configs import HPOConfig\n",
    "\n",
    "# HPO configuration\n",
    "hpo_cfg = HPOConfig(\n",
    "    method=\"tpe\",           # tpe, grid, random, hyperband\n",
    "    n_trials=50,            # Number of trials\n",
    "    n_epochs_per_trial=10,  # Epochs per trial\n",
    "    search_space={\n",
    "        \"optim.lr\": [1e-5, 1e-3],           # Log-uniform\n",
    "        \"train.batch_size\": [64, 512],       # Integer\n",
    "        \"model.embed_dim\": [64, 128, 256],   # Categorical\n",
    "        \"rl.entropy_weight\": [0.0, 0.1],     # Uniform\n",
    "    },\n",
    ")\n",
    "\n",
    "print(\"HPO Configuration:\")\n",
    "print(f\"  Method: {hpo_cfg.method}\")\n",
    "print(f\"  Trials: {hpo_cfg.n_trials}\")\n",
    "print(f\"  Epochs per trial: {hpo_cfg.n_epochs_per_trial}\")\n",
    "print(f\"\\nSearch Space:\")\n",
    "for param, range_val in hpo_cfg.search_space.items():\n",
    "    print(f\"  {param}: {range_val}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "hpo-cli",
   "metadata": {},
   "source": [
    "### 8.2 HPO via CLI\n",
    "\n",
    "```bash\n",
    "# TPE optimization with 50 trials\n",
    "python main.py train_lightning \\\n",
    "    hpo.n_trials=50 \\\n",
    "    hpo.method=tpe \\\n",
    "    hpo.n_epochs_per_trial=10 \\\n",
    "    'hpo.search_space={\"optim.lr\": [1e-5, 1e-3], \"train.batch_size\": [64, 512]}'\n",
    "\n",
    "# Grid search\n",
    "python main.py train_lightning \\\n",
    "    hpo.n_trials=100 \\\n",
    "    hpo.method=grid \\\n",
    "    'hpo.search_space={\"model.embed_dim\": [64, 128, 256], \"model.num_heads\": [4, 8]}'\n",
    "\n",
    "# DEHB (multi-fidelity)\n",
    "python main.py train_lightning \\\n",
    "    hpo.method=dehb \\\n",
    "    hpo.min_fidelity=1 \\\n",
    "    hpo.max_fidelity=50 \\\n",
    "    hpo.fevals=100\n",
    "```\n",
    "\n",
    "### 8.3 DEHB (Differential Evolution Hyperband)\n",
    "\n",
    "DEHB combines:\n",
    "- **Differential Evolution** for global search\n",
    "- **Hyperband** for multi-fidelity scheduling\n",
    "\n",
    "Benefits:\n",
    "- Early stopping of poor configurations\n",
    "- Efficient use of compute budget"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "section-9",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 9. Practical Examples\n",
    "\n",
    "### 9.1 Complete Training Script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "complete-example",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pytorch_lightning as pl\n",
    "from pytorch_lightning import seed_everything\n",
    "from pytorch_lightning.loggers import CSVLogger\n",
    "\n",
    "from logic.src.envs import get_env\n",
    "from logic.src.models.policies import AttentionModelPolicy\n",
    "from logic.src.pipeline.rl import REINFORCE\n",
    "from logic.src.pipeline.trainer import WSTrainer\n",
    "from logic.src.callbacks import SpeedMonitor\n",
    "\n",
    "# Set seed for reproducibility\n",
    "seed_everything(42)\n",
    "\n",
    "# 1. Create Environment\n",
    "env = get_env(\n",
    "    \"vrpp\",\n",
    "    num_loc=20,\n",
    "    device=\"cpu\",  # Use \"cuda\" if available\n",
    ")\n",
    "\n",
    "# 2. Create Policy\n",
    "policy = AttentionModelPolicy(\n",
    "    env_name=\"vrpp\",\n",
    "    embed_dim=64,\n",
    "    hidden_dim=128,\n",
    "    n_encode_layers=2,\n",
    "    n_heads=4,\n",
    ")\n",
    "\n",
    "# 3. Create RL Module\n",
    "model = REINFORCE(\n",
    "    env=env,\n",
    "    policy=policy,\n",
    "    baseline=\"exponential\",\n",
    "    optimizer=\"adam\",\n",
    "    optimizer_kwargs={\"lr\": 1e-4},\n",
    "    train_data_size=5000,\n",
    "    val_data_size=500,\n",
    "    batch_size=64,\n",
    "    num_workers=0,  # Use 0 for notebooks\n",
    ")\n",
    "\n",
    "# 4. Create Trainer\n",
    "trainer = WSTrainer(\n",
    "    max_epochs=3,  # Short training for demo\n",
    "    accelerator=\"auto\",\n",
    "    devices=1,\n",
    "    logger=CSVLogger(\"logs\", name=\"tutorial_demo\"),\n",
    "    callbacks=[SpeedMonitor(epoch_time=True)],\n",
    "    enable_progress_bar=True,\n",
    ")\n",
    "\n",
    "print(\"Training configuration ready!\")\n",
    "print(f\"  Environment: {env.__class__.__name__}\")\n",
    "print(f\"  Policy: {policy.__class__.__name__}\")\n",
    "print(f\"  Algorithm: REINFORCE with {model.baseline_type} baseline\")\n",
    "print(f\"  Epochs: {trainer.max_epochs}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "run-training",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run training (uncomment to execute)\n",
    "# trainer.fit(model)\n",
    "\n",
    "# Get final metrics\n",
    "# print(f\"\\nFinal validation reward: {trainer.callback_metrics.get('val/reward', 'N/A')}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cli-examples",
   "metadata": {},
   "source": [
    "### 9.2 CLI Examples Reference\n",
    "\n",
    "#### Basic Training\n",
    "```bash\n",
    "# VRPP with Attention Model\n",
    "python main.py train_lightning \\\n",
    "    model=am \\\n",
    "    env.name=vrpp \\\n",
    "    env.num_loc=50 \\\n",
    "    train.n_epochs=100\n",
    "\n",
    "# WCVRP (Waste Collection)\n",
    "python main.py train_lightning \\\n",
    "    model=am \\\n",
    "    env.name=wcvrp \\\n",
    "    env.num_loc=50 \\\n",
    "    env.capacity=100\n",
    "```\n",
    "\n",
    "#### Algorithm Variants\n",
    "```bash\n",
    "# PPO with custom parameters\n",
    "python main.py train_lightning \\\n",
    "    rl.algorithm=ppo \\\n",
    "    rl.ppo_epochs=10 \\\n",
    "    rl.eps_clip=0.2 \\\n",
    "    rl.value_loss_weight=0.5\n",
    "\n",
    "# POMO with augmentation\n",
    "python main.py train_lightning \\\n",
    "    rl.algorithm=pomo \\\n",
    "    rl.num_augment=8 \\\n",
    "    rl.num_starts=50\n",
    "\n",
    "# Imitation learning from HGS\n",
    "python main.py train_lightning \\\n",
    "    rl.algorithm=imitation \\\n",
    "    rl.expert=hgs\n",
    "```\n",
    "\n",
    "#### Advanced Features\n",
    "```bash\n",
    "# Meta-learning with RNN strategy\n",
    "python main.py train_lightning \\\n",
    "    rl.use_meta=true \\\n",
    "    rl.meta_strategy=rnn \\\n",
    "    rl.meta_lr=1e-3 \\\n",
    "    rl.meta_history_length=10\n",
    "\n",
    "# Learning rate scheduling\n",
    "python main.py train_lightning \\\n",
    "    optim.lr_scheduler=cosine \\\n",
    "    'optim.lr_scheduler_kwargs={\"T_max\": 100}'\n",
    "\n",
    "# Mixed precision training\n",
    "python main.py train_lightning \\\n",
    "    train.precision=16-mixed\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "section-10",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 10. Advanced Topics\n",
    "\n",
    "### 10.1 Custom RL Algorithm\n",
    "\n",
    "To implement a custom RL algorithm, inherit from `RL4COLitModule`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "custom-algorithm",
   "metadata": {},
   "outputs": [],
   "source": [
    "from logic.src.pipeline.rl.core.base import RL4COLitModule\n",
    "from typing import Optional\n",
    "import torch\n",
    "from tensordict import TensorDict\n",
    "\n",
    "class CustomRL(RL4COLitModule):\n",
    "    \"\"\"\n",
    "    Example custom RL algorithm.\n",
    "    \n",
    "    Implements a simple variant with custom advantage normalization.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(\n",
    "        self,\n",
    "        temperature: float = 1.0,\n",
    "        **kwargs,\n",
    "    ):\n",
    "        super().__init__(**kwargs)\n",
    "        self.temperature = temperature\n",
    "    \n",
    "    def calculate_loss(\n",
    "        self,\n",
    "        td: TensorDict,\n",
    "        out: dict,\n",
    "        batch_idx: int,\n",
    "        env: Optional[\"RL4COEnvBase\"] = None,\n",
    "    ) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Custom loss computation.\n",
    "        \n",
    "        Uses temperature-scaled advantage.\n",
    "        \"\"\"\n",
    "        reward = out[\"reward\"]\n",
    "        log_likelihood = out[\"log_likelihood\"]\n",
    "        \n",
    "        # Get baseline\n",
    "        if hasattr(self, \"_current_baseline_val\") and self._current_baseline_val is not None:\n",
    "            baseline_val = self._current_baseline_val\n",
    "        else:\n",
    "            baseline_val = self.baseline.eval(td, reward, env=env)\n",
    "        \n",
    "        # Temperature-scaled advantage\n",
    "        advantage = (reward - baseline_val) / self.temperature\n",
    "        advantage = (advantage - advantage.mean()) / (advantage.std() + 1e-8)\n",
    "        \n",
    "        # Policy gradient loss\n",
    "        loss = -(advantage.detach() * log_likelihood).mean()\n",
    "        \n",
    "        return loss\n",
    "\n",
    "print(\"CustomRL algorithm defined successfully!\")\n",
    "print(f\"  Key parameter: temperature={1.0}\")\n",
    "print(f\"  Inherits from: RL4COLitModule\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "custom-baseline",
   "metadata": {},
   "source": [
    "### 10.2 Custom Baseline\n",
    "\n",
    "To implement a custom baseline:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "custom-baseline-code",
   "metadata": {},
   "outputs": [],
   "source": [
    "from logic.src.pipeline.rl.core.baselines import Baseline\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from tensordict import TensorDict\n",
    "\n",
    "class PercentileBaseline(Baseline):\n",
    "    \"\"\"\n",
    "    Baseline using batch percentile.\n",
    "    \n",
    "    Returns the q-th percentile of rewards as baseline.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, percentile: float = 50.0):\n",
    "        super().__init__()\n",
    "        self.percentile = percentile\n",
    "    \n",
    "    def eval(\n",
    "        self,\n",
    "        td: TensorDict,\n",
    "        reward: torch.Tensor,\n",
    "        env = None,\n",
    "    ) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Compute percentile baseline.\n",
    "        \n",
    "        Args:\n",
    "            td: TensorDict (unused)\n",
    "            reward: Batch rewards\n",
    "            env: Environment (unused)\n",
    "            \n",
    "        Returns:\n",
    "            Percentile value expanded to reward shape\n",
    "        \"\"\"\n",
    "        # Compute percentile\n",
    "        baseline_val = torch.quantile(\n",
    "            reward.float(), \n",
    "            self.percentile / 100.0\n",
    "        )\n",
    "        return baseline_val.expand_as(reward)\n",
    "\n",
    "# Test the custom baseline\n",
    "baseline = PercentileBaseline(percentile=75.0)\n",
    "test_rewards = torch.randn(64)\n",
    "baseline_val = baseline.eval(None, test_rewards)\n",
    "\n",
    "print(f\"PercentileBaseline (75th):\")\n",
    "print(f\"  Input rewards shape: {test_rewards.shape}\")\n",
    "print(f\"  Baseline value: {baseline_val[0].item():.4f}\")\n",
    "print(f\"  Actual 75th percentile: {torch.quantile(test_rewards, 0.75).item():.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "multi-gpu",
   "metadata": {},
   "source": [
    "### 10.3 Multi-GPU Training\n",
    "\n",
    "PyTorch Lightning automatically handles distributed training:\n",
    "\n",
    "```bash\n",
    "# Single GPU\n",
    "python main.py train_lightning device=cuda\n",
    "\n",
    "# Multiple GPUs (DDP)\n",
    "python main.py train_lightning \\\n",
    "    device=cuda \\\n",
    "    --trainer.devices=4 \\\n",
    "    --trainer.strategy=ddp\n",
    "\n",
    "# DeepSpeed (for large models)\n",
    "python main.py train_lightning \\\n",
    "    --trainer.strategy=deepspeed_stage_2\n",
    "```\n",
    "\n",
    "### 10.4 Checkpoint Management\n",
    "\n",
    "```bash\n",
    "# Resume from checkpoint\n",
    "python main.py train_lightning \\\n",
    "    --ckpt_path=/path/to/checkpoint.ckpt\n",
    "\n",
    "# Custom checkpoint directory\n",
    "python main.py train_lightning \\\n",
    "    output_dir=assets/model_weights/experiment1\n",
    "```\n",
    "\n",
    "### 10.5 Debugging Tips\n",
    "\n",
    "```bash\n",
    "# Fast dev run (1 batch per epoch)\n",
    "python main.py train_lightning \\\n",
    "    --trainer.fast_dev_run=true\n",
    "\n",
    "# Limit batches for debugging\n",
    "python main.py train_lightning \\\n",
    "    --trainer.limit_train_batches=10 \\\n",
    "    --trainer.limit_val_batches=5\n",
    "\n",
    "# Profiling\n",
    "python main.py train_lightning \\\n",
    "    --trainer.profiler=simple\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "summary",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Summary\n",
    "\n",
    "This tutorial covered the Lightning-based RL training pipeline:\n",
    "\n",
    "### Key Components\n",
    "\n",
    "1. **Hydra Configuration**: Flexible, hierarchical configs with CLI overrides\n",
    "2. **PyTorch Lightning**: Automatic training loop, logging, checkpointing\n",
    "3. **RL Algorithms**: REINFORCE, PPO, POMO, SymNCO, HRL, Imitation\n",
    "4. **Baselines**: None, Exponential, Rollout, Critic, POMO, Warmup\n",
    "5. **Environments**: VRPP, WCVRP variants with TensorDict state\n",
    "6. **Meta-Learning**: Bi-level optimization for adaptive training\n",
    "7. **HPO**: Optuna (TPE, Grid, Random) and DEHB\n",
    "\n",
    "### Quick Reference\n",
    "\n",
    "```bash\n",
    "# Basic training\n",
    "python main.py train_lightning model=am env.name=vrpp\n",
    "\n",
    "# PPO with rollout baseline\n",
    "python main.py train_lightning rl.algorithm=ppo rl.baseline=rollout\n",
    "\n",
    "# POMO with augmentation\n",
    "python main.py train_lightning rl.algorithm=pomo rl.num_augment=8\n",
    "\n",
    "# Meta-learning\n",
    "python main.py train_lightning rl.use_meta=true rl.meta_strategy=rnn\n",
    "\n",
    "# Hyperparameter optimization\n",
    "python main.py train_lightning hpo.n_trials=50 hpo.method=tpe\n",
    "```\n",
    "\n",
    "### Further Reading\n",
    "\n",
    "- `CLAUDE.md` - Project overview and coding standards\n",
    "- `logic/src/pipeline/rl/` - Source code for all RL components\n",
    "- `logic/src/configs/` - Configuration dataclasses\n",
    "- `logic/src/envs/` - Environment implementations"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
