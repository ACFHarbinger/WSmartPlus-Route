{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# WSmart+ Route Optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "metadata": {}
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Already added home_dir to system path: /home/pkhunter/Repositories/wsmart_route\n"
     ]
    }
   ],
   "source": [
    "from notebook_setup import setup_home_directory, setup_google_colab\n",
    "\n",
    "\n",
    "NOTEBOOK_NAME = 'optimization'\n",
    "home_dir = setup_home_directory(NOTEBOOK_NAME)\n",
    "IN_COLAB, gdrive, gfiles = setup_google_colab(NOTEBOOK_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "if IN_COLAB:\n",
    "    %pip install fast-tsp\n",
    "    %pip install gurobipy\n",
    "    %pip install shapely\n",
    "    %pip install matplotlib\n",
    "    %pip install tqdm\n",
    "    %pip install pandas\n",
    "    %pip install torch\n",
    "    %pip install cuda-cudart\n",
    "    %pip install cudatoolkit\n",
    "    %pip install jupyter\n",
    "    %pip install networkx\n",
    "    %pip install numpy\n",
    "    %pip install torch_geometric\n",
    "    %pip install ortools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "import re\n",
    "import os\n",
    "import sys\n",
    "import time\n",
    "import torch\n",
    "import traceback\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pprint as pp\n",
    "import gurobipy as gp\n",
    "\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from tqdm import tqdm\n",
    "from __future__ import print_function\n",
    "from app.src.utils.definitions import SIM_METRICS, TQDM_COLOURS, DAY_METRICS\n",
    "from app.src.utils.functions import load_model\n",
    "from app.src.utils.plot_utils import plot_attention_maps_wrapper\n",
    "from app.src.utils.log_utils import log_to_json, log_plot, log_to_pickle\n",
    "from app.src.pipeline.simulator.bins import Bins\n",
    "from app.src.pipeline.simulator.wsmart_bin_analysis import OldGridBase\n",
    "from app.src.pipeline.simulator.day import set_daily_waste, get_daily_results\n",
    "from app.src.pipeline.simulator.processor import process_data, process_model_data\n",
    "from app.src.pipeline.simulator.network import compute_distance_matrix, get_paths_between_states, apply_edges\n",
    "from app.src.pipeline.simulator.loader import load_indices, load_depot, load_simulator_data, load_area_and_waste_type_params\n",
    "from app.src.or_policies import (\n",
    "    get_route_cost, find_route,\n",
    "    create_points, find_solutions,\n",
    "    policy_regular, policy_gurobi_vrpp,\n",
    "    policy_last_minute, policy_last_minute_and_path,\n",
    "    policy_lookahead, policy_lookahead_vrpp, policy_lookahead_sans,\n",
    "    policy_hexaly_vrpp\n",
    ")\n",
    "\n",
    "\n",
    "SEED = 42\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "\n",
    "FIXED_POINT_NOTATION = True # False to use scientific notation instead\n",
    "FLOAT_DIGITS_PRECISION = 15\n",
    "np.set_printoptions(precision=FLOAT_DIGITS_PRECISION)\n",
    "np.set_printoptions(suppress=FIXED_POINT_NOTATION)\n",
    "\n",
    "pd.options.mode.chained_assignment = None  # default='warn'\n",
    "if IN_COLAB: \n",
    "    gdrive.mount('/content/drive')\n",
    "\n",
    "# Required to use matplotlib in Windows without breaking the Kernel\n",
    "if os.name == 'nt':\n",
    "    os.environ['KMP_DUPLICATE_LIB_OK'] = 'True'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simulator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Experimental Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "Ndays = 31\n",
    "number_of_bins = 20\n",
    "n_bins = number_of_bins + 1 #with depot\n",
    "NC = n_bins\n",
    "binsids = np.arange(0, NC-1).tolist()\n",
    "\n",
    "area = \"Rio Maior\"\n",
    "waste_type = 'plastic'\n",
    "data_distribution = \"gamma\" #\"emp\"\n",
    "area = re.sub(r'[^a-zA-Z]', '', area.lower()) #area.translate(str.maketrans('', '', '-_ ')).lower()\n",
    "\n",
    "model_id = 0\n",
    "gamma_option = 0\n",
    "model_names = [\"am\"] #[\"am\", \"amgc\", \"transgcn\"]\n",
    "inner_dir = f\"gamma{gamma_option+1}\" if data_distribution == 'gamma' else \"emp\"\n",
    "\n",
    "data_dir = os.path.join(home_dir, \"data\", \"wsr_simulator\")\n",
    "output_dir = os.path.join(home_dir, \"assets\", \"output\", f\"{Ndays}_days\", f\"{area}_{number_of_bins}\")\n",
    "try:\n",
    "    os.makedirs(data_dir, exist_ok=True)\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "except Exception:\n",
    "    traceback.print_exc(file=sys.stdout)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "metadata": {}
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  [*] Loading model from /home/pkhunter/Repositories/wsmart_route/assets/model_weights/vrpp20_riomaior_plastic/gamma1/am/epoch-30.pt\n",
      "{'accumulation_steps': 1,\n",
      " 'activation': 'gelu',\n",
      " 'af_nparams': 3,\n",
      " 'af_param': 1.0,\n",
      " 'af_replacement': None,\n",
      " 'af_threshold': None,\n",
      " 'af_urange': [0.125, 0.3333333333333333],\n",
      " 'aggregation': 'sum',\n",
      " 'aggregation_graph': 'mean',\n",
      " 'area': 'riomaior',\n",
      " 'baseline': None,\n",
      " 'batch_size': 256,\n",
      " 'bl_alpha': 0.05,\n",
      " 'bl_warmup_epochs': 0,\n",
      " 'checkpoint_encoder': False,\n",
      " 'checkpoint_epochs': 1,\n",
      " 'data_distribution': 'gamma1',\n",
      " 'distance_method': 'gmaps',\n",
      " 'dm_filepath': 'data/wsr_simulator/distance_matrix/gmaps_distmat_plastic[riomaior].csv',\n",
      " 'dropout': 0.1,\n",
      " 'edge_method': 'knn',\n",
      " 'edge_threshold': 1.0,\n",
      " 'efficiency_weight': 0.8,\n",
      " 'embedding_dim': 128,\n",
      " 'enable_scaler': False,\n",
      " 'encoder': 'gat',\n",
      " 'epoch_size': 128000,\n",
      " 'epoch_start': 0,\n",
      " 'epsilon_alpha': 1e-05,\n",
      " 'eval_batch_size': 0,\n",
      " 'eval_focus_size': 0,\n",
      " 'eval_only': False,\n",
      " 'eval_time_days': 1,\n",
      " 'exp_beta': 0.8,\n",
      " 'focus_graph': 'graphs_20V_1N_plastic.json',\n",
      " 'focus_size': 128000,\n",
      " 'gnorm_groups': 4,\n",
      " 'graph_size': 20,\n",
      " 'hidden_dim': 512,\n",
      " 'learn_affine': True,\n",
      " 'load_path': None,\n",
      " 'log_dir': 'logs',\n",
      " 'log_step': 50,\n",
      " 'lr_critic_value': 0.0001,\n",
      " 'lr_decay': 1.0,\n",
      " 'lr_min_decay': 1e-08,\n",
      " 'lr_min_value': 0.0,\n",
      " 'lr_model': 0.0001,\n",
      " 'lr_post_processing': 0.001,\n",
      " 'lr_scheduler': 'lambda',\n",
      " 'lrnorm_k': None,\n",
      " 'lrs_cooldown': 0,\n",
      " 'lrs_dfactor': 0.1,\n",
      " 'lrs_milestones': [7, 14, 21, 28],\n",
      " 'lrs_mode': 'min',\n",
      " 'lrs_patience': 10,\n",
      " 'lrs_restart_steps': 7,\n",
      " 'lrs_rfactor': 2,\n",
      " 'lrs_step_size': 1,\n",
      " 'lrs_thresh': 0.0001,\n",
      " 'lrs_thresh_mode': 'rel',\n",
      " 'lrs_total_steps': 5,\n",
      " 'mask_graph': False,\n",
      " 'mask_inner': True,\n",
      " 'mask_logits': True,\n",
      " 'max_grad_norm': 1.0,\n",
      " 'model': 'am',\n",
      " 'momentum_beta': 0.1,\n",
      " 'n_decode_layers': None,\n",
      " 'n_encode_layers': 3,\n",
      " 'n_encode_sublayers': None,\n",
      " 'n_epochs': 31,\n",
      " 'n_heads': 8,\n",
      " 'n_predict_layers': None,\n",
      " 'no_cuda': False,\n",
      " 'no_progress_bar': False,\n",
      " 'no_tensorboard': False,\n",
      " 'normalization': 'instance',\n",
      " 'optimizer': 'rmsprop',\n",
      " 'output_dir': 'model_weights',\n",
      " 'overflow_weight': 0.2,\n",
      " 'post_processing_epochs': 0,\n",
      " 'problem': 'vrpp',\n",
      " 'resume': None,\n",
      " 'run_name': 'amgat_gamma1_20251013T205404',\n",
      " 'save_dir': 'model_weights/vrpp_20/amgat_gamma1_20251013T205404',\n",
      " 'seed': 42,\n",
      " 'shrink_size': None,\n",
      " 'tanh_clipping': 10.0,\n",
      " 'temporal_horizon': 0,\n",
      " 'time_filename': 'data/datasets/vrpp/vrpp20_gamma1_time31_seed42.pkl',\n",
      " 'track_stats': False,\n",
      " 'train_dataset': None,\n",
      " 'train_time': True,\n",
      " 'val_dataset': None,\n",
      " 'val_size': 0,\n",
      " 'vertex_method': 'mmn',\n",
      " 'w_length': 1.0,\n",
      " 'w_lost': None,\n",
      " 'w_overflows': 1.0,\n",
      " 'w_penalty': None,\n",
      " 'w_prize': None,\n",
      " 'w_waste': 1.0,\n",
      " 'wandb_mode': 'disabled',\n",
      " 'waste_filepath': None,\n",
      " 'waste_type': 'plastic'}\n",
      "Device set to cuda:0\n"
     ]
    }
   ],
   "source": [
    "models = {}\n",
    "configs = {}\n",
    "decode_type = \"greedy\"\n",
    "softmax_temperature = 1\n",
    "for model_name in model_names:\n",
    "    model_path = os.path.join(home_dir, \"assets\", \"model_weights\", f\"vrpp{number_of_bins}_{area}_{waste_type}\", inner_dir, model_name)\n",
    "    try:\n",
    "        model, config = load_model(model_path)\n",
    "        pp.pprint(config)\n",
    "        \n",
    "        device = torch.device(\"cpu\" if not torch.cuda.is_available() else f\"cuda:{torch.cuda.device_count()-1}\")\n",
    "        print(\"Device set to\", device)\n",
    "\n",
    "        models[model_name] = model\n",
    "        configs[model_name] = config\n",
    "        models[model_name].to(device)\n",
    "        models[model_name].eval()\n",
    "        models[model_name].set_decode_type(decode_type, temp=softmax_temperature)\n",
    "    except Exception as e:\n",
    "        print(f\"Failed to load {model_name} model from {model_path}\")\n",
    "        traceback.print_exc(file=sys.stdout)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "last_minute_cfs = []\n",
    "last_minute_variants = 'both' # 'only'|'path'|'both'\n",
    "\n",
    "regular_levels = []\n",
    "\n",
    "look_ahead_configs = []\n",
    "look_ahead_configurations = {\n",
    "    'a': [500,75,0.7,0,0.095,0,0], \n",
    "    'b': [2000,75,0.7,0,0.095,0,0]\n",
    "}\n",
    "look_ahead_variants = [] # 'base', 'vrpp', 'sans'\n",
    "\n",
    "gp_params = []\n",
    "if gp_params or 'vrpp' in look_ahead_variants:\n",
    "    gp_env_params = {'OutputFlag': 0}\n",
    "    gp_env = gp.Env(params=gp_env_params)\n",
    "\n",
    "hex_params = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "metadata": {}
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['am_gamma1']\n"
     ]
    }
   ],
   "source": [
    "policies = []\n",
    "if len(model_names) > 0:\n",
    "    for model_name in model_names:\n",
    "        policy = \"{}{}_{}\".format(model_name, f\"{model_id}\" if model_id >= 1 else \"\", inner_dir)\n",
    "        policies.append(policy)\n",
    "\n",
    "if last_minute_cfs:\n",
    "    for lmcf in last_minute_cfs:\n",
    "        if last_minute_variants in ['only', 'both']:\n",
    "            policy = f\"policy_last_minute{lmcf}_gamma{gamma_option+1}\" if inner_dir[:-1] == \"gamma\" else f\"policy_last_minute{lmcf}_emp\"\n",
    "            policies.append(policy)\n",
    "        if last_minute_variants in ['path', 'both']:\n",
    "            policy = f\"policy_last_minute_and_path{lmcf}_gamma{gamma_option+1}\" if inner_dir[:-1] == \"gamma\" else f\"policy_last_minute_and_path{lmcf}_emp\"\n",
    "            policies.append(policy)\n",
    "\n",
    "if regular_levels:\n",
    "    for lvl in regular_levels:\n",
    "        policy = f\"policy_regular{lvl}_gamma{gamma_option+1}\" if inner_dir[:-1] == \"gamma\" else f\"policy_regular{lvl}_emp\"\n",
    "        policies.append(policy)\n",
    "\n",
    "for lac in look_ahead_configs:\n",
    "    for lav in look_ahead_variants:\n",
    "        if lav == 'base':\n",
    "            policy = f\"policy_look_ahead_{lac}_gamma{gamma_option+1}\" if inner_dir[:-1] == \"gamma\" else f\"policy_look_ahead_{lac}_emp\"\n",
    "        else:\n",
    "            policy = f\"policy_look_ahead_{lac}_{lav}_gamma{gamma_option+1}\" if inner_dir[:-1] == \"gamma\" else f\"policy_look_ahead_{lac}_{lav}_emp\"\n",
    "        policies.append(policy)\n",
    "\n",
    "if gp_params:\n",
    "    for gpp in gp_params:\n",
    "        policy = f\"gurobi_vrpp{gpp}_gamma{gamma_option+1}\" if inner_dir[:-1] == \"gamma\" else f\"gurobi_vrpp{gpp}_emp\"\n",
    "        policies.append(policy)\n",
    "\n",
    "if hex_params:\n",
    "    for hexp in hex_params:\n",
    "        policy = f\"hexaly_vrpp{hexp}_gamma{gamma_option+1}\" if inner_dir[:-1] == \"gamma\" else f\"hexaly_vrpp{hexp}_emp\"\n",
    "        policies.append(policy)\n",
    "\n",
    "#policies = policies[1:] #+ policies[:1]\n",
    "print(policies)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Area riomaior (173 full) for 20 bins\n",
      "Lat: [39.25353454, 39.4429361111111]\n",
      "Lng: [-8.984290944, -8.79266007]\n"
     ]
    }
   ],
   "source": [
    "depot = load_depot(data_dir, area)\n",
    "data, bins_coordinates = load_simulator_data(data_dir, number_of_bins, area, waste_type)\n",
    "assert data.shape == bins_coordinates.shape\n",
    "\n",
    "print(f\"Area {area} ({bins_coordinates.shape[0]} full) for {number_of_bins} bins\")\n",
    "print(f\"Lat: [{bins_coordinates['Lat'].min()}, {bins_coordinates['Lat'].max()}]\")\n",
    "print(f\"Lng: [{bins_coordinates['Lng'].min()}, {bins_coordinates['Lng'].max()}]\")\n",
    "\n",
    "edge_thresh = 1.0\n",
    "edge_method = \"knn\"\n",
    "norm_method = \"mmn\"\n",
    "dist_mat_method = \"gmaps\"\n",
    "if area == 'riomaior':\n",
    "    grid = OldGridBase(data_dir, area)\n",
    "else:\n",
    "    grid = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "depot_tmp = depot.copy()\n",
    "depot_location = \"og\"\n",
    "if depot_location == 'mean':\n",
    "    depot_tmp['Lat'] = bins_coordinates['Lat'].mean()\n",
    "    depot_tmp['Lng'] = bins_coordinates['Lng'].mean()\n",
    "else:\n",
    "    assert depot_location == 'og'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run Experiments on Simulator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "Nsamples = 10\n",
    "start_id = 0\n",
    "assert start_id < Nsamples\n",
    "\n",
    "log_filepath = os.path.join(output_dir, f'log_mean_{Nsamples}N.json')\n",
    "if Nsamples > 1:\n",
    "    logstd_filepath = os.path.join(output_dir, f'log_std_{Nsamples}N.json')\n",
    "    logfull_filepath = os.path.join(output_dir, f\"log_full_{Nsamples}N.json\")\n",
    "\n",
    "data_size = bins_coordinates.shape[0]\n",
    "dm_filepath = os.path.join(data_dir, \"distance_matrix\", f\"gmaps_distmat_{waste_type}[{area}].csv\")\n",
    "daily_log_path = os.path.join(output_dir, f\"daily_{inner_dir}_{Nsamples}N.json\")\n",
    "if data_size > number_of_bins:\n",
    "    idx_filename = f\"graphs_{number_of_bins}V_1N_{waste_type}.json\"\n",
    "    indices_ls = load_indices(idx_filename, Nsamples, number_of_bins, data_size)\n",
    "else:\n",
    "    indices_ls = [None] * Nsamples\n",
    "\n",
    "daily_waste_path = os.path.join(data_dir, \"daily_waste\", \"{}{}_{}_wsr31_N10_seed{}.pkl\".format(area, number_of_bins, inner_dir, SEED))\n",
    "if not os.path.exists(daily_waste_path): \n",
    "    load_waste = False\n",
    "    daily_waste_path = None\n",
    "    print(f\"Specified daily waste fill file {daily_waste_path} does not exist\")\n",
    "else:\n",
    "    load_waste = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "am_gamma1 #0: 100%|\u001b[31m██████████\u001b[0m| 31/31 [00:00<00:00, 83.42it/s]\n",
      "am_gamma1 #1: 100%|\u001b[31m██████████\u001b[0m| 31/31 [00:00<00:00, 85.33it/s]\n",
      "am_gamma1 #2: 100%|\u001b[31m██████████\u001b[0m| 31/31 [00:00<00:00, 85.97it/s]\n",
      "am_gamma1 #3: 100%|\u001b[31m██████████\u001b[0m| 31/31 [00:00<00:00, 84.88it/s]\n",
      "am_gamma1 #4: 100%|\u001b[31m██████████\u001b[0m| 31/31 [00:00<00:00, 84.68it/s]\n",
      "am_gamma1 #5: 100%|\u001b[31m██████████\u001b[0m| 31/31 [00:00<00:00, 85.10it/s]\n",
      "am_gamma1 #6: 100%|\u001b[31m██████████\u001b[0m| 31/31 [00:00<00:00, 83.94it/s]\n",
      "am_gamma1 #7: 100%|\u001b[31m██████████\u001b[0m| 31/31 [00:00<00:00, 84.92it/s]\n",
      "am_gamma1 #8: 100%|\u001b[31m██████████\u001b[0m| 31/31 [00:00<00:00, 84.95it/s]\n",
      "am_gamma1 #9: 100%|\u001b[31m██████████\u001b[0m| 31/31 [00:00<00:00, 84.18it/s]\n"
     ]
    }
   ],
   "source": [
    "log = []\n",
    "if Nsamples > 1:\n",
    "    tmp_log = {pol: [] for pol in policies}\n",
    "\n",
    "run_tsp = False\n",
    "regular_cache = True\n",
    "full_daily_log = {}\n",
    "attention_dict = {model_name: [] for model_name in model_names}\n",
    "for sample_id in range(start_id, Nsamples):\n",
    "    indices = indices_ls[sample_id]\n",
    "    new_data, coordinates = process_data(data, bins_coordinates, depot_tmp, indices=indices)\n",
    "    distance_matrix = compute_distance_matrix(coordinates, dist_mat_method, focus_idx=indices, dm_filepath=dm_filepath)\n",
    "    dist_matrix_edges, shortest_paths, adj_matrix = apply_edges(distance_matrix, edge_thresh, edge_method)\n",
    "    pathbetweenstates = get_paths_between_states(n_bins, shortest_paths)\n",
    "    distancesC = np.round(dist_matrix_edges*10).astype('int32')\n",
    "    if len(models) > 0: distC_tensor = torch.from_numpy(distancesC).to(device)\n",
    "    tour_ls = []\n",
    "    cost_ls = []\n",
    "    for pol_id, pol in enumerate(policies):\n",
    "        desc = f\"{pol} #{sample_id}\"\n",
    "        policy = pol.rsplit('_', 1)[0]\n",
    "        daily_log = {key: [] for key in DAY_METRICS}\n",
    "        if len(models) > 0:\n",
    "            model_strip_name = re.split(r'[^a-zA-Z]', pol, maxsplit=1)[0]\n",
    "            if model_strip_name in ['am', 'amgc', 'transgcn']:\n",
    "                model = models[policy]\n",
    "                config = configs[policy]\n",
    "                model_data, graph, profit_vars = process_model_data(coordinates, distancesC, device, norm_method, config, \n",
    "                                                                edge_thresh, edge_method, area, waste_type, adj_matrix)\n",
    "        else:\n",
    "            model_data, graph = (None, None)\n",
    "\n",
    "        overflows = 0\n",
    "        cached = [] if regular_cache else None\n",
    "        fill_history = []\n",
    "        current_collection_day = 0\n",
    "        bins = Bins(NC - 1, data_dir, data_distribution, grid, waste_file=daily_waste_path)\n",
    "        bins.set_indices(indices)\n",
    "        if data_distribution == 'gamma':\n",
    "            bins.setGammaDistribution(option=gamma_option)\n",
    "        if daily_waste_path is not None:\n",
    "            bins.set_sample_waste(sample_id)\n",
    "\n",
    "        colour = TQDM_COLOURS[pol_id % len(TQDM_COLOURS)]\n",
    "        tic = time.perf_counter()\n",
    "        for day in tqdm(range(1, Ndays+1), desc=desc, colour=colour):\n",
    "            tour = []\n",
    "            new_overflows, fill, sum_lost = bins.loadFilling(day - 1) if load_waste else bins.stochasticFilling()\n",
    "            fill_history.append(fill)\n",
    "            overflows += new_overflows\n",
    "            if 'policy_last_minute_and_path' in policy:\n",
    "                last_minute_cf = int(policy.rsplit(\"_and_path\", 1)[1])\n",
    "                if last_minute_cf not in [50, 70, 90]:\n",
    "                    print('Valid cf values for policy_last_minute_and_path: [50, 70, 90]')\n",
    "                    raise ValueError(f'Invalid cf value for policy_last_minute_and_path: {last_minute_cf}')\n",
    "                bins.setCollectionLvlandFreq(cf=last_minute_cf/100)\n",
    "                tour = policy_last_minute_and_path(bins.c, distancesC, pathbetweenstates, bins.collectlevl, waste_type, area)\n",
    "                cost = get_route_cost(distance_matrix, tour) if tour else 0\n",
    "            elif 'policy_last_minute' in policy:\n",
    "                last_minute_cf = int(policy.rsplit(\"_last_minute\", 1)[1])\n",
    "                if last_minute_cf not in [50, 70, 90]:\n",
    "                    print('Valid cf values for policy_last_minute: [50, 70, 90]')\n",
    "                    raise ValueError(f'Invalid cf value for policy_last_minute: {last_minute_cf}')\n",
    "                bins.setCollectionLvlandFreq(cf=last_minute_cf/100)\n",
    "                tour = policy_last_minute(bins.c, distancesC, bins.collectlevl, waste_type, area)\n",
    "                cost = get_route_cost(distance_matrix, tour) if tour else 0\n",
    "            elif 'policy_regular' in policy:\n",
    "                regular_level = int(policy.rsplit(\"_regular\", 1)[1]) - 1\n",
    "                if regular_level not in [1, 2, 5]:\n",
    "                    print('Valid lvl values for policy_regular: [2, 3, 6]')\n",
    "                    raise ValueError(f'Invalid lvl value for policy_regular: {regular_level + 1}')\n",
    "                tour = policy_regular(bins.n, bins.c, distancesC, regular_level, day, cached, waste_type, area)\n",
    "                cost = get_route_cost(distance_matrix, tour) if tour else 0\n",
    "                if cached is not None and not cached and tour: cached = tour\n",
    "            elif policy[:2] == 'am' or policy[:4] == 'ddam' or \"transgcn\" in policy:\n",
    "                daily_data = set_daily_waste(model_data, bins.c, device, fill)\n",
    "                tour, cost, output_dict = model.compute_simulator_day(daily_data, graph, distC_tensor, profit_vars, run_tsp)\n",
    "                attention_dict[policy].append(output_dict)\n",
    "            elif 'gurobi' in policy:\n",
    "                gp_param = float(policy.rsplit(\"_vrpp\", 1)[1])\n",
    "                try:\n",
    "                    to_collect = policy_gurobi_vrpp(bins.c, dist_matrix_edges.tolist(), gp_env, gp_param, \n",
    "                                                    bins.means, bins.std, waste_type, area, time_limit=600)\n",
    "                except:\n",
    "                    to_collect = policy_gurobi_vrpp(bins.c, dist_matrix_edges.tolist(), gp_env, gp_param, \n",
    "                                                    bins.means, bins.std, waste_type, area, time_limit=3600)\n",
    "\n",
    "                if to_collect:\n",
    "                    tour = find_route(distancesC, np.array(to_collect[0])) if run_tsp else to_collect[0]\n",
    "                    cost = get_route_cost(dist_matrix_edges, tour)\n",
    "            elif 'hexaly' in policy:\n",
    "                hex_param = float(policy.rsplit(\"_vrpp\", 1)[1])\n",
    "                try:\n",
    "                    routes = policy_hexaly_vrpp(bins.c, distance_matrix.tolist(), hex_param, bins.means, bins.std, waste_type, area, time_limit=600)\n",
    "                except:\n",
    "                    routes = policy_hexaly_vrpp(bins.c, distance_matrix.tolist(), hex_param, bins.means, bins.std, waste_type, area, time_limit=3600)\n",
    "                \n",
    "                if to_collect:\n",
    "                    tour = find_route(distancesC, np.array(to_collect[0])) if run_tsp else to_collect[0]\n",
    "                    cost = get_route_cost(dist_matrix_edges, tour)\n",
    "            elif 'policy_look_ahead' in policy:\n",
    "                look_ahead_config = policy[policy.find('ahead_') + len('ahead_')]\n",
    "                try:\n",
    "                    chosen_combination = look_ahead_configurations[look_ahead_config]\n",
    "                except KeyError as ke:\n",
    "                    print('Possible policy_look_ahead configurations:')\n",
    "                    for pos_pol, ploa_configs in look_ahead_configurations.items():\n",
    "                        print(f'{pos_pol} configuration: {ploa_configs}')\n",
    "                    raise ValueError(f'Invalid policy_look_ahead configuration: {policy}')\n",
    "\n",
    "                binsids = np.arange(0, number_of_bins).tolist()\n",
    "                must_go_bins = policy_lookahead(binsids, bins.c, bins.means, current_collection_day)\n",
    "                if len(must_go_bins) > 0:\n",
    "                    vehicle_capacity, R, B, _ = load_area_and_waste_type_params(area, waste_type)\n",
    "                    C = 1 # travelling cost per travel unit (in €)\n",
    "                    E = 2.5 # m^3 #bin capacity\n",
    "                    values = {\n",
    "                        'R': R, 'C': C, 'E': E, 'B': B, \n",
    "                        'vehicle_capacity': vehicle_capacity,\n",
    "                    }\n",
    "                    if 'vrpp' in policy:\n",
    "                        values['time_limit'] = 600\n",
    "                        fh = np.array(fill_history).transpose()\n",
    "                        routes, profit, _ = policy_lookahead_vrpp(fh, binsids, must_go_bins, distance_matrix, values, env=gp_env)\n",
    "                        if routes:\n",
    "                            tour = find_route(distancesC, np.array(routes)) if run_tsp else routes\n",
    "                            cost = get_route_cost(distance_matrix, tour)\n",
    "                    elif 'sans' in policy:\n",
    "                        values['time_limit'] = 60\n",
    "                        fh = np.array(fill_history).transpose()\n",
    "                        T_min = 0.01\n",
    "                        T_init = 75\n",
    "                        iterations_per_T = 50000\n",
    "                        alpha = 0.7\n",
    "                        params = (T_init, iterations_per_T, alpha, T_min)\n",
    "                        routes, profit, _ = policy_lookahead_sans(fh, coordinates, distance_matrix, params, must_go_bins, values, binsids)\n",
    "                        if routes:\n",
    "                            tour = find_route(distancesC, np.array(routes[0])) if run_tsp else routes[0]\n",
    "                            cost = get_route_cost(distance_matrix, tour)\n",
    "                    else:\n",
    "                        values['shift_duration'] = 390 # minutes\n",
    "                        values['perc_bins_can_overflow'] = 0 # 0%\n",
    "                        points = create_points(new_data, coordinates)\n",
    "                        new_data.loc[1:number_of_bins+1, 'Stock'] = (bins.c/100).astype('float32')\n",
    "                        new_data.loc[1:number_of_bins+1, 'Accum_Rate'] = (bins.means/100).astype('float32')\n",
    "                        try:\n",
    "                            routes, profit, removed_bins = find_solutions(new_data, coordinates, distance_matrix, chosen_combination,\n",
    "                                                                        must_go_bins, values, number_of_bins, points, time_limit=600)\n",
    "                        except:\n",
    "                            routes, profit, removed_bins = find_solutions(new_data, coordinates, distance_matrix, chosen_combination,\n",
    "                                                                        must_go_bins, values, number_of_bins, points, time_limit=3600)\n",
    "                        \n",
    "                        if routes:\n",
    "                            tour = find_route(distancesC, np.array(routes[0])) if run_tsp else routes[0]\n",
    "                            cost = get_route_cost(distance_matrix, tour)\n",
    "            else:\n",
    "                raise ValueError(\"Unknown policy:\", policy)\n",
    "            cost_ls.append(cost)\n",
    "            tour_ls.append(tour)\n",
    "            #if cost > 0: print(\"Tour (cost {}): {}\".format(cost, tour))\n",
    "            collected, ncol = bins.collect(tour)\n",
    "            bins, dlog = get_daily_results(bins, cost, tour, day, new_overflows, sum_lost, coordinates)\n",
    "            for key, val in dlog.items():\n",
    "                daily_log[key].append(val)\n",
    "        full_daily_log[\"{}#{}\".format(pol, sample_id)] = daily_log\n",
    "        lg = [np.sum(bins.inoverflow), np.sum(bins.collected), np.sum(bins.ncollections), \n",
    "          np.sum(bins.lost), bins.travel, np.nan_to_num(np.sum(bins.collected)/bins.travel, 0), \n",
    "          np.sum(bins.inoverflow)-np.sum(bins.collected)+bins.travel, bins.ndays, time.perf_counter()-tic]\n",
    "        if Nsamples > 1:\n",
    "            save_id = sample_id - start_id\n",
    "            tmp_log[pol].append(lg)\n",
    "            log_to_json(logfull_filepath, SIM_METRICS, {pol: tmp_log[pol][save_id]}, sample_id=sample_id)\n",
    "            log_to_json(daily_log_path, DAY_METRICS, {f\"{policy} #{sample_id}\": daily_log.values()})\n",
    "        else:\n",
    "            log.append(lg)\n",
    "            log_to_json(log_filepath, SIM_METRICS, {policy: log[-1]})\n",
    "            log_to_json(daily_log_path, DAY_METRICS, {policy: daily_log.values()})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## To-Do\n",
    "### Fazer a conversao do lixo colectado de % pra KG (nos ficheiros com os resultados)\n",
    "### Remover fast-tsp do calculo das rotas da gurobi\n",
    "### Confirmar o cálculo das distâncias das rotas e dos Kg colectados\n",
    "- Diferenca nos KM percorridos e KG colectados devido ao bin 22056 estar em falta nos dados do Ygor\n",
    "### Fazer os graficos e os mapas para os resultados obtidos\n",
    "### Comecar escrever o paper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"am_gamma1\": {\n",
      " \"overflows\": 0.5,\n",
      " \"kg\": 15898.599956039145,\n",
      " \"ncol\": 1240.0,\n",
      " \"kg_lost\": 5.5375265801060065,\n",
      " \"km\": 4022.84,\n",
      " \"kg/km\": 3.9521333895496475,\n",
      " \"cost\": -11875.259956039145,\n",
      " \"days\": 62,\n",
      " \"time\": 0.36706260279970593\n",
      "}\n",
      "Standard deviation\n",
      "\"am_gamma1\": {\n",
      " \"overflows\": 0.7071067811865476,\n",
      " \"kg\": 144.21392823542556,\n",
      " \"ncol\": 0.0,\n",
      " \"kg_lost\": 12.021824847787151,\n",
      " \"km\": 9.867477444165473,\n",
      " \"kg/km\": 0.04030518545749113,\n",
      " \"cost\": 147.8267147064895,\n",
      " \"days\": 0.0,\n",
      " \"time\": 0.003108156702344389\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import statistics\n",
    "\n",
    "if Nsamples > 1:\n",
    "    log_mean = []\n",
    "    log_std = []\n",
    "    for pol_log in tmp_log.values():\n",
    "        log_mean.append([*map(statistics.mean, zip(*pol_log))])\n",
    "        log_std.append([*map(statistics.stdev, zip(*pol_log))])\n",
    "\n",
    "    for lg, pol in zip(log_mean, policies):\n",
    "        print(f'\"{pol}\":', end=\" \")   \n",
    "        print(json.dumps(dict(zip(SIM_METRICS, lg)), indent=True))\n",
    "\n",
    "    print(\"Standard deviation\")\n",
    "    for lg, pol in zip(log_std, policies):\n",
    "        print(f'\"{pol}\":', end=\" \")\n",
    "        print(json.dumps(dict(zip(SIM_METRICS, lg)), indent=True))\n",
    "else:\n",
    "    for lg, pol in zip(log, policies):\n",
    "        print(f'\"{pol}\":', end=\" \")\n",
    "        print(json.dumps(dict(zip(SIM_METRICS, lg)), indent=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "samples_attn_viz = [0] # list(range(0, Nsamples))\n",
    "\n",
    "for name in model_names:\n",
    "    for sample_idx in samples_attn_viz:\n",
    "        for layer_idx in range(configs[name]['n_encode_layers']):\n",
    "            for head_idx in range(configs[name]['n_heads']):\n",
    "                indices = indices_ls[sample_id]\n",
    "                labels = ['Depot'] + list(map(lambda id: \"Bin {}\".format(id), data.iloc[indices]['ID'].tolist()))\n",
    "                attn_maps = plot_attention_maps_wrapper(home_dir, Ndays, number_of_bins, \"output\", area, attention_dict, name, \n",
    "                                                        log_plot, layer_idx, sample_idx, head_idx, x_labels=labels, y_labels=labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics_viz = ['cost']\n",
    "policies_daily_viz = policies[:-1]\n",
    "days_ls = [x for x in range(1, Ndays+1)]\n",
    "\n",
    "plt.figure(dpi=200)\n",
    "for metric in metrics_viz:\n",
    "    plt.title(f\"Daily {metric} (Mean with Min/Max range over all simulation samples)\")\n",
    "    plt.xlabel(\"Day\")\n",
    "    plt.ylabel(f\"{metric.capitalize()}\")\n",
    "    for viz_pol in policies_daily_viz:\n",
    "        metric_pol_data = []\n",
    "        for sample_id in range(Nsamples):\n",
    "            metric_pol_data.append(full_daily_log[f\"{viz_pol}#{sample_id}\"][metric])\n",
    "\n",
    "        metric_arr = np.array(metric_pol_data)\n",
    "        means = np.mean(metric_arr, axis=0)\n",
    "        mins = np.min(metric_arr, axis=0)\n",
    "        maxs = np.max(metric_arr, axis=0)\n",
    "        plt.plot(days_ls, means, marker='o', linestyle='-', label=viz_pol)\n",
    "        plt.fill_between(days_ls, mins, maxs, alpha=0.2)\n",
    "    \n",
    "    plt.legend()\n",
    "    plt.grid(True, linestyle='--', alpha=0.7)\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save Logs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dit = {}\n",
    "for pol, val in zip(policies, log_mean):\n",
    "    dit[pol] = val if isinstance(val, list) else val.tolist()\n",
    "\n",
    "log_filepath = os.path.join(output_dir, f'log_mean_{Nsamples}N.json')\n",
    "log_to_json(log_filepath, SIM_METRICS, dit)\n",
    "if IN_COLAB:\n",
    "    log_to_pickle(os.path.join(output_dir, f'log_mean_{Nsamples}N.pkl'), log_mean, dw_func=gfiles.download)\n",
    "\n",
    "if Nsamples > 1:\n",
    "    std_dit = {}\n",
    "    for pol, val in zip(policies, log_std):\n",
    "        std_dit[pol] = val if isinstance(val, list) else val.tolist()\n",
    "    \n",
    "    logstd_filepath = os.path.join(output_dir, f'log_std_{Nsamples}N.json')\n",
    "    log_to_json(logstd_filepath, SIM_METRICS, std_dit)\n",
    "    if IN_COLAB:\n",
    "        log_to_pickle(os.path.join(output_dir, f'log_std_{Nsamples}N.pkl'), log_std, dw_func=gfiles.download)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "wsr",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.23"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
