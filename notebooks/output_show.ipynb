{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f341b1b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setup completed - added home_dir to system path: /home/pkhunter/Repositories/WSmart-Route\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import os\n",
    "import re\n",
    "import sys\n",
    "import traceback\n",
    "\n",
    "import folium\n",
    "import pandas as pd\n",
    "\n",
    "from .notebook_setup import setup_home_directory\n",
    "\n",
    "NOTEBOOK_NAME = \"output_show\"\n",
    "home_dir = setup_home_directory(NOTEBOOK_NAME)\n",
    "\n",
    "inout_dir = os.path.join(home_dir, \"assets\", \"output\")\n",
    "data_dir = os.path.join(home_dir, \"data\", \"wsr_simulator\")\n",
    "daily_waste_dir = os.path.join(data_dir, \"daily_waste\")\n",
    "coords_dir = os.path.join(data_dir, \"coordinates\")\n",
    "try:\n",
    "    os.makedirs(inout_dir, exist_ok=True)\n",
    "    os.makedirs(data_dir, exist_ok=True)\n",
    "    os.makedirs(daily_waste_dir, exist_ok=True)\n",
    "    os.makedirs(coords_dir, exist_ok=True)\n",
    "except Exception:\n",
    "    traceback.print_exc(file=sys.stdout)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ce2f588c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_route_folium(\n",
    "    route,\n",
    "    df_coords,\n",
    "    output_path,\n",
    "    marker_color=\"gray\",\n",
    "    deposito_lat=None,\n",
    "    deposito_lon=None,\n",
    "):\n",
    "    df_coords.columns = [col.strip().upper() for col in df_coords.columns]\n",
    "    # Ensure that the ID is of type string to avoid issues during comparison\n",
    "    df_coords[\"ID\"] = df_coords[\"ID\"].astype(str)\n",
    "    points = []\n",
    "    for idx in route:\n",
    "        if idx == 0 and deposito_lat is not None and deposito_lon is not None:\n",
    "            lat, lon = deposito_lat, deposito_lon\n",
    "        else:\n",
    "            # Search for the ID in the DataFrame\n",
    "            row = df_coords[df_coords[\"ID\"] == str(idx)]\n",
    "            if row.empty:\n",
    "                print(f\"WARNING: could not find ID {idx} in the DataFrame!\")\n",
    "                continue\n",
    "            row = row.iloc[0]\n",
    "            lat = float(str(row[\"LAT\"]).replace(\",\", \".\"))\n",
    "            lon = float(str(row[\"LNG\"]).replace(\",\", \".\"))\n",
    "            f\"ID {idx} (ID_coordinate: {row['ID']})\"\n",
    "        points.append((lat, lon))\n",
    "\n",
    "    map_center = points[0] if points else [0, 0]\n",
    "    m = folium.Map(location=map_center, zoom_start=13)\n",
    "    for (lat, lon), idx in zip(points, route):\n",
    "        if idx == 0:\n",
    "            folium.Marker(\n",
    "                location=(lat, lon),\n",
    "                popup=\"Depósito (id 0)\",\n",
    "                icon=folium.Icon(color=\"green\", icon=\"home\"),\n",
    "            ).add_to(m)\n",
    "        else:\n",
    "            folium.CircleMarker(\n",
    "                location=(lat, lon),\n",
    "                radius=6,\n",
    "                color=marker_color,\n",
    "                fill=True,\n",
    "                fill_opacity=0.7,\n",
    "                popup=f\"ID {idx}\",\n",
    "            ).add_to(m)\n",
    "    if points:\n",
    "        folium.PolyLine(points, color=\"blue\", weight=2, opacity=0.8).add_to(m)\n",
    "\n",
    "    m.save(output_path)\n",
    "    return\n",
    "\n",
    "\n",
    "# Function for log\n",
    "def json_log_to_excels(json_path, output_path):\n",
    "    with open(json_path, \"r\") as f:\n",
    "        data = json.load(f)\n",
    "    # If necessary transforms DataFrame to \"long\" (key as column)\n",
    "    df = pd.DataFrame.from_dict(data, orient=\"index\").reset_index()\n",
    "    df.rename(columns={\"index\": \"policy\"}, inplace=True)\n",
    "    df.to_excel(output_path, index=False)\n",
    "    print(f\"Saved DataFrame to '{output_path}'.\")\n",
    "    return\n",
    "\n",
    "\n",
    "# Function for daily\n",
    "def json_daily_to_excels(json_path, pasta_saida, coords_xlsx_path, deposito_lat=None, deposito_lon=None):\n",
    "    os.makedirs(pasta_saida, exist_ok=True)\n",
    "    df_coords = pd.read_excel(coords_xlsx_path)\n",
    "    with open(json_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        data = json.load(f)\n",
    "\n",
    "    for modelo, campos in data.items():\n",
    "        n = len(campos[\"day\"])\n",
    "        linhas = []\n",
    "        pasta_modelo = os.path.join(pasta_saida, modelo.replace(\"#\", \"_\").replace(\"/\", \"_\").replace(\"\\\\\", \"_\"))\n",
    "        os.makedirs(pasta_modelo, exist_ok=True)\n",
    "        pasta_mapas = os.path.join(pasta_modelo, \"mapas\")\n",
    "        os.makedirs(pasta_mapas, exist_ok=True)\n",
    "        for i in range(n):\n",
    "            # --- generates newline in excel\n",
    "            linha = {\n",
    "                \"day\": campos[\"day\"][i],\n",
    "                \"kg\": campos[\"kg\"][i],\n",
    "                \"overflows\": campos[\"overflows\"][i],\n",
    "                \"ncol\": campos[\"ncol\"][i],\n",
    "                \"kg_lost\": campos[\"kg_lost\"][i],\n",
    "                \"km\": campos[\"km\"][i],\n",
    "                \"kg_per_km\": campos[\"kg/km\"][i],\n",
    "                \"cost\": campos[\"cost\"][i],\n",
    "                \"tour\": \"[\" + \",\".join(str(x) for x in campos[\"tour\"][i]) + \"]\",\n",
    "            }\n",
    "            linhas.append(linha)\n",
    "            # --- generates HTML map for the current day\n",
    "            route = campos[\"tour\"][i]\n",
    "            output_map = os.path.join(pasta_mapas, f\"rota_dia{campos['day'][i]}.html\")\n",
    "            plot_route_folium(\n",
    "                route,\n",
    "                df_coords,\n",
    "                output_map,\n",
    "                deposito_lat=deposito_lat,\n",
    "                deposito_lon=deposito_lon,\n",
    "            )\n",
    "        # Save the excel for the given model\n",
    "        df = pd.DataFrame(linhas)\n",
    "        safe_modelo = modelo.replace(\"#\", \"_\").replace(\"/\", \"_\").replace(\"\\\\\", \"_\")\n",
    "        path_excel = os.path.join(pasta_modelo, f\"{safe_modelo}.xlsx\")\n",
    "        df.to_excel(path_excel, index=False)\n",
    "    print(f\"Saved maps and archives to '{pasta_saida}'.\")\n",
    "    return"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0a40281",
   "metadata": {},
   "source": [
    "Gerando um excel para os dados de saida do simulador: Pega os logs e gera um excel para comparar cada modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ada335b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved DataFrame to '/home/pkhunter/Repositories/WSmart-Route/assets/output/31_days/riomaior_104/log_mean_1N.xlsx'.\n"
     ]
    }
   ],
   "source": [
    "n_days = 31\n",
    "n_bins = 104\n",
    "n_samples = 1\n",
    "area = \"Rio Maior\"\n",
    "area = re.sub(r\"[^a-zA-Z]\", \"\", area.lower())\n",
    "\n",
    "input_path = os.path.join(inout_dir, f\"{n_days}_days\", f\"{area}_{n_bins}\", f\"log_mean_{n_samples}N.json\")\n",
    "output_path = os.path.join(inout_dir, f\"{n_days}_days\", f\"{area}_{n_bins}\", f\"log_mean_{n_samples}N.xlsx\")\n",
    "json_log_to_excels(input_path, output_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4a01e68",
   "metadata": {},
   "source": [
    "O seguinte código lê o json de saida do simulador e gera uma pasta para cada modelo contendo um excel com as informações necessárias e uma subpasta com os mapas de cada rota gerada"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "344f1868",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved maps and archives to '/home/pkhunter/Repositories/WSmart-Route/assets/output/31_days/riomaior_104/daily_emp_1N'.\n"
     ]
    }
   ],
   "source": [
    "data_dist = \"emp\"\n",
    "input_path = os.path.join(\n",
    "    inout_dir,\n",
    "    f\"{n_days}_days\",\n",
    "    f\"{area}_{n_bins}\",\n",
    "    f\"daily_{data_dist}_{n_samples}N.json\",\n",
    ")\n",
    "output_path = os.path.join(inout_dir, f\"{n_days}_days\", f\"{area}_{n_bins}\", f\"daily_{data_dist}_{n_samples}N\")\n",
    "json_daily_to_excels(\n",
    "    input_path,\n",
    "    output_path,\n",
    "    coords_xlsx_path=os.path.join(coords_dir, \"Coordinates_unique.xlsx\"),\n",
    "    deposito_lat=39.1838505324,\n",
    "    deposito_lon=-9.14806472054,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14be3042",
   "metadata": {},
   "source": [
    "## Verify daily waste fills"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf543d80",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Already added home_dir to system path: /home/pkhunter/Repositories/wsmart_route\n"
     ]
    }
   ],
   "source": [
    "import glob\n",
    "import pickle\n",
    "import re\n",
    "import traceback\n",
    "\n",
    "import numpy as np\n",
    "import openpyxl\n",
    "from logic.src.pipeline.simulations.loader import load_simulator_data\n",
    "\n",
    "from .notebook_setup import setup_home_directory\n",
    "\n",
    "NOTEBOOK_NAME = \"output_show\"\n",
    "home_dir = setup_home_directory(NOTEBOOK_NAME)\n",
    "SEED = 42\n",
    "\n",
    "n_days = 31\n",
    "n_bins = 170\n",
    "n_samples = 10\n",
    "data_dist = \"gamma1\"\n",
    "area = \"Rio Maior\"\n",
    "area = re.sub(r\"[^a-zA-Z]\", \"\", area.lower())\n",
    "data_dir = os.path.join(home_dir, \"data\", \"wsr_simulator\")\n",
    "daily_waste_dir = os.path.join(data_dir, \"daily_waste\")\n",
    "output_dir = os.path.join(home_dir, \"assets\", \"output\", f\"{n_days}_days\", f\"{area}_{n_bins}\")\n",
    "fill_history_dir = os.path.join(output_dir, \"fill_history\", data_dist)\n",
    "try:\n",
    "    os.makedirs(data_dir, exist_ok=True)\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    os.makedirs(daily_waste_dir, exist_ok=True)\n",
    "    os.makedirs(fill_history_dir, exist_ok=True)\n",
    "except Exception:\n",
    "    traceback.print_exc(file=sys.stdout)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c04d110",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_files_for_first_prefix(directory):\n",
    "    \"\"\"\n",
    "    Get files for the first prefix found (alphabetically)\n",
    "    \"\"\"\n",
    "    # Get all sample files in the directory\n",
    "    pattern = os.path.join(directory, \"*sample[0-9]*.xlsx\")\n",
    "    all_files = glob.glob(pattern)\n",
    "\n",
    "    if not all_files:\n",
    "        return []\n",
    "\n",
    "    # Extract all unique prefixes\n",
    "    prefixes = set()\n",
    "    for file in all_files:\n",
    "        filename = os.path.basename(file)\n",
    "        # Split on the last underscore before 'sample'\n",
    "        if \"_\" in filename and \"sample\" in filename:\n",
    "            prefix = filename.rsplit(\"_\", 1)[0]\n",
    "            prefixes.add(prefix)\n",
    "\n",
    "    # Get the first prefix alphabetically\n",
    "    first_prefix = sorted(prefixes)[0]\n",
    "\n",
    "    # Get ALL files for this first prefix\n",
    "    prefix_pattern = os.path.join(directory, f\"{first_prefix}_*sample.xlsx\")\n",
    "    return sorted(glob.glob(prefix_pattern))\n",
    "\n",
    "\n",
    "def delete_files_except_pattern(directory, pattern):\n",
    "    \"\"\"\n",
    "    Delete all files in directory except those matching the pattern\n",
    "    \"\"\"\n",
    "    # Get all files in the directory\n",
    "    all_files = glob.glob(os.path.join(directory, \"*\"))\n",
    "\n",
    "    # Get files to keep (matching the pattern)\n",
    "    files_to_keep = glob.glob(os.path.join(directory, pattern))\n",
    "\n",
    "    # Convert to sets for easier comparison\n",
    "    all_files_set = set(all_files)\n",
    "    keep_files_set = set(files_to_keep)\n",
    "\n",
    "    # Files to delete = all files - files to keep\n",
    "    files_to_delete = all_files_set - keep_files_set\n",
    "\n",
    "    # Delete files\n",
    "    deleted_count = 0\n",
    "    parent_dir = os.path.dirname(os.getcwd())\n",
    "    for file_path in files_to_delete:\n",
    "        if os.path.isfile(file_path):  # Only delete files, not directories\n",
    "            os.remove(file_path)\n",
    "            print(f\"Deleted: {os.path.relpath(file_path, parent_dir)}\")\n",
    "            deleted_count += 1\n",
    "\n",
    "    print(f\"Deleted {deleted_count} files. Kept {len(files_to_keep)} files.\")\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c57835a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "data317, coords317 = load_simulator_data(data_dir, number_of_bins=317, area=area)\n",
    "with open(os.path.join(data_dir, \"bins_selection\", f\"graphs_{n_bins}V_1N_plastic.json\")) as json_file:\n",
    "    bin_sel = json.load(json_file)\n",
    "\n",
    "df = data317.iloc[bin_sel[0]].reset_index()\n",
    "bins = df[\"ID\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cefd6cbd",
   "metadata": {},
   "source": [
    "O seguinte Script gera uma pasta com as arquivos .xlsx com as tabelas de enchimento diário dos contentores para cada Sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5b45759",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = os.path.join(\n",
    "    daily_waste_dir,\n",
    "    f\"{area}{n_bins}_{data_dist}_wsr{n_days}_N{n_samples}_seed{SEED}.pkl\",\n",
    ")\n",
    "with open(path, \"rb\") as f:\n",
    "    data = pickle.load(f)\n",
    "\n",
    "folder_name = os.path.join(daily_waste_dir, f\"{area}{n_bins}_{data_dist}_wsr{n_days}_N{n_samples}_seed{SEED}\")\n",
    "try:\n",
    "    os.makedirs(folder_name, exist_ok=True)\n",
    "except Exception:\n",
    "    traceback.print_exc(file=sys.stdout)\n",
    "\n",
    "num_versions = len(data)\n",
    "num_days = len(data[0])\n",
    "num_bins = len(data[0][0])\n",
    "for i in range(num_versions):\n",
    "    df = pd.DataFrame(data[i])\n",
    "    df = df.transpose()  # Transpose to make bins as rows and days as columns\n",
    "    df.index = bins\n",
    "    df.index.name = \"ID\"\n",
    "    df.columns = [f\"Dia {j + 1}\" for j in range(num_days)]\n",
    "    df.reset_index(inplace=True)\n",
    "    file_path = os.path.join(folder_name, f\"tabela_versao_{i}.xlsx\")\n",
    "    df.to_excel(file_path, index=False)\n",
    "\n",
    "parent_dir = os.path.dirname(os.getcwd())\n",
    "print(f\"All {num_versions} tables where saved to '{os.path.relpath(folder_name, parent_dir)}'.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61b56c15",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create column names for the days\n",
    "day_columns = [f\"day {i + 1}\" for i in range(n_days)]\n",
    "for file_path in get_files_for_first_prefix(fill_history_dir):\n",
    "    # Load the workbook and select sheet\n",
    "    wb = openpyxl.load_workbook(file_path, data_only=True)  # data_only=True to get calculated values\n",
    "    ws = wb.active  # Get the active sheet, or use wb['SheetName']\n",
    "\n",
    "    # Convert worksheet data to numpy matrix\n",
    "    data = []\n",
    "    for row in ws.iter_rows(values_only=True):\n",
    "        data.append(row)\n",
    "\n",
    "    matrix = np.array(data, dtype=float)  # Use dtype=object if you have mixed types\n",
    "\n",
    "    # Create the new DataFrame\n",
    "    result_df = pd.DataFrame(\n",
    "        data=matrix.astype(int),  # Your 50x31 numpy matrix\n",
    "        columns=day_columns,  # Column names: day 1, day 2, ..., day 31\n",
    "    )\n",
    "\n",
    "    # Add the 'ID' column as the first column\n",
    "    result_df.insert(0, \"ID\", df[\"ID\"].values)\n",
    "    result_df.insert(1, \"Mean\", np.round(np.mean(matrix, axis=1), 1))\n",
    "    result_df.insert(2, \"StD\", np.round(np.std(matrix, axis=1), 1))\n",
    "\n",
    "    # Save to Excel\n",
    "    sid = file_path[file_path.find(\"#\") + 1]\n",
    "    parent_dir = os.path.dirname(os.getcwd())\n",
    "    output_path = os.path.join(\n",
    "        output_dir,\n",
    "        \"fill_history\",\n",
    "        data_dist,\n",
    "        f\"enchimentos_seed{SEED}_sample{sid}.xlsx\",\n",
    "    )\n",
    "    result_df.to_excel(output_path, index=False, engine=\"openpyxl\")\n",
    "    print(f\"DataFrame saved to: {os.path.relpath(output_path, parent_dir)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "039e7bc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "delete_files_except_pattern(fill_history_dir, \"enchimentos*\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33837336",
   "metadata": {},
   "outputs": [],
   "source": [
    "from logic.src.pipeline.simulations.loader import (\n",
    "    load_depot,\n",
    "    load_indices,\n",
    "    load_simulator_data,\n",
    ")\n",
    "from logic.src.pipeline.simulations.network import apply_edges, compute_distance_matrix\n",
    "from logic.src.pipeline.simulations.processor import process_data\n",
    "\n",
    "from .notebook_setup import setup_home_directory\n",
    "\n",
    "NOTEBOOK_NAME = \"output_show\"\n",
    "home_dir = setup_home_directory(NOTEBOOK_NAME)\n",
    "\n",
    "area = \"Rio Maior\"\n",
    "src_area = area.translate(str.maketrans(\"\", \"\", \"-_ \")).lower()\n",
    "data_dir = os.path.join(home_dir, \"data\", \"wsr_simulator\")\n",
    "sub_dm_dir = os.path.join(data_dir, \"distance_matrix\", \"submatrix\")\n",
    "sel_coords_dir = os.path.join(data_dir, \"coordinates\", \"selected_coordinates\")\n",
    "try:\n",
    "    os.makedirs(data_dir, exist_ok=True)\n",
    "    os.makedirs(sub_dm_dir, exist_ok=True)\n",
    "    os.makedirs(sel_coords_dir, exist_ok=True)\n",
    "except Exception:\n",
    "    traceback.print_exc(file=sys.stdout)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "020def20",
   "metadata": {},
   "outputs": [],
   "source": [
    "depot = load_depot(data_dir, src_area)\n",
    "depot_tmp = depot.copy()\n",
    "data317, coords317 = load_simulator_data(data_dir, number_of_bins=317, area=src_area)\n",
    "dist_matrix317 = compute_distance_matrix(pd.concat([depot_tmp, coords317]).reset_index(drop=True), method=\"og\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "108b70b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "edge_thresh = 0.0\n",
    "edge_method = \"knn\"\n",
    "env_file = \"vars.env\"\n",
    "waste_type = \"plastic\"\n",
    "n_bins_ls = [20, 50, 100, 170]\n",
    "\n",
    "dist_mat_method = \"gmaps\"\n",
    "parent_dir = os.path.dirname(os.getcwd())\n",
    "for n_bins in n_bins_ls:\n",
    "    idx_filename = f\"graphs_{n_bins}V_1N_{waste_type}.json\"\n",
    "    indices_ls = load_indices(idx_filename, 10, n_bins, 317)\n",
    "    new_data, coordinates = process_data(data317, coords317, depot_tmp, indices=indices_ls[0])\n",
    "    clean_coords = coordinates.drop(\"#bin\", axis=1)\n",
    "    coords_filepath = os.path.join(sel_coords_dir, f\"coordinates{n_bins}_{waste_type}[{src_area}].xlsx\")\n",
    "    clean_coords.to_excel(coords_filepath, index=False)\n",
    "    print(f\"Coordinates for selected bins saved to: {os.path.relpath(coords_filepath, parent_dir)}\")\n",
    "\n",
    "    dm_filepath = os.path.join(\n",
    "        data_dir,\n",
    "        \"distance_matrix\",\n",
    "        f\"{dist_mat_method}_distmat_{waste_type}[{src_area}].csv\",\n",
    "    )\n",
    "    distance_matrix = compute_distance_matrix(\n",
    "        coordinates,\n",
    "        dist_mat_method,\n",
    "        focus_idx=indices_ls[0],\n",
    "        dm_filepath=dm_filepath,\n",
    "        env_filename=env_file,\n",
    "    )\n",
    "    dist_matrix_edges, shortest_paths, adj_matrix = apply_edges(distance_matrix, edge_thresh, edge_method)\n",
    "    submatrix_filepath = os.path.join(sub_dm_dir, f\"{dist_mat_method}_distmat{n_bins}_{waste_type}[{src_area}].csv\")\n",
    "    np.savetxt(submatrix_filepath, dist_matrix_edges, delimiter=\",\", fmt=\"%.3f\")\n",
    "    print(f\"Distance submatrix saved to: {os.path.relpath(submatrix_filepath, parent_dir)}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.25"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
