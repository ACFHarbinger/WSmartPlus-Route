# @package _global_
# =============================================================================
# Deep Decoder Attention Model (DDAM) Configuration
# =============================================================================
# Variant of AM with deeper encoder for improved spatial understanding.
#
# Key Difference from AM:
#   - n_encode_layers: 6 (vs 3 in AM)
#   - Richer node embeddings for complex problems
#   - Better performance on large instances (100+ nodes)
#   - Slower training and inference
#
# Best For: Large-scale VRP instances (150+ nodes)
# Trade-off: +20-30% training time for +2-5% solution quality
# =============================================================================

model:
  name: "deep_decoder"  # Model identifier
  embed_dim: 128  # Node embedding dimension (same as AM)
  hidden_dim: 512  # Feed-forward hidden size (same as AM)

  # =============================================================================
  # Deep Encoder Configuration
  # =============================================================================
  n_encode_layers: 6  # Deeper encoder (vs 3 in AM).
                      # Range: 3-12. Higher = richer encoding but slower.
                      # 6 is good balance for 100-200 node problems.

  n_encoder_sublayers: 1  # Sublayers per encoder block
  n_predictor_layers: 2  # GRF predictor depth
  n_decode_layers: 2  # Decoder depth (same as AM)
  n_heads: 8  # Attention heads (same as AM)
  spatial_bias: false  # Spatial bias (same as AM)
  encoder_type: "gat"  # Graph encoder type (same as AM)

  # =============================================================================
  # Standard Parameters (Inherited from AM)
  # =============================================================================
  temporal_horizon: 0  # Temporal lookahead (0 for static problems)
  normalization: "instance"  # Normalization type
  activation: "gelu"  # Activation function
  dropout: 0.1  # Dropout rate

  # Aggregation
  aggregation_node: "sum"  # Node-level aggregation
  aggregation_graph: "avg"  # Graph-level pooling
