# @package _global_
# =============================================================================
# Training Task Configuration
# =============================================================================
# Configuration for reinforcement learning training with PyTorch Lightning.
# Used by: python main.py train
#
# Key Features:
#   - Adaptive Imitation Learning (IL → RL transition)
#   - Multi-day temporal training for WCVRP
#   - Must-go bin selection strategies
#   - Lightning-based distributed training
# =============================================================================

defaults:
  - /tasks/policies/hgs_alns@rl.imitation.policy_config
  - /tasks/policies/hgs_alns@rl.adaptive_imitation.policy_config
  - _self_

# =============================================================================
# General Configuration
# =============================================================================
verbose: true # Enable detailed logging output
seed: 42 # Random seed for reproducibility
start: 0 # Starting epoch (for resuming training)

# =============================================================================
# Environment
# =============================================================================
env:
  name: "wcvrp" # Problem type. Options: vrpp, cvrpp, wcvrp, cwcvrp, sdwcvrp, scwcvrp

# =============================================================================
# Training Settings
# =============================================================================
train:
  train_data_size: 1280 # Number of training instances per epoch
  val_data_size: 128 # Number of validation instances

  train_time: true # Enable multi-day temporal training (for WCVRP)
  eval_time_days: 7 # Evaluation horizon in days (temporal problems)

  data_distribution: "gamma1" # Waste generation distribution. Options: gamma1, gamma2, uniform, beta
  load_dataset: "time" # Dataset loading mode. Options: "time" (temporal), "static"

  n_epochs: 100 # Total training epochs
  batch_size: 128 # Training batch size. Reduce if OOM errors occur.
  eval_batch_size: 128 # Validation batch size (can be larger than train)

  log_step: 10 # Log metrics every N batches
  logs_dir: "logs/output" # Directory for training logs
  model_weights_path: "model_weights" # Directory for checkpoints

  # DataLoader optimization
  persistent_workers: true # Keep workers alive between epochs (faster but more memory)
  pin_memory: false # Pin memory for faster GPU transfer
  reload_dataloaders_every_n_epochs: 0 # Reload data every N epochs (0 = never, for dynamic data)

# =============================================================================
# Data Configuration
# =============================================================================
data:
  dataset_size: 1280 # Total dataset instances (should match train_data_size)
  data_distributions: ["gamma1"] # List of distributions for multi-distribution training

# =============================================================================
# Optimizer Settings
# =============================================================================
optim:
  optimizer: "rmsprop" # Optimizer type. Options: adam, adamw, rmsprop, sgd
  lr: 0.0001 # Learning rate. Typical range: 1e-5 to 1e-3
  lr_decay: 1.0 # Learning rate decay per epoch (1.0 = no decay)
  lr_scheduler: "lambda" # LR scheduler. Options: lambda, step, cosine, exponential

# =============================================================================
# Reinforcement Learning / Algorithm Settings
# =============================================================================
rl:
  algorithm:
    "adaptive_imitation" # RL algorithm. Options:
    # - "adaptive_imitation": IL → RL transition
    # - "reinforce": REINFORCE with baseline
    # - "ppo": Proximal Policy Optimization
    # - "imitation": Pure imitation learning

  # Baseline Configuration
  baseline: "exponential" # Baseline type. Options: exponential, rollout, critic, no
  bl_alpha: 0.05 # Exponential baseline moving average weight
  exp_beta: 0.8 # Exponential moving average coefficient

  # Training Stability
  max_grad_norm: 1.0 # Gradient clipping threshold (prevents explosion)
  gamma: 1.0 # Discount factor for future rewards (1.0 = no discounting for episodic tasks)

  # =============================================================================
  # Algorithm-Specific Configurations
  # =============================================================================

  # PPO (Proximal Policy Optimization)
  ppo:
    epochs: 7 # Number of PPO update epochs per batch
    eps_clip: 0.2 # Clipping parameter for PPO objective

  # Imitation Learning
  imitation:
    # Expert policy configuration loaded from defaults: /tasks/policies/rl/hgs_alns
    # Override via CLI: rl.imitation.policy_config@=/tasks/policies/rl/hgs
    # Override params: rl.imitation.policy_config.time_limit=60.0
    # policy_config is populated by defaults list (see top of file)
    loss_fn: "nll" # Loss function. Options: nll, weighted_nll, kl, reverse_kl, js

  # Adaptive Imitation Learning (IL → RL)
  adaptive_imitation:
    # Expert policy configuration loaded from defaults: /tasks/policies/rl/hgs_alns
    # Override via CLI: rl.adaptive_imitation.policy_config@=/tasks/policies/rl/hgs
    # policy_config is populated by defaults list (see top of file)
    il_weight: 1.0 # Initial imitation loss weight (1.0 = pure IL, 0.0 = pure RL)
    il_decay: 0.95 # IL weight decay per epoch (gradual transition to RL)
    patience: 5 # Epochs to wait before decaying IL weight if no improvement
    threshold: 0.0001 # Improvement threshold (lowered from 0.05 due to small reward scale)
    decay_step: 1 # Step size for IL weight decay
    epsilon: 1.0e-6 # Minimum IL weight (prevents complete IL shutdown)
    loss_fn: "weighted_nll" # Loss function. Options: nll, weighted_nll, kl, reverse_kl

# =============================================================================
# Model Settings (Defaults - Override with model=<config>)
# =============================================================================
model:
  name: "am" # Model architecture (overridden by model=<config> CLI arg)

# =============================================================================
# Must-Go Selection Strategy
# =============================================================================
# Determines which bins MUST be collected during each routing decision.
# Critical for WCVRP training to prevent overflow penalties.
#
# Strategy Options:
#   - last_minute: Collect bins exceeding threshold (reactive)
#   - regular: Fixed-frequency collection (e.g., every 3 days)
#   - lookahead: Predict future overflows and collect proactively
#   - revenue: Collect if revenue > cost
#   - service_level: Statistical overflow prediction
#   - manager: Neural network decides must-go bins (HRL)
#   - combined: Hybrid of multiple strategies
#   - null: No must-go constraints (free routing)
# =============================================================================
must_go:
  strategy: "lookahead" # Default strategy for training

  # LastMinute/Manager threshold
  threshold:
    0.7 # Fill level threshold (0-1). Bins >= threshold are must-go.
    # For manager: probability threshold for must-go decision.

  # Regular parameters
  frequency: 3 # Collection frequency in days (for strategy=regular)

  # ServiceLevel parameters
  confidence_factor: 1.0 # Standard deviations for statistical prediction (σ multiplier)

  # Revenue parameters
  revenue_kg: 1.0 # Revenue per kg of waste collected
  bin_capacity: 1.0 # Maximum bin capacity
  revenue_threshold: 0.0 # Minimum revenue to justify collection

  # Common parameters
  max_fill: 1.0 # Overflow threshold (bins at max_fill overflow)

  # Manager (HRL high-level policy) parameters
  hidden_dim: 128 # Manager network hidden dimension
  lstm_hidden: 64 # LSTM hidden size for temporal encoding
  history_length: 10 # Number of past timesteps in waste history
  critical_threshold: 0.9 # Fill level considered critical for manager
  manager_weights: null # Path to pre-trained manager checkpoint (optional)
  device: "cuda" # Device for manager inference

# =============================================================================
# Global flags
# =============================================================================
wandb_mode: "disabled" # Weights & Biases logging. Options: online, offline, disabled
