# @package _global_
# =============================================================================
# Hyperparameter Optimization (HPO) Task Configuration
# =============================================================================
# Configuration for automated hyperparameter search using Optuna/DEHB.
# Used by: python main.py hp_optim
#
# Searches over model/training hyperparameters to find optimal configurations.
# =============================================================================

# =============================================================================
# General Configuration
# =============================================================================
verbose: true  # Enable detailed logging
seed: 42  # Random seed for reproducibility
start: 0  # Starting trial index

# =============================================================================
# Environment Override (Smaller for Faster HPO)
# =============================================================================
env:
  name: "wcvrp"  # Problem type for HPO
  num_loc: 20  # Use small problem size for faster trials (20 nodes vs 50-100)
  area: "riomaior"  # Geographic area
  waste_type: "plastic"  # Waste category
  edge_threshold: 1.0  # Graph connectivity threshold
  edge_method: "knn"  # Edge construction method
  vertex_method: "mmn"  # Vertex placement method

# =============================================================================
# HPO Configuration
# =============================================================================
hpo:
  # HPO Method
  method: "dehbo"  # Hyperparameter optimization method. Options:
                   # - "dehbo": Differential Evolution Hyperband Optimization (faster)
                   # - "optuna": Optuna with TPE sampler (more flexible)
                   # - "random": Random search (baseline)

  metric: "both"  # Optimization metric. Options:
                  # - "cost": Minimize total route cost
                  # - "reward": Maximize collected reward/waste
                  # - "both": Multi-objective (cost + reward)

  # Search Budget
  n_trials: 25  # Number of hyperparameter configurations to try.
                # Range: 10-100. More trials = better but slower.
                # 25 is good balance for initial search.

  n_epochs_per_trial: 7  # Training epochs per configuration.
                         # Reduced from full training for faster search.
                         # 7 epochs is enough to distinguish good/bad configs.

  fevals: 25  # Function evaluations (alias for n_trials)

  # Search Space
  hop_range: [0.0, 2.0]  # Range for temporal horizon parameter (TAM models).
                         # [0.0, 2.0] searches between static (0) and 2-day lookahead.

  # =============================================================================
  # Differential Evolution / Genetic Algorithm Parameters
  # =============================================================================
  # Used when method="dehbo"

  eta: 5.0  # Hyperband eta parameter (resource allocation aggressiveness).
            # Higher = more aggressive pruning of bad configurations.

  n_pop: 20  # Population size for evolutionary algorithms.
             # Larger = more diverse but slower.

  mutpb: 0.3  # Mutation probability (0.0-1.0).
              # Higher = more exploration, lower = more exploitation.
              # 0.3 is standard.
