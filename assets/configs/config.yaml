# =============================================================================
# WSmart-Route Main Configuration File
# =============================================================================
# This is the root Hydra configuration that composes all other config groups.
#
# Composition Order (Hydra defaults list):
#   1. _self_ - This file's settings (base layer)
#   2. tasks/* - Task-specific configs (train, eval, test_sim, gen_data, hpo, meta_train)
#   3. envs/* - Environment configs (cwcvrp, wcvrp, vrpp, cvrpp, sdwcvrp, scwcvrp)
#   4. model/* - Neural model architectures (am, tam, deep_decoder, ptr, symnco, moe)
#
# Override from CLI:
#   python main.py train model=tam envs=vrpp seed=1234
#   python main.py eval model.embed_dim=256
#
# See docs/HYDRA_GUIDE.md for full override syntax and multi-run examples.
# =============================================================================

defaults:
  - _self_ # This file's base settings
  - tasks: train # Default task config (tasks/train.yaml)
  - envs: cwcvrp # Default environment (Capacitated Waste Collection VRP)
  - models: am # Default model (Attention Model)

  # - data: train_data  # Optional data config (currently unused)

# =============================================================================
# Global Settings
# =============================================================================

seed: 42 # Random seed for reproducibility. Overridden by CLI or task configs.
verbose: true # Enable detailed logging output. Set false for production runs.
task: "train" # Active task type. Valid: train, eval, test_sim, gen_data, mrl_train, hp_optim.
start: 0 # Starting epoch/iteration. Useful for resuming interrupted training.
device: "cuda" # Compute device. Options: "cuda", "cpu", "cuda:0", "cuda:1", etc.

# =============================================================================
# Training Defaults (Overridden by tasks/train.yaml)
# =============================================================================
train:
  logs_dir: "logs" # Directory for training logs and checkpoints
  model_weights_path: "model_weights" # Subdirectory for saved model weights
  log_step: 10 # Log metrics every N steps
  callbacks:
    - display:
        _target_: logic.src.pipeline.callbacks.training_display.TrainingDisplayCallback
        metric_keys: ["train/reward", "val/reward"]
        chart_title: "ðŸ“ˆ Training Progress"
        history_length: null

# =============================================================================
# Output Paths
# =============================================================================
log_dir: "logs" # Root directory for all logging output
output_dir: "assets/model_weights" # Directory for final trained models
wandb_mode: "disabled" # Weights & Biases integration. Options: "online", "offline", "disabled"
